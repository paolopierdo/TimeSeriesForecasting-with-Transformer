{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "12f2ef06-cdf3-47f9-b5d0-f6de51435225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r Req_ristretto.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631a1b02-ea10-4b55-922c-672937624d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eee4e9ec-0464-4f67-a213-a9cf026a9625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "%run my_import.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f63991-c70a-4776-8329-a5668657370f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python  %run prepro.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318f2f9e-084c-4214-9c53-382821fda5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Su VSCode PARTO DIRETTAMENTE DA QUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fc18c50e-311d-4bf3-aeb8-def6d4ee2baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run my_import.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf788d65-14db-41e9-9b51-554d833ca448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run prepro.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cafdd63-17df-4715-9fc0-0235c8f2a67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ELENCO DATASET CHE HO QUI IN MEMORIA PER I MODELLI:\\\n",
    "series\\\n",
    "oil_prices\\\n",
    "missdata\\\n",
    "out\\\n",
    "df_solar\\\n",
    "f1\\\n",
    "df_elec\\\n",
    "favorita_train\\\n",
    "favorita_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011ee4a7-6c4e-4cb1-a3b9-c8ef720c2b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MODELS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e5558b0-e331-4842-9a95-4ce75b9fe720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **WHITE NOISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9202fcd-6c0f-4625-a32b-6ec04d5e92ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = series.reset_index()\n",
    "df = df.reset_index().rename(columns={'index': 'ds','level_0':'indice'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df = df.rename(columns={0: 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0f1149-77bf-4fcb-811d-efc07d718995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a01114de-751b-4ff2-aa3e-6d87d6e4f4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ea5eb19-1fc9-40c8-a77e-b39be2d80672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 7 # for monthly data = 12 \n",
    "horizon = len(test_df) # number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3942a3-3683-4df1-9bf3-efa655917455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c3b8b5b-aab9-4e14-bd12-8263577b9f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definisco i modelli\n",
    "models = [\n",
    "    AutoARIMA(season_length=season_length),\n",
    "    AutoETS(season_length=season_length)\n",
    "]\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='D')\n",
    "# fit\n",
    "sf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ed2fe60-7d31-402f-880d-29d97062d0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5750e585-9fc5-4390-a6f6-2e3ed95eb4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Il modello AutoARIMA ha rilevato come miglior modello un ARIMA (0,0,0). Quindi previsioni costanti. Normale per un White Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4913f64-5962-428e-95e3-6e8db1a43817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8904737-9e26-45ed-9f84-4a418cc438ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AutoETS ha rilevato come miglior modello un ANN cioè errore additivo (A), no trend, no stagionalità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9435e078-9180-4485-a489-541cd5751417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# residuals analysis for both models\n",
    "residual=pd.DataFrame(result2.get(\"residuals\"), columns=[\"residual Model\"])\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "print(\"---------Residui per ETS---------\")\n",
    "residual.plot(ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Residuals\")\n",
    "sns.distplot(residual, ax=axs[0,1])\n",
    "axs[0,1].set_title(\"Density plot - Residual\")\n",
    "stats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\n",
    "axs[1,0].set_title('Plot Q-Q')\n",
    "plot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\n",
    "axs[1,1].set_title(\"Autocorrelation\")\n",
    "plt.show()\n",
    "\n",
    "residual=pd.DataFrame(result.get(\"residuals\"), columns=[\"residual Model\"])\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "print(\"---------Residui per ARIMA---------\")\n",
    "residual.plot(ax=axs[0,0])\n",
    "axs[0,0].set_title(\"Residuals\")\n",
    "sns.distplot(residual, ax=axs[0,1])\n",
    "axs[0,1].set_title(\"Density plot - Residual\")\n",
    "stats.probplot(residual[\"residual Model\"], dist=\"norm\", plot=axs[1,0])\n",
    "axs[1,0].set_title('Plot Q-Q')\n",
    "plot_acf(residual,  lags=35, ax=axs[1,1],color=\"fuchsia\")\n",
    "axs[1,1].set_title(\"Autocorrelation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f86dd1-77a4-438e-be90-5680a98c4d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast on the test lenght + 100\n",
    "Y_hat_df = sf.forecast(df=train_df.drop(columns=['indice']), h=(horizon+100), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c618f2-8168-4cbc-a1e3-fcd60f540f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "metrics_etsA = evaluate(\n",
    "    test_df.drop(columns=['indice']).merge(Y_hat_df),\n",
    "    metrics=[ufl.mae, ufl.mape, partial(ufl.mase, seasonality=season_length), ufl.rmse, ufl.smape],\n",
    "    train_df=train_df,\n",
    ").drop(columns=['unique_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b83f85-56f5-4137-a538-65dbbf257c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In statsforecast mape a smape vengono restituiti in forma decimale: in realtà sarebbero 122% e 92%. Li sistemo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29924f0b-d1b8-4576-823c-58687840f199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_etsA.loc[[1,4], ['AutoARIMA','AutoETS']] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d48e3ed-76f0-46d2-9e6e-6f0f23760e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_etsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c61275b-7796-4df7-8c35-781255264df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(Y_hat_df, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=values['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA training', line=dict(color='green'),showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=values['AutoETS'], mode='lines', \n",
    "    name='AutoETS training', line=dict(color='orange', dash='dot'),showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# Intervallo di confidenza AUTOARIMA \n",
    "if 'AutoARIMA-lo-95' in forecast_df_etsA.columns and 'AutoARIMA-hi-95' in forecast_df_etsA.columns:\n",
    "    fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-lo-95'], \n",
    "        mode='lines', line_color='green', showlegend=False))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-hi-95'],\n",
    "        mode='lines', fill='tonexty', fillcolor='rgba(200,200,200,0.2)',\n",
    "        line_color='green', name='95% CI AutoARIMA'\n",
    "    ))\n",
    "\n",
    "# Intervallo di confidenza AUTOETS \n",
    "if 'AutoETS-lo-95' in forecast_df_etsA.columns and 'AutoETS-hi-95' in forecast_df_etsA.columns:\n",
    "    fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-lo-95'], \n",
    "        mode='lines', line_color='orange', showlegend=False))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-hi-95'],\n",
    "        mode='lines', fill='tonexty', fillcolor='rgba(255,165,0,0.15)',\n",
    "        line_color='orange', name='95% CI AutoETS'))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74a8e51-ae1f-464a-a0a2-aa146af33c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Residual analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c1ec2a-3e8e-45e1-adfc-014609868893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# testing residuals analysis\n",
    "ultimo_test = test_df['ds'].max()\n",
    "primo_test = test_df['ds'].min()\n",
    "merged_df = forecast_df_etsA.loc[(forecast_df_etsA['ds']<=ultimo_test) & (forecast_df_etsA['ds']>=primo_test)]\n",
    "\n",
    "residuals_test_arima = pd.Series(merged_df['y'] - merged_df['AutoARIMA'])\n",
    "residuals_test_ets = pd.Series(merged_df['y'] - merged_df['AutoETS'])\n",
    "plot_residuals(residuals_test_arima, model_name=\"ARIMA - test\")\n",
    "plot_residuals(residuals_test_ets, model_name=\"ETS - test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b597e281-c2ae-42f3-ba36-4c3118119813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "crossvalidation_df = sf.cross_validation(df=train_df,\n",
    "                                         h=horizon, \n",
    "                                         step_size=100,\n",
    "                                         n_windows=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24128a0-1d04-4788-8d71-618af52b5b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9b81cd-88e4-4bc0-9607-81ce51b3b0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Lag features (1-7)\n",
    "    for lag in range(1, 8):\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling features, 7, 30 e std 7\n",
    "    df['rolling_mean_7'] = df[target_col].shift(1).rolling(window=7).mean()\n",
    "    df['rolling_mean_30'] = df[target_col].shift(1).rolling(window=30).mean()\n",
    "    df['rolling_std_7'] = df[target_col].shift(1).rolling(window=7).std()\n",
    "    \n",
    "    # Differencing features, 1 e 7\n",
    "    df['diff_1'] = df[target_col] - df[target_col].shift(1)\n",
    "    df['diff_7'] = df[target_col] - df[target_col].shift(7)\n",
    "    \n",
    "    # Time-based features per future regressors\n",
    "    df['day_of_week'] = df['ds'].dt.dayofweek\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43121f39-5aff-48df-a60b-77360ef603cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 30 obs.\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c157b61-8520-4837-9388-7732b5ecd924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.8)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f42d684a-12d0-454b-b5d3-1e2d55c87c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f51e4f-9556-4e96-8591-1179a5d9eaea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna con TimeSeriesSplit\n",
    "cv_result_global = []\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Applicazione di TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    y = df_xgb['y']\n",
    "    df_xgb_feature = df_xgb.drop(columns=['y', 'ds'])\n",
    "    all_rmses = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(df_xgb_feature):\n",
    "        X_train_cv, X_test_cv = df_xgb_feature.iloc[train_index], df_xgb_feature.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_cv.values, label=y_train_cv.values)\n",
    "        dtest = xgb.DMatrix(X_test_cv.values, label=y_test_cv.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500, evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_cv.values, preds))\n",
    "        all_rmses.append(rmse)\n",
    "    cv_result_global.append(all_rmses)\n",
    "    # Restituisci la media degli RMSE di tutte le fold\n",
    "    mean_rmse = np.mean(all_rmses)\n",
    "    print(f\"Mean RMSE across time series folds: {mean_rmse}\")\n",
    "    return mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b893db-412b-4c0b-b483-0ffa49b86478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a5bcc7-ed27-432d-9cfb-b0c696dabd3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce1f7f0-4485-4879-84b4-1b182961daa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot about CV-Optuna\n",
    "# hyperparameter importance\n",
    "fig = vis.plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "# RMSE per ogni fold\n",
    "cv_df = pd.DataFrame(cv_result_global)\n",
    "cv_df.columns = [f\"Fold {i+1}\" for i in range(cv_df.shape[1])]\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=cv_df)\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Distribuzione RMSE su ogni fold (Optuna CV)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06addbd-0623-4ff1-8818-68fe01af5fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b041de-8b96-4dec-ac5f-8f4056710343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f596858d-5dc6-41fe-a84c-b74df493a237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=20)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5b6056-bc49-4443-85a5-203cd60ab7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analisi dei residui (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8acf291-1ce9-4901-af9f-8a7311df3c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training residuals analysis\n",
    "pred_train = model.predict(xgb.DMatrix(X_train_xgb,label=y_train_xgb))\n",
    "residuals_train_xgb = y_train_xgb - pred_train\n",
    "plot_residuals(residuals_train_xgb, model_name=\"XGBoost - training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27319ee9-fa9a-46f7-9313-8cf6b20d1e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MULTI-STEP FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ed474e-4fc3-47d5-9776-e029be6b232e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon, freq='D') #dopo modificare freq = M\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b5691c-855a-4b60-bf6c-fb42ab325688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "forecast_xgb = direct_multistep_forecast(train_df_xgb, feature_cols, target_col='y',horizon=len(test_df_xgb)+100)\n",
    "print(f\"Previsioni per i prossimi {(len(test_df_xgb)+100)} step:\", forecast_xgb.head()) #ho scelto +100 liberamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3036843-801b-4cb9-a93c-9078a24df442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# pred_train = model.predict(xgb.DMatrix(X_train_xgb,label=y_train_xgb)) #just to plot, shouldn't be used\n",
    "# fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=pred_train, mode='lines', name='Fit'))\n",
    "\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba2ff697-a578-42fa-b6d3-993626aead8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a342960-8d04-4e4f-bc29-c70e5ebeda0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'],modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12953b97-13cf-42fc-b65e-860f5bbd1945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analisi dei residui (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29dd1858-4aaa-42e7-a793-de501132e02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# testing residuals analysis\n",
    "residuals_test_xgb = pd.Series(merged_df['y'] - merged_df['forecast'])\n",
    "plot_residuals(residuals_test_xgb, model_name=\"XGBoost - test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed44646-bd25-4743-b5b9-e23177a4dd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST 2 (easier but worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e79027-04e0-4316-97ea-107cbd34f072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 30 obs.\n",
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.8)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a30eb830-a48b-4756-9e3a-fb37d54457a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna con TimeSeriesSplit\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 10,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Applicazione di TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    y = df_xgb['y']\n",
    "    df_xgb_feature = df_xgb.drop(columns=['y', 'ds'])\n",
    "    all_rmses = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(df_xgb_feature):\n",
    "        X_train_cv, X_test_cv = df_xgb_feature.iloc[train_index], df_xgb_feature.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_cv.values, label=y_train_cv.values)\n",
    "        dtest = xgb.DMatrix(X_test_cv.values, label=y_test_cv.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500, evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_cv.values, preds))\n",
    "        all_rmses.append(rmse)\n",
    "\n",
    "    # Restituisci la media degli RMSE di tutte le fold\n",
    "    mean_rmse = np.mean(all_rmses)\n",
    "    print(f\"Mean RMSE across time series folds: {mean_rmse}\")\n",
    "    return mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48d7e8c-f940-42e8-a353-7c2a878533fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'eval_metric': 'rmse'\n",
    "})\n",
    "\n",
    "reg_final = xgb.XGBRegressor(**best_params)\n",
    "reg_final.fit(X_train_xgb, y_train_xgb,\n",
    "              eval_set=[(X_train_xgb, y_train_xgb), (X_test_xgb, y_test_xgb)],\n",
    "              verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61be90a2-04e0-4ab6-a858-6369c081a643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Predictions\n",
    "pred = reg_final.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34bca76f-5579-400b-8650-e350900dc291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=pred, mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d201d9da-46bb-4991-8abe-107bf99e111d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_xgb2 = calcola_metriche(test_df_xgb['y'], pred, train_df_xgb['y'], modelname=\"XGBoost2\")\n",
    "metriche_xgb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944c2ca9-c3ae-44cf-8c2a-c634da9a4f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM (neuralforecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "638d3397-ddd4-49af-9328-009977ac95d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = df.copy()\n",
    "df_lstm.drop(columns=['indice'],inplace = True)\n",
    "train_size = int(len(df_lstm) * 0.8)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c44fcef-9047-4bf3-8503-719bb6add332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definiamo il modello\n",
    "nf = NeuralForecast(\n",
    "    models=[LSTM(h=len(test_df_lstm)+100,\n",
    "                 input_size=len(test_df_lstm)+100,\n",
    "                 loss=DistributionLoss(distribution=\"Normal\", level=[80, 95]),\n",
    "                 scaler_type='robust',\n",
    "                 encoder_n_layers=2,\n",
    "                 encoder_hidden_size=128,\n",
    "                 decoder_hidden_size=128,\n",
    "                 decoder_layers=2,\n",
    "                 max_steps=10,\n",
    "                 early_stop_patience_steps=5,\n",
    "                 recurrent=False,\n",
    "                 start_padding_enabled=True\n",
    "                 ),\n",
    "    ],\n",
    "    freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db6b737-f285-4f84-be00-e5439dc248d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# time series cross validation\n",
    "cv_df = nf.cross_validation(\n",
    "    val_size = len(test_df_lstm)+100,\n",
    "    df=train_df_lstm,       # Deve essere un df con colonne: unique_id, ds, y\n",
    "    #h=len(test_df_lstm),\n",
    "    n_windows=5,\n",
    "    step_size=100,\n",
    "    refit=True          # True = rifitta il modello da zero per ogni finestra\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932b04e6-0284-48b7-8576-257751340629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mae_lstm = mean_absolute_error(cv_df['y'], cv_df['LSTM'])\n",
    "print(f\"MAE LSTM: {mae_lstm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f280c713-99fd-45b6-8363-24d12f248965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fit\n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm)+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e91d053-4c8c-4138-ad95-aeb8cbf689cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creazione future dataframe\n",
    "start_date = test_df_lstm['ds'].min()\n",
    "all_dates = pd.date_range(start=start_date, periods=len(test_df_lstm)+100, freq='D')\n",
    "\n",
    "# Crea il DataFrame completo\n",
    "data = pd.DataFrame({\n",
    "    'ds': all_dates.strftime('%Y-%m-%d'),\n",
    "    'unique_id': 'serie_1'\n",
    "})\n",
    "\n",
    "# Aggiungi i valori y dai dati originali\n",
    "data['y'] = pd.Series(\n",
    "    list(test_df_lstm['y'].values) + [np.nan] * 100)\n",
    "data['ds'] = pd.to_datetime(data['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fcd3b42-1154-49a0-960b-58cadfecef72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict on test + 100\n",
    "Y_hat_df_lstm = nf.predict(futr_df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a8b0154-4a42-4ec8-a32d-d91e36a3c43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict insample, si blocca VSC\n",
    "# Y_hat_insample = nf.predict_insample()\n",
    "# print(Y_hat_insample.columns)\n",
    "# print(Y_hat_insample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19e7fcd0-d62a-459c-93bc-fbd2b06e00cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509e9f1c-ab37-4dae-9a96-9273d8039ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c8eeea-c2de-4dad-94fe-b20508b12f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Intervallo di confidenza\n",
    "if 'LSTM-lo-95' in Y_hat_df_lstm.columns and 'LSTM-hi-95' in Y_hat_df_lstm.columns:\n",
    "    fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-lo-95'], \n",
    "        mode='lines', line_color='green', showlegend=False))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-hi-95'],\n",
    "        mode='lines', fill='tonexty', fillcolor='rgba(200,200,200,0.2)',\n",
    "        line_color='green', name='95% CI LSTM'\n",
    "    ))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14a971af-a5df-4309-9270-d61fa74462c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# testing residuals analysis\n",
    "ultimo_test = test_df['ds'].max()\n",
    "primo_test = test_df['ds'].min()\n",
    "merged_df = Y_hat_df_lstm['LSTM'].loc[(Y_hat_df_lstm['ds']<=ultimo_test) & (Y_hat_df_lstm['ds']>=primo_test)]\n",
    "residuals_test_lstm = pd.Series(merged_df - test_df['y'].values)\n",
    "plot_residuals(residuals_test_lstm, model_name=\"LSTM - test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "251bbccc-c888-4d1b-a0e5-6f7cf19f639c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7102fbac-7c09-41ef-9751-77bd08d6de43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_np = df.drop(columns=['indice'])\n",
    "train_size = int(len(df_np) * 0.8)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "504061b8-0c96-4d76-9d14-ea27060d9a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definizione del modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=32,\n",
    "    # n_forecasts=1,                            # Specifica l'orizzonte di previsione\n",
    "    # n_lags=14,                                # componente AR, 14 gg, due settimane di dati; aggiunge 14 NaN\n",
    "    # daily_seasonality=Auto (True/False),      # Disattiva la stagionalità giornaliera se non necessaria\n",
    "    # weekly_seasonality=Auto,                  # Attiva stagionalità settimanale\n",
    "    # yearly_seasonality=Auto,                  # Attiva stagionalità annuale\n",
    "    loss_func='Huber'                           # Più robusta agli outlier rispetto a MSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f0b38f-321b-4dee-970f-8690810b1bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# training & predict                        \n",
    "init = time()\n",
    "\n",
    "#fit\n",
    "metrics_df = neuralprophet.fit(train_df_np[['ds','y']], validation_df=test_df_np[['ds','y']].head(30))\n",
    "# print(\"\\nTraining metrics:\")\n",
    "# print(metrics_df)\n",
    "\n",
    "#Prediction on test set\n",
    "forecast_test = neuralprophet.predict(test_df_np[['ds','y']])\n",
    "#Predictions out of sample\n",
    "future = neuralprophet.make_future_dataframe(df=df_np[['ds','y']], periods=100)\n",
    "forecast_future = neuralprophet.predict(future)\n",
    "\n",
    "end = time()\n",
    "time_np = (end - init) / 60\n",
    "print(f'Prophet Time: {time_np:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a1929a5-5920-44cb-84a1-96c9b31c001a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Plot Loss\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss_val'], label='Validation Loss', color='lightblue')\n",
    "\n",
    "# Plot MAE\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE_val'], label='Validation MAE', color='lightgreen')\n",
    "\n",
    "# Plot RMSE\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE_val'], label='Validation RMSE', color='orange')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe40fdf4-aa58-4d0d-a2eb-0e67a50a1385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# residual analysis (training)\n",
    "forecast_train = neuralprophet.predict(train_df_np[['ds','y']])\n",
    "residuals_train_np = pd.Series(forecast_train['y'] - forecast_train['yhat1'])\n",
    "plot_residuals(residuals_train_np, model_name=\"NeuralProphet - train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da304e0-f230-4110-a1ff-833f9eeff85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting columns & changing names\n",
    "forecast_test = forecast_test[['ds', 'yhat1', 'yhat1 2.5%', 'yhat1 97.5%','trend','season_yearly','season_weekly']]\n",
    "forecast_test.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-95', 'NeuralProphet-hi-95', \n",
    "                       'trend','season_yearly','season_weekly']\n",
    "forecast_future = forecast_future[['ds', 'yhat1', 'yhat1 2.5%', 'yhat1 97.5%','trend','season_yearly','season_weekly']]\n",
    "forecast_future.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-95', 'NeuralProphet-hi-95', \n",
    "                       'trend','season_yearly','season_weekly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d16233de-561a-4dd6-a632-96e4dee3f639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = neuralprophet.plot_parameters()\n",
    "params.show() #Mostra la forza di ogni componente del modello e come contribuisce alla previsione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425a5807-1a0f-4ffb-bfd2-1388cc3273a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig_components = neuralprophet.plot_components(forecast_test)\n",
    "fig_components.show() # Plot components like trends and different seasonalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c1d28f-1d82-46c2-a3b0-8be3ac9acc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_future.tail() #last forecasts available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f210d5bd-cb05-4f43-add3-ac8b82091582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_np['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_np['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# training period\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_train['ds'], \n",
    "    y=forecast_train['yhat1'], \n",
    "    mode='lines', \n",
    "    line=dict(color='green'),\n",
    "    showlegend=False,\n",
    "    name = \"NeuralProphet-train set\"\n",
    "))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['NeuralProphet'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Out-of-sample forecast (future)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_future['ds'], \n",
    "    y=forecast_future['NeuralProphet'], \n",
    "    mode='lines', \n",
    "    line=dict(color='green'),\n",
    "    showlegend=False,  # Evita duplicati nella legenda\n",
    "    name=\"Out-of-sample\"\n",
    "))\n",
    "\n",
    "# Intervallo di confidenza - test period\n",
    "if 'NeuralProphet-lo-95' in forecast_test.columns and 'NeuralProphet-hi-95' in forecast_test.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_test['ds'], \n",
    "        y=forecast_test['NeuralProphet-lo-95'], \n",
    "        mode='lines', \n",
    "        line=dict(color='green', width=0), \n",
    "        name = \"95% CI\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_test['ds'], \n",
    "        y=forecast_test['NeuralProphet-hi-95'],\n",
    "        mode='lines', \n",
    "        fill='tonexty', \n",
    "        fillcolor='rgba(200,200,200,0.2)',\n",
    "        line=dict(color='green', width=0), \n",
    "        name='95% CI'\n",
    "    ))\n",
    "\n",
    "# Intervallo di confidenza - future period\n",
    "if 'NeuralProphet-lo-95' in forecast_future.columns and 'NeuralProphet-hi-95' in forecast_future.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_future['ds'], \n",
    "        y=forecast_future['NeuralProphet-lo-95'], \n",
    "        mode='lines', \n",
    "        line=dict(color='green', width=0), \n",
    "        name = \"95% CI\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_future['ds'], \n",
    "        y=forecast_future['NeuralProphet-hi-95'],\n",
    "        mode='lines', \n",
    "        fill='tonexty', \n",
    "        fillcolor='rgba(200,200,200,0.2)',\n",
    "        line=dict(color='green', width=0), \n",
    "        name = \"95% CI\",\n",
    "        showlegend=False \n",
    "    ))\n",
    "    \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22584d39-31c9-4583-8887-7d15ab789098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['NeuralProphet']})\n",
    "merged_df = pd.merge(test_df_np[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df['y'],modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7807866d-2ab8-47dc-86de-5d80da8e654a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# testing residuals analysis\n",
    "residuals_test_np = pd.Series(merged_df['y'] - merged_df['forecast'])\n",
    "plot_residuals(residuals_test_np, model_name=\"NeuralProphet - test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94720add-18b7-4beb-b5e3-7a890f3a56c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save and re-import a model\n",
    "# import pickle\n",
    "# # Save the model\n",
    "# with open(\"neuralprophet_model.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(neuralprophet, file) #pickle.dump\n",
    "# print(\"Model saved as 'neuralprophet_model.pkl'.\")\n",
    "# # Load the saved model\n",
    "# with open(\"neuralprophet_model.pkl\", \"rb\") as file:\n",
    "#     loaded_model = pickle.load(file) #pickle.load\n",
    "# print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bca6334-e038-49db-839c-a3e9eeafe11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ac8bb8-e194-4ab6-b240-988085793233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_etsA = metrics_etsA.set_index('metric')\n",
    "final_metrics = metrics_etsA.join(metriche_xgb).join(metriche_xgb2).join(metriche_lstm).join(metriche_np).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68474359-82cf-45a5-9958-3aaa02ac10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c8b4c9-7673-4b24-beb0-18f0d78552f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PLOT FOR WHITE NOISE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_np['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_np['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='yellow')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGB', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='lightpink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['NeuralProphet'], \n",
    "    mode='lines', \n",
    "    name='NP in-sample', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "# Out-of-sample forecast (future)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_future['ds'], \n",
    "    y=forecast_future['NeuralProphet'], \n",
    "    mode='lines', \n",
    "    line=dict(color='aquamarine'),\n",
    "    name=\"NP Out-of-sample\"\n",
    "))\n",
    "    \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f55c72d9-1229-4037-a3d2-ac1c966d155f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **SOLAR ENERGY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d72e23e5-e82a-4047-88ad-1e50a7e82315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df_solar.drop(columns=['AC_POWER'])\n",
    "df = df.reset_index().rename(columns={'DATE_TIME': 'ds','DC_POWER':'y'})\n",
    "df = df.dropna()\n",
    "df['unique_id'] = 'serie_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "587de08d-3ac2-4f2a-bdbc-c6263a7bd1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mancano due righe di notte, le fillo easy\n",
    "row_to_copy = df[df['ds'] == '2020-06-17 06:00:00']\n",
    "# Crea copie modificate per le 06:15 e 06:30\n",
    "row_0615 = row_to_copy.copy()\n",
    "row_0615['ds'] = pd.to_datetime('2020-06-17 06:15:00')\n",
    "\n",
    "row_0630 = row_to_copy.copy()\n",
    "row_0630['ds'] = pd.to_datetime('2020-06-17 06:30:00')\n",
    "\n",
    "df = pd.concat([df, row_0615, row_0630], ignore_index=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df = df.sort_values(by='ds').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be6dd4ed-e4e1-4f68-b353-9a1ddbcc4219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 2, 4, 24, 36, 48, 96]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features, 1, 4, 96\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1) #diff tra t-1 e t-2\n",
    "    df['diff_4'] = (df[target_col] - df[target_col].shift(4)).shift(1) #diff tra t-1 e t-5\n",
    "    df['diff_96'] = (df[target_col] - df[target_col].shift(96)).shift(1) #diff tra t-1 e t-97\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_4'] = df[target_col].rolling(window=4).mean().shift(4) #ultima ora\n",
    "    df['rolling_mean_48'] = df[target_col].rolling(window=6).mean().shift(1) #ultime 12 ore\n",
    "    df['rolling_mean_96'] = df[target_col].rolling(window=12).mean().shift(1) #ultimo giorno\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc5da16-9e51-4475-a21a-3896c5a4fb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "af15fb69-881a-4d30-887f-a7054576cc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_arima = df[['y','ds','unique_id']].iloc[97:]\n",
    "train_size = int(len(df_arima) * 0.9)\n",
    "train_df = df_arima.iloc[:train_size]\n",
    "test_df = df_arima.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 96\n",
    "horizon = len(test_df) # number of predictions\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52420db2-b632-4c3e-9f07-bd7c7e3553b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "15d16cec-80bc-44b4-bdec-d16178a967c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definisco i modelli\n",
    "models = [\n",
    "    AutoARIMA(), \n",
    "    AutoETS() \n",
    "]\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='15min')\n",
    "\n",
    "start_time = time.time()\n",
    "# fit\n",
    "sf.fit(train_df)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "edd0763d-e57a-4772-8af7-b53ed2b33f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aae16a8-efca-4a82-95a4-49d6fd3b62e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arima_string(sf.fitted_[0,0].model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0eb90a62-3060-4cb1-be57-ab36bf0237f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7b2c7b-ef5e-4d0c-b7f8-996ea5f0ccb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast\n",
    "Y_hat_df = sf.forecast(df=train_df, h=len(test_df), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5114fcd-9c24-49ca-8529-6b7893d013c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e4fd7938-c914-4fe4-9678-9fe55cfd1ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "crossvalidation_df = sf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=horizon, \n",
    "    step_size=len(test_df),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = crossvalidation_df['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == fold]\n",
    "    \n",
    "    for model in ['AutoARIMA', 'AutoETS']:\n",
    "        # Filtra i dati per il modello corrente\n",
    "        model_data = fold_data[['y', f'{model}']]\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        # Calcola MAE e RMSE\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        \n",
    "        # Aggiungi i risultati\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': model,\n",
    "            f'MAE_{model}': mae,\n",
    "            f'RMSE_{model}': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_arima_ets = pd.DataFrame(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "417eef61-1b56-4834-ae4f-01802feb7d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df['unique_id'].unique():\n",
    "    series_data = train_df[train_df['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data[f'MAE_{model}'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data[f'RMSE_{model}'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef51ef-7c9e-4be4-b4e1-44cd1fae936d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_intervals #EXPANDING WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12843255-bc3b-4135-96cd-bfeec01d5919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "test_pred = sf.forecast(df=train_df, h=len(test_df), level=[95])\n",
    "Y_hat_df = test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060f63f7-79c6-4c44-a811-bff6bca15d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_arima = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoARIMA'],train_df['y'], \n",
    "                                modelname=\"ARIMA\").round(10)\n",
    "metriche_ets = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoETS'],train_df['y'], \n",
    "                                modelname=\"ETS\").round(10)\n",
    "metriche_arima, metriche_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48f02a9-5393-4873-a890-cbc5addc6587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(Y_hat_df, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# # Intervallo di confidenza AUTOARIMA \n",
    "# if 'AutoARIMA-lo-95' in forecast_df_etsA.columns and 'AutoARIMA-hi-95' in forecast_df_etsA.columns:\n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-lo-95'], \n",
    "#         mode='lines', line_color='green', showlegend=False))\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-hi-95'],\n",
    "#         mode='lines', fill='tonexty', fillcolor='rgba(200,200,200,0.2)',\n",
    "#         line_color='green', name='95% CI AutoARIMA'\n",
    "#     ))\n",
    "\n",
    "# # Intervallo di confidenza AUTOETS \n",
    "# if 'AutoETS-lo-95' in forecast_df_etsA.columns and 'AutoETS-hi-95' in forecast_df_etsA.columns:\n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-lo-95'], \n",
    "#         mode='lines', line_color='orange', showlegend=False))\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-hi-95'],\n",
    "#         mode='lines', fill='tonexty', fillcolor='rgba(255,165,0,0.15)',\n",
    "#         line_color='orange', name='95% CI AutoETS'))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "079f7036-e696-4442-8bb6-bab00cd7cb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05483717-e33f-4a01-9f6c-de5e4a1a1900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c1dc67-84b8-408c-a184-1d2a75fde9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.9)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a159a48-a5fe-4f4f-af7f-ca6d0698914f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1ad0a9-5e15-4ad2-b816-f3a1e854220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5462a142-71c5-4261-a3ca-495e03ee5c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  # Puoi modificarlo se usi più modelli\n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "58293341-9c2f-4c94-9f13-eac6abdfe188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8493cab-4729-4a8c-9b4a-bb671a8a83ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc82b90-ff76-4622-8334-f0737aefd140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11902e40-2356-4b8d-a950-4c9ca8cecc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial (best parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c547f5-2741-40c9-9e9b-366548bd90dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label=f'Solar energy')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff71b1c0-7d45-4537-824f-f29423941933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8da68c-51bc-4277-ab5d-ea905a87f9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggiorniamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef14f207-6ae0-4cb3-95c4-11fd1b6bff23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69e8064-8e48-490c-8e3b-e22655bdd62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MULTI-STEP FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186cc686-fc30-46ba-90e4-ab074535fc2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(minutes=15), periods=horizon, freq='15min') #dopo modificare\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ff6dcfd9-bd04-41bd-b9af-5a90e75fcfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "#forecast_xgb = direct_multistep_forecast(train_df_xgb, feature_cols, target_col='y',horizon=len(test_df_xgb)) #306"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f16704-4d6c-4473-bf6c-bc71f7067405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RECURSIVE FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97dd420f-c05f-43de-9e21-9cccbbc55f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4ae3b7-3cff-42d7-afab-b555d2fce11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d245178-b97a-4c72-90cf-c73fa7db8be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a07df00c-a460-4943-ae48-32f3083da9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7c2d8a-c7de-487d-afcc-451b4323d64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85629e20-6f14-4c8e-a8d9-b4405b41d729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'], \n",
    "                                modelname=\"XGBoost\").round(10)\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac8019e-6747-4740-a1a9-785dece57a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8b63b3-54a4-4fa7-b1b8-77829039970f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df_lstm) * 0.9)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bedcf9c0-dd91-477a-92b6-77a198a2a23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "h = len(test_df_lstm)\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    cv_metrics = []\n",
    "    # Hyperparametri da ottimizzare\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(1.75*h), int(3*h)] )\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 96, 128])\n",
    "\n",
    "    # Fisso il seed per riproducibilità\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=h,  # nel tuning\n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=100, #aumentare a 150\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='15min')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce53d222-38a9-42f6-b5e7-55f80d953fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13df796-8f36-41c3-a66e-7f1de820921a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217d58af-1e93-473b-b233-b1797a52cafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo i best params trovati da Optuna\n",
    "# with open('pickles/LSTM_bestPar_SolarEnergy.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9834bfc-009e-4176-9c57-e4d7ca5abc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d020931d-11a2-48b9-9f2f-a7f27feca3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_SolarEnergy.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00563770-044d-49f2-9c07-08cba7f6494b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=3*len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=100,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=0.01,\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='15min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cee85f4-aa2a-46ea-bbe5-23ca3ea4a7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm,\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_SolarEnergy.pkl', 'wb') as file:\n",
    "#      pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_SolarEnergy.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10924782-1b08-409a-adcb-26a9fbb7dac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "465bff32-4f56-44b8-a559-96eb1646d97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50, #aumentare a 150\n",
    "    early_stop_patience_steps=15,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01, \n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='15min')\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f75e5825-2c86-4f1d-8bb1-035a5e2d4e96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11b309d-7d13-4526-b944-b5b16d37d919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)\n",
    "# salvo previsioni in pickle\n",
    "Y_hat_df_lstm.to_pickle('pickles/LSTM_pred_solarEnergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1ea230-d41d-4a44-8bb1-c99d8f5972ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni\n",
    "Y_hat_df_lstm = pd.read_pickle('pickles/LSTM_pred_solarEnergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c52d460-b8fb-45ad-8903-c1b6c46785b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "#     name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2e8465-c7ea-419d-acb3-dc53878d1a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],\n",
    "                                 train_df_lstm['y'],modelname=\"LSTM\").round(10)\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdaa00c-675d-4773-be68-a547ff9c5287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34735e9-4012-4d6e-9805-a64ffabd5e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np.drop(columns=['unique_id'])\n",
    "df_np.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6411655-02dd-44c6-a603-4469bd40bad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Controllo gli zeri nel dataframe\n",
    "df = df_np.copy()\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# 2. Definisci gli orari di interesse\n",
    "start_time = pd.to_datetime('18:45').time()\n",
    "end_time = pd.to_datetime('05:45').time()\n",
    "\n",
    "# 3. Crea una maschera booleana per selezionare le righe nell'intervallo orario\n",
    "mask = (df['ds'].dt.time >= start_time) | (df['ds'].dt.time <= end_time)\n",
    "\n",
    "# 4. Applica la maschera al DataFrame per ottenere le righe nell'intervallo orario\n",
    "df_notturno = df[mask]\n",
    "\n",
    "# 5. Verifica se ci sono valori diversi da zero nella colonna 'y' nel subset notturno\n",
    "valori_non_zero_notte = df_notturno[df_notturno['y'] != 0]\n",
    "\n",
    "# 6. Stampa il risultato\n",
    "if not valori_non_zero_notte.empty:\n",
    "    print(\"Sono stati trovati valori diversi da zero nella colonna 'y' tra le 18:45 e le 05:45 (estremi inclusi):\")\n",
    "    print(valori_non_zero_notte)\n",
    "else:\n",
    "    print(\"Non sono stati trovati valori diversi da zero nella colonna 'y' tra le 18:45 e le 05:45 (estremi inclusi).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30026be9-d0f9-4f31-b5db-6c77af9fbe7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83262c9a-2b1d-4d13-8696-c6a2166eb567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e58fd7d-6704-485b-ac26-eaef3efa4a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION',\n",
    "       'giorno_settimana', 'ora_del_giorno', 'lag_1', 'lag_2',\n",
    "       'lag_4', 'lag_24', 'lag_36', 'lag_48', 'lag_96', 'diff_1', 'diff_4',\n",
    "       'diff_96', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.001, \n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "139b959a-9ba8-4e93-85fb-5a0b93eb191c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccfad79e-685e-4bc2-a05c-90aa4d378f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "initial_train_size = 1223\n",
    "h = 306  # dimensione test\n",
    "n_windows = 5\n",
    "\n",
    "# Verifica\n",
    "required_length = initial_train_size + h * n_windows\n",
    "if len(np_df_train) < required_length:\n",
    "    raise ValueError(\"Dataset troppo corto per questo schema di cross-validation.\")\n",
    "\n",
    "# Reset risultati\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = np_df_train.iloc[:train_end]\n",
    "    test_window = np_df_train.iloc[test_start:test_end]\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.001,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "        global_normalization=True,\n",
    "        unknown_data_normalization=True\n",
    "    )\n",
    "\n",
    "    for reg in regressors:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"15min\", epochs=100)\n",
    "\n",
    "    # Previsione\n",
    "    forecast = model.predict(test_window)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot\n",
    "results_df_np.plot(x='Fold', y=['MAE_nprophet', 'RMSE_nprophet'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "547faab9-5d1a-4521-82bb-fef36279c29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo date\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5eaff1f-1390-4d11-9b60-755618c4380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e9732b-7722-4156-bdaf-041539e3c7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc333a42-1cab-489f-a34b-b3fa7700a867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION',\n",
    "       'giorno_settimana', 'ora_del_giorno', 'lag_1', 'lag_2',\n",
    "       'lag_4', 'lag_24', 'lag_36', 'lag_48', 'lag_96', 'diff_1', 'diff_4',\n",
    "       'diff_96', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "    \n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],\n",
    "    learning_rate=0.005,\n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber',\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc37086-bde0-4c55-9b1a-bfe50e33d91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "#fit \n",
    "metrics_df = neuralprophet.fit(np_df_train, \n",
    "                               freq=\"15min\",\n",
    "                               epochs=150,\n",
    "                               )\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e063218e-04b9-41d8-8c6e-5236b4e31816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd9a500-638c-4bff-b855-5a0d4879918c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(np_df_test)\n",
    "#salvo in pickle\n",
    "#forecast_test.to_pickle('pickles/NP_solarEnergy.pkl')\n",
    "forecast_test.tail() #last forecasts available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9464f4-5002-4d9c-b26b-709af3014583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c9b199-585e-4f3d-b8c8-909c169728b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni\n",
    "#forecast_test = pd.read_pickle('pickles/NP_solarEnergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74543647-18d3-47ef-b058-1bd2eb57fce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338d8307-9122-4a85-a9b8-248c1f64976a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7424f536-5e65-4d5b-8b8b-3e51b7ac6b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a5339b-5928-4405-9242-1185b37c66ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT (H=1, ricorsivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02399840-30bf-4e8e-a288-7c2f759abba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "train_size = int(len(df_tft) * 0.9)\n",
    "train_df_tft = df_tft.iloc[:train_size]\n",
    "test_df_tft = df_tft.iloc[train_size:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a1c101-0ccb-4188-ad15-4dc4bcc5bf62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_df_tft['unique_id'] = 'Serie unica'\n",
    "test_df_tft['unique_id'] = 'Serie unica'\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b3ecb8-8784-40ad-8dc9-c29965eb89d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d047ea35-a677-4d07-908c-771eaf59f0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(1.5*h), int(2*h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='15min')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f579d15-a59e-407f-9fcb-75741c1aae1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68098d00-cd21-4232-b969-b709b8e745ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params\n",
    "#salvo i best params in pickle\n",
    "# with open('pickles/TFT_bestPars_SolarEnergy.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47dc0746-7316-454a-a0fb-d8167a0027ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico i best params\n",
    "with open('pickles/TFT_bestPars_SolarEnergy.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264c8d9b-de2d-4f6e-8a39-c958c5cbdb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La CV è troppo lenta, si blocca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58896a89-d4b0-4e4e-980f-b3acc7310f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(best_params['input_size']/2),\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"15min\")\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft[1100:],\n",
    "    step_size=int(len(test_df_tft)/3),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3558a899-c412-4dab-b70d-6e61c687bf33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_cv_metrics_df_SolarEnergy.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(cv_metrics_df_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae83f03-d569-4796-bb46-8e4dc2ed895f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carico la cv\n",
    "with open('pickles/TFT_cv_metrics_df_SolarEnergy.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "452c2aff-fdf7-46c7-b58d-21b51d05b3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_tft['ds'], train_df_tft['y'], label='Serie unica')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_TFT'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_TFT'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2c94df-e256-4ce3-bd9a-5903149c7d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a988394-6725-4d86-ac6d-702d2c8baf6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model fit, LENTO\n",
    "h= len(test_df_tft)\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        TFT(\n",
    "            h=h,\n",
    "            input_size=int(h/1.5),\n",
    "            hidden_size=4,  # Deve essere divisibile per 4\n",
    "            grn_activation=\"ReLU\",  # Prima era ELU\n",
    "            rnn_type=\"lstm\",\n",
    "            n_rnn_layers=1,  # Consigliato fra 2 e 5\n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "            learning_rate=0.01,\n",
    "            hist_exog_list=train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "            #futr_exog_list=future_features,\n",
    "            max_steps=100,\n",
    "            val_check_steps=10,\n",
    "            batch_size=4,\n",
    "            early_stop_patience_steps=15,\n",
    "            scaler_type=\"standard\",\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",\n",
    "        ),\n",
    "    ],\n",
    "    freq='15min',\n",
    ")\n",
    "import time\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft[1000:], val_size=h)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f989af25-e669-4ab4-92c9-97e349fd9093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4625689e-c9e8-49f9-b301-dc9e97491f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo le prediction\n",
    "# with open('pickles/TFT_SolarEnergy_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c999a443-b3d5-4859-992d-9b64d728dfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # #carico le prediction\n",
    "with open('pickles/TFT_SolarEnergy.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5aad7e1-caa0-4369-a3c5-ea77f3e78904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e746b1d-531f-400b-8880-c02107d26da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc0d232e-769f-4c72-b398-8a29de906a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6650f9c1-8d87-44e4-847b-ed56276c33ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4f11bf-8d86-4207-9f93-7664e4c484b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = nf.models[0].attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f04bc40-a4b7-4ae6-9324-cbad06c9b525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(nf.models[0], plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6564d6f2-bded-4ec0-9ed1-8c05512d4946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF ALL FUTURE TIME STEPS\n",
    "plot_attention(nf.models[0], plot=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7f324c-db08-4f43-8143-da75bb19a309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(nf.models[0], plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05e45f6-bdc7-49a6-b6e3-5b62e1731df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(nf.models[0], plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393d8727-41e8-4e79-b9b2-651a8e976e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = nf.models[0].feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6841b56-6280-44a1-b3a9-4f982f20cec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bfa4034-3aba-4231-980b-a719886d4baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e73566-c27a-45a1-a2c4-f624970cab1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8ba05d-6ac8-48a2-a632-7ec67a3aafd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10539d10-8af0-41f9-aae9-789846f2b411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b43804-f41d-48f2-963c-3736d826382d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b61403-b84f-4548-a1ae-985c8984807f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3737849-358b-46e7-9e13-781fef3b17e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a152eea-b205-4dd1-9968-a1a3bbdd039a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0bfc111-1e8b-4c1a-a1ac-b17557bf38ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c84f0a3-dbeb-4122-8add-77193c8dcae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nf.models[0].feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1685ca6b-c428-4ee0-931d-d3796e021b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0192b7ba-9ba6-435d-97fa-9fe6eb98bf8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target','unique_id':'item_id'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_chronos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c8b57ce-3fcd-4ab0-9168-9be900773691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train e test\n",
    "known_covariates_names = [\n",
    "    'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION',\n",
    "       'giorno_settimana', 'ora_del_giorno', 'lag_1', 'lag_2', 'lag_4',\n",
    "       'lag_24', 'lag_36', 'lag_48', 'lag_96', 'diff_1', 'diff_4', 'diff_96',\n",
    "       'day_sin', 'day_cos', 'hour_sin', 'hour_cos'\n",
    "]\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba83625-1a87-479c-968b-35fc86887408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9f3ab3-ad08-490f-848c-0e584926a1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad59426d-edcc-4366-9976-781f512dd7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(\"15min\"),\n",
    "        periods=h,\n",
    "        freq=\"15min\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"15min\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(\n",
    "        train_df_chronos,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    "    )\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'Fold': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "print(\"\\n=== TimeSeriesCV Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f033d65-0062-41e1-8b3a-0cda5ef5b23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12b8926-d635-4f8c-961d-b7d26f9dce3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "ax = results_df_chronos.plot(x='Fold', y=['MAE_chronos', 'RMSE_chronos'], marker='o', title='Backtesting Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e84f22ce-362a-41a4-b997-85645d190a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c347eb52-5ee4-487d-94bb-f6fd4fbea4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq=\"15min\"\n",
    ")\n",
    "\n",
    "train_df_chronos = TimeSeriesDataFrame(\n",
    "    train_df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6005303c-d008-4469-a89d-8072c3255384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI 1 CLASSICHE\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "#prediction 0 shot\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d10b396-f8af-4e66-a9a6-704e9b7b0416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # PREVISIONI 2 MULTISTEP DIRETTE ????\n",
    "# # Separo le covariate in due gruppi\n",
    "# known_ahead_covariates = ['giorno_settimana', 'ora_del_giorno']  # Queste sono realmente conosciute in anticipo\n",
    "\n",
    "# lagged_covariates = [\n",
    "#     'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION',\n",
    "#        'lag_1', 'lag_2', 'lag_4',\n",
    "#        'lag_24', 'lag_36', 'lag_48', 'lag_96', 'diff_1', 'diff_4', 'diff_96',\n",
    "#        'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'rolling_mean_4',\n",
    "#        'rolling_mean_48', 'rolling_mean_96']\n",
    "# # Configurazione del predictor\n",
    "# predictor = TimeSeriesPredictor(\n",
    "#     prediction_length=len(test_df_chronos),  \n",
    "#     eval_metric=\"MASE\",\n",
    "#     target=\"target\",\n",
    "#     known_covariates_names=known_ahead_covariates,\n",
    "#     freq=\"15min\"\n",
    "# )\n",
    "\n",
    "# test_known_covariates = test_df_chronos[known_ahead_covariates]\n",
    "\n",
    "# # Fit del modello\n",
    "# predictor.fit(\n",
    "#     train_df_chronos,\n",
    "#     hyperparameters={\n",
    "#         \"Chronos\": [\n",
    "#             {\n",
    "#                 \"model_path\": \"bolt_small\",\n",
    "#                 \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "#             },\n",
    "#             {\n",
    "#                 \"model_path\": \"bolt_small\",\n",
    "#                 \"covariate_regressor\": \"CAT\",\n",
    "#                 \"target_scaler\": \"robust\",\n",
    "#                 \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "#             },\n",
    "#         ],\n",
    "#     },\n",
    "#     time_limit=600,\n",
    "#     enable_ensemble=True,\n",
    "# )\n",
    "\n",
    "# # Previsioni appropriate al contesto\n",
    "# # Per un multi-step puro senza feedback\n",
    "# predictions = predictor.predict(\n",
    "#     train_df_chronos,\n",
    "#     known_covariates=test_known_covariates  # Solo le covariate realmente note in anticipo\n",
    "# )\n",
    "# predictions.reset_index(inplace=True)\n",
    "\n",
    "# predictions_0shot = predictor.predict(\n",
    "#     train_df_chronos,\n",
    "#     known_covariates=test_known_covariates,\n",
    "#     model=\"ChronosZeroShot[bolt_small]\"\n",
    "# )\n",
    "# predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "363b44ea-c6aa-43fb-826f-91437198a1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f5ec87-aa37-4c8a-b07f-9b3be72bd0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025579ba-26e2-4acd-9b63-3ce75e477dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c6669d-a6b3-4a41-990c-094f8f049e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef77c5e4-da58-4f08-8fad-aaaa82850353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be47ed7-34ed-49f5-9c24-1202f415be43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98d26aa0-11df-4c5d-93c1-6740e9f36047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR SOLAR ENERGY\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    " # PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='CHRONOS 0-SHOT', line=dict(color='lightgreen')))\n",
    "   \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b701e2c6-0257-4f9a-9a1c-57ea5d810f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE_AutoARIMA','RMSE_AutoARIMA']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE_AutoETS','RMSE_AutoETS']]\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)\n",
    "df_arima.rename(columns={'MAE':'MAE_AutoARIMA','RMSE':'RMSE_AutoARIMA'}, inplace=True)\n",
    "df_ets.rename(columns={'MAE':'MAE_AutoETS','RMSE':'RMSE_AutoETS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5034656-8d46-4c7e-87e7-49130a70bf7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = df_ets.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft.drop(columns=['Model']),\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_TFT', 'RMSE_TFT',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc8a6e48-91ec-46c3-bc81-18f87dfe3e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf9540a-86ff-4e09-8b1a-f99f34584e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **SOLAR ENERGY (MULTISTEP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5f565d8-8157-4d2a-887b-e913aca1c629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futr_exog_list = ['ora_del_giorno', 'giorno_settimana', 'day_sin','day_cos','hour_sin','hour_cos']\n",
    "future_features = ['ds', 'y'] + futr_exog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6223e035-63eb-4514-93ae-bbe6b32ac78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df_solar.drop(columns=['AC_POWER'])\n",
    "df = df.reset_index().rename(columns={'DATE_TIME': 'ds','DC_POWER':'y'})\n",
    "df = df.dropna()\n",
    "df['unique_id'] = 'serie_1'\n",
    "h= 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f6e460b-3484-4472-92c2-617dc1a3caaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mancano due righe di notte, le fillo easy\n",
    "row_to_copy = df[df['ds'] == '2020-06-17 06:00:00']\n",
    "# Crea copie modificate per le 06:15 e 06:30\n",
    "row_0615 = row_to_copy.copy()\n",
    "row_0615['ds'] = pd.to_datetime('2020-06-17 06:15:00')\n",
    "\n",
    "row_0630 = row_to_copy.copy()\n",
    "row_0630['ds'] = pd.to_datetime('2020-06-17 06:30:00')\n",
    "\n",
    "df = pd.concat([df, row_0615, row_0630], ignore_index=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df = df.sort_values(by='ds').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ac40d938-0c5c-4c39-93cd-6cc928a9c3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Controllo gli zeri nel dataframe\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# 2. Definisci gli orari di interesse\n",
    "start_time = pd.to_datetime('18:45').time()\n",
    "end_time = pd.to_datetime('05:45').time()\n",
    "\n",
    "# 3. Crea una maschera booleana per selezionare le righe nell'intervallo orario\n",
    "mask = (df['ds'].dt.time >= start_time) | (df['ds'].dt.time <= end_time)\n",
    "\n",
    "# 4. Applica la maschera al DataFrame per ottenere le righe nell'intervallo orario\n",
    "df_notturno = df[mask]\n",
    "\n",
    "# 5. Verifica se ci sono valori diversi da zero nella colonna 'y' nel subset notturno\n",
    "valori_non_zero_notte = df_notturno[df_notturno['y'] != 0]\n",
    "\n",
    "# 6. Stampa il risultato\n",
    "if not valori_non_zero_notte.empty:\n",
    "    print(\"Sono stati trovati valori diversi da zero nella colonna 'y' tra le 18:45 e le 05:45 (estremi inclusi):\")\n",
    "    print(valori_non_zero_notte)\n",
    "else:\n",
    "    print(\"Non sono stati trovati valori diversi da zero nella colonna 'y' tra le 18:45 e le 05:45 (estremi inclusi).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be0747ec-1866-43a6-aa59-9fd6f7196530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 2, 4, 24, 36, 48, 96]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features, 1, 4, 96\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1) #diff tra t-1 e t-2\n",
    "    df['diff_4'] = (df[target_col] - df[target_col].shift(4)).shift(1) #diff tra t-1 e t-5\n",
    "    df['diff_96'] = (df[target_col] - df[target_col].shift(96)).shift(1) #diff tra t-1 e t-97\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_4'] = df[target_col].rolling(window=4).mean().shift(4) #ultima ora\n",
    "    df['rolling_mean_48'] = df[target_col].rolling(window=6).mean().shift(1) #ultime 12 ore\n",
    "    df['rolling_mean_96'] = df[target_col].rolling(window=12).mean().shift(1) #ultimo giorno\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63261621-a14a-47e8-ba1b-66aa5081fbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4877bd33-c79c-43df-9043-bdb959539576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag\n",
    "hist_exog_features = df_xgb.drop(columns=['ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "df_xgb = df_xgb[future_features]\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b617128-a4c6-4547-a2c9-5181f1a57428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_df_xgb = df_xgb.iloc[:-h]\n",
    "test_df_xgb = df_xgb.iloc[-h:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=df_xgb['ds'], y=y_train_xgb, mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=y_test_xgb, mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f52a95f-6e4f-4a72-9ef2-139ce24cae53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20830e2b-5b95-44af-8e66-9ffce27b016e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e304cc7a-aa70-4311-8f40-bf92d669cb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  # Puoi modificarlo se usi più modelli\n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05241d8-3f35-49d1-9198-db950ddbea67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75900711-7432-4f2e-bee6-a8a378f3402f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7496cd-2b13-426c-8544-d714e0742a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5443c94b-908f-457b-b97e-ebae4fb834e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial (best parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71798de6-f8de-40d0-8eb8-307533e8e4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label=f'Solar energy')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df65ec4-093c-47bf-8fe6-752ba0e1436e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de868063-4be7-4534-953d-6c4f356f5c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggiorniamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "import time\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d3b55c-fdcb-4449-aa16-a272bda169a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a240ae20-7985-41b7-9cb1-ed141bdb435b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625c61d2-9a0a-499e-95d4-12d61d12bffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c284c39d-e438-414e-8702-39cc91541977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9e2c4f-c961-4722-bbdd-50bb3eaba6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0684ac30-e434-4dbc-a177-a345b8f9560f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2c5873-e142-4069-8af1-d357eeeb935f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'], \n",
    "                                modelname=\"XGBoost\").round(10)\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09088129-0795-4732-8e85-24a33dda4fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "399b51e5-2d02-4338-ab33-75496dbf6060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_df_lstm = df_lstm.iloc[:-h]\n",
    "test_df_lstm = df_lstm.iloc[-h:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff893d2f-8ac9-4577-8527-74a466ae5a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h), int(h*2), int(h*5)])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=len(test_df_lstm), \n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='15min')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=len(test_df_lstm),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc369351-55de-4720-8a6d-e541302f031e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a14269-7a64-44a7-a3e6-b2d7741385b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dc5357-7280-45ee-943f-58a7af20800b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo i best params trovati da Optuna\n",
    "with open('pickles/LSTM_bestPar_SolarEnergy_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4841df-d4b6-41da-a76a-41b0914b3026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd23fa62-838b-4618-8c12-a92c0d9796f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_SolarEnergy_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ed75ffe-359f-4fa2-8d75-11337ddcb162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=3*len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=100,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=0.01,\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='15min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af92bb6-6a4d-46b6-ba33-49df6eed1971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_lstm,\n",
    "    step_size=len(test_df_lstm),\n",
    "    n_windows=5\n",
    ")\n",
    "with open('pickles/LSTM_CV_SolarEnergy_multistep.pkl', 'wb') as file:\n",
    "     pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_SolarEnergy_multistep.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7dfac1f-565e-41cc-94b3-f28d36406998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d0a368-f54b-4f29-ab31-58ab61b003d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='standard',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=200,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=0.01, #best_params['learning_rate']\n",
    "    recurrent=True\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='15min')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3dbc3f6-8360-415c-8ce0-c47f646ae06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884ed736-53d5-46b4-830b-7cbc8713ca0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])\n",
    "# salvo previsioni in pickle\n",
    "Y_hat_df_lstm.to_pickle('pickles/LSTM_solarEnergy_multistep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f5b0bc-c849-4c40-bde3-4c82bf78b357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni\n",
    "Y_hat_df_lstm = pd.read_pickle('pickles/LSTM_solarEnergy_multistep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3557237-2e85-4b3e-b72a-adb1cd320241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "#PREVISIONI MEDIANA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536bf721-ddff-4a36-ab92-2329bc8a7298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],\n",
    "                                 train_df_lstm['y'],modelname=\"LSTM\").round(10)\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "198100da-a69c-4014-88d3-da0da832e827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aea478c-6906-4a12-9a6b-5d6c4921bd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1b88bf1a-c6b4-43e1-8b71-cd01a957393d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f591486-b740-45e9-826b-e84694b5c6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "initial_train_size = len(train_df_np) - 5 * len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = train_df_np.iloc[:train_end].copy()\n",
    "    test_window = train_df_np.iloc[test_start:test_end].copy()\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello NeuralProphet\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    # Aggiungi regressori\n",
    "    for reg in futr_exog_list:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"15min\", epochs=100)\n",
    "\n",
    "    # Predizione\n",
    "    future_df = test_window.copy()  # Deve contenere anche i regressori futuri\n",
    "    forecast = model.predict(future_df)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati CV\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot Errori\n",
    "results_df_np.plot(\n",
    "    x='Fold',\n",
    "    y=['MAE_nprophet', 'RMSE_nprophet'],\n",
    "    marker='o',\n",
    "    title='TimeSeriesCV Errors',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in train_df_np.columns:\n",
    "        start_date = train_df_np['ds'].iloc[test_start]\n",
    "        end_date = train_df_np['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6cfafb-d695-4cf3-b9d4-840929246ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297ad349-a4bd-4bc1-9b9d-a6bacbd85dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"15min\",epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ef55b0-932a-45e1-a8a9-70951339d6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb56cf7f-33dd-4b7c-9ceb-882275c081ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast = neuralprophet.predict(test_df_np[future_features])\n",
    "#salvo in pickle\n",
    "forecast.to_pickle('pickles/NP_solarEnergy_multistep.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6517a29d-16f4-4f5c-a412-6f46b2e07cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ac0e610-fcbc-4cc3-918e-ec995f547e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni\n",
    "forecast_test = pd.read_pickle('pickles/NP_solarEnergy_multistep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e79d7b-df59-4870-b7cc-35436f7339e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70178ab7-4e45-4190-8efe-d472af3dedba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9017f115-8c45-4ae7-9bbc-5a6a4e39c565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "619759bf-ef7a-4670-b470-afeed5f3c3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d26cc2d1-7a88-40ba-a608-1e99caffa5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "hist_exog_features = df_tft.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "# 2. Suddivisione in train/test\n",
    "train_df_tft = df_tft.iloc[:-h]\n",
    "test_df_tft = df_tft.iloc[-h:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d561dd03-330d-4fb8-b3c8-7dba787d1b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d906020a-f440-4309-b57f-aaa04692a9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a997ecba-7429-43fa-a958-afe1e0990135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(4*h), int(2*h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='15min')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e6e3c144-1f07-4770-9958-9f37959b91fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21f0cd5-3c93-4f90-b0b0-a73efbcc7503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f84817-2c63-4fdd-98ee-f4f2116dae26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_BestPars_SolarEnergy_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903af72e-b921-481e-8966-d9174c1ac30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico i best params\n",
    "with open(\"pickles/TFT_BestPars_SolarEnergy_multistep.pkl\", 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f4dd134a-cd9e-4e11-aa76-5e9232181362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(h/2), \n",
    "        hidden_size=32,\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list = hist_exog_features,\n",
    "        max_steps=200, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=8,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=8\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"15min\")\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft,\n",
    "    step_size=len(test_df_tft),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca47521-ca79-431f-b085-bebf715f6bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_CV_SolarEnergy_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(cv_metrics_df_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8754f18e-40d0-42b7-a2a1-851c91b6ab60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico la CV\n",
    "with open('pickles/TFT_CV_SolarEnergy_multistep.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "310452c5-856b-43ba-aeb5-29b47b1ac788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_tft['unique_id'].unique():\n",
    "    series_data = train_df_tft[train_df_tft['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_TFT'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_TFT'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45e7acd-3029-4e32-8583-5933dc5f2d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba9e443-d7ae-4c90-92b9-75ce47620ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "import time\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size']*2,\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=500, #aumentare a 100\n",
    "        val_check_steps=25,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"15min\")\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=len(test_df_tft))\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "571d1a81-ce72-463f-90b7-ff43aaef06cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft[['unique_id','ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad807ad-e7f0-4bd7-b28d-aaf543ef21ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo le pred in pickle\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_pred_SolarEnergy_multistep.pkl\"\n",
    "# with open(save_path, 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87b01251-9566-4479-8f17-a18608bacc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le pred\n",
    "with open(\"pickles/TFT_pred_SolarEnergy_multistep.pkl\", 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d2183759-06ea-4576-bb14-c8edd80a6b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee1e3324-e015-4634-bc24-1b7e810e6be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fde4e3c7-ecc5-4145-be5e-07fed78f0ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dea92bb-efbb-4191-985a-5d3bb10d2bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe536fe-d589-4c8b-a145-ec184dd110e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tft_model = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587ad9f2-7f4c-4e70-90ca-2436debfa9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # NON MI FA SALVARE IL MODELLO PERCHE' TROPPO PESANTE\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_model_SolarEnergy_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(tft_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22fbf626-3de7-4fb4-a925-ed8798e0d879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # carico il modello tft\n",
    "# with open('pickles/TFT_model_SolarEnergy_multistep.pkl', 'rb') as file:\n",
    "#     tft_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a00cf5e5-fbce-4a51-99d8-73b7a14d8e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = tft_model.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde3a31f-d14a-4c4d-be6f-b8efd8ef3703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(tft_model, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4bffc2-0742-4768-8af0-d94495811b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(tft_model, plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ebe468a-bf16-4182-a332-04f2f2ba2851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(tft_model, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f380121-e648-4f5c-a2fc-7d258a5f51ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = tft_model.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84379fd-9afa-4982-8fd6-76497f1e2a1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31bcebf1-9264-4f6d-bea6-89768f3d42f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40cae41e-a9d1-4f9c-8cfa-5d749891eb2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db93be3-5e03-4ba7-9e94-666a067e46bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee70ce0-e8b0-4a22-a2a2-b2278302fb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fd791a2-ed8b-4230-97af-07a16e74e80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bbadc29-df3d-47ea-826d-6a59efdbdb90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f379d4-2ba5-457f-8554-eeedeafca895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab566e7-b161-4dd0-a4e1-893975a05d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf63ae9-b9c1-4085-ade4-ae74702ad2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d57dd4b-69a1-4824-8e78-bd25fb89cdf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nf.models[0].feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf567c49-f996-4fe8-b340-3d512bb0c8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4740e5d0-0e33-4bde-a76d-4b4c4ee012e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8af56bc-ea92-4c0d-b91b-b6673f2e12dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "232f5e11-c860-417b-867a-21caede406e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(minutes=15),\n",
    "        periods=h,\n",
    "        freq=\"15min\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"15min\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(\n",
    "        train_df_chronos,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    "        )\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'Fold': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('Fold',inplace=True)\n",
    "print(\"\\n=== TimeSeriesCV Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72348ac-c473-4db7-bb3c-50ab1f8db8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f942d7ee-3630-418e-9538-c666d2b9d900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='Fold', y=['MAE_chronos', 'RMSE_chronos'], marker='o', title='Backtesting Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1caf5e9-fb8a-4b61-8a78-6755200587cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparo dataset per training\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"15min\"\n",
    ")\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16aef256-265a-4424-a8d3-60636c108aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,\n",
    ")\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVISIONI MULTI STEP \n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,  # Solo le covariate realmente note in anticipo\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84bbc28-d493-4a6a-89be-89b230ab6697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45b1ab87-ba9d-4948-9c7b-c9495aad3e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb26233-b7bc-44c1-b643-92a7943ddc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51c10dd9-9f33-41ce-8183-f53049a5b3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c041adb3-3de3-4a71-b433-a73800816202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "460bc6e5-ae3d-40e2-af73-df4baaacb428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_xgb.join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c08fb89c-bd4c-4b2b-991d-4fc9db8fad13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR SOLAR ENERGY\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "#     name='TFT Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    " # PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='CHRONOS 0-SHOT', line=dict(color='lightgreen')))\n",
    "   \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6673709a-0ff9-46e4-a89d-8b3539cfec37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "results_df_tft.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = cv_metrics_df_lstm.merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_TFT', 'RMSE_TFT', #'MAE_lstm', 'RMSE_lstm',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb5a977-a046-402f-81bb-926fa84d9923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a0c5e8-6afc-4add-90d5-ce1af321983b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **OIL PRICES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6932496-f059-4053-9de8-ac92bf6aee52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "oil_prices.dropna(inplace=True)\n",
    "df = oil_prices.reset_index().rename(columns={'date': 'ds','price':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "044ed2de-9344-4bf5-9b6e-e7b6cc9cab9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    # features\n",
    "    lags = [1, 2, 4, 12]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    df['diff_12'] = (df[target_col] - df[target_col].shift(12)).shift(1)\n",
    "    \n",
    "    df['change'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['percentChange'] = df[target_col].pct_change().shift(1)\n",
    "\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_3'] = df[target_col].rolling(window=3).mean().shift(1) #ultimo trimestre\n",
    "    df['rolling_mean_6'] = df[target_col].rolling(window=6).mean().shift(1) #ultima metà dell'anno\n",
    "    df['rolling_mean_12'] = df[target_col].rolling(window=12).mean().shift(1) #ultimo anno\n",
    "    df['rolling_std_3'] = df[target_col].rolling(window=3).std().shift(1)\n",
    "    df['rolling_std_6'] = df[target_col].rolling(window=6).std().shift(1)\n",
    "    df['rolling_std_12'] = df[target_col].rolling(window=12).std().shift(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8c66583-045e-45e3-89b2-67238871fd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6390e77e-9ab9-4793-9900-9d45ebe426f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_arima = df[['y','ds','unique_id']].iloc[13:] #droppo le prime 13 per rendere paragonabile con altri modelli che utilizzano features esogene\n",
    "train_size = int(len(df_arima) * 0.9)\n",
    "train_df = df_arima.iloc[:train_size]\n",
    "test_df = df_arima.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 12 # for monthly data = 12 \n",
    "horizon = len(test_df) # number of predictions\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9115c9b-bef3-4636-bef2-ff649803cc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70da9c4-3516-499f-a91a-da3c3d541804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fit dei modelli\n",
    "models = [\n",
    "    AutoARIMA(season_length=season_length),\n",
    "    AutoETS(season_length=season_length)\n",
    "]\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='MS')\n",
    "import time\n",
    "start_time = time.time()\n",
    "# fit\n",
    "sf.fit(train_df)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90e38511-502d-446c-80d4-7b5ec70273fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dedbc4a-54c6-4c75-b656-8886bd0ad204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arima_string(sf.fitted_[0,0].model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d85cdf01-280d-4b7b-bd4d-6a0071e3cba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c46201e1-faac-4f78-942f-f6f2eb300102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast\n",
    "Y_hat_df = sf.forecast(df=train_df, h=len(test_df), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e56bb0d-0966-4b41-8a85-d854d773dc26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67680ad8-86d4-4c07-8810-17e0a68d37e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d9ff8dd-41c0-4733-bbd4-27b0c5d9130c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crossvalidation_df = sf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=horizon, \n",
    "    step_size=len(test_df),\n",
    "    n_windows=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b86c6bb-ac88-47fc-8548-042a305c59ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = crossvalidation_df['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == fold]\n",
    "    \n",
    "    for model in ['AutoARIMA', 'AutoETS']:\n",
    "        # Filtra i dati per il modello corrente\n",
    "        model_data = fold_data[['y', f'{model}']]\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        # Calcola MAE e RMSE\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        \n",
    "        # Aggiungi i risultati\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': model,\n",
    "            f'MAE_{model}': mae,\n",
    "            f'RMSE_{model}': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_arima_ets = pd.DataFrame(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26544718-56d4-4a6a-89a9-ebfeb2eb6150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df['unique_id'].unique():\n",
    "    series_data = train_df[train_df['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# fold con colori diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot MAE \n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data[f'MAE_{model}'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data[f'RMSE_{model}'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ebc299d-874d-4e8c-9779-3ef5270d9846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_intervals #EXPANDING WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acbea15c-88ba-4957-9c05-4aecade7d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's forecast on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7110cf07-f522-4e32-8ab7-8ea41a64b68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "test_pred = sf.predict(h=len(test_df), level=[95])\n",
    "Y_hat_df = test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f9563b-8186-459f-a9a9-b1395b3191f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_arima = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoARIMA'],train_df['y'], \n",
    "                                modelname=\"ARIMA\").round(10)\n",
    "metriche_ets = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoETS'],train_df['y'], \n",
    "                                modelname=\"ETS\").round(10)\n",
    "metriche_arima, metriche_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55d7bd0-3c28-42d7-ab18-489b1ed062b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(Y_hat_df, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# # Intervallo di confidenza ARIMA 95%\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=list(test_pred['ds']) + \n",
    "#     list(test_pred['ds'][::-1]),\n",
    "#     y=list(test_pred['AutoARIMA-hi-95']) + list(test_pred['AutoARIMA-lo-95'][::-1]),\n",
    "#     fill='toself',\n",
    "#     fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "#     line=dict(color='rgba(255,255,255,0)'),\n",
    "#     hoverinfo=\"skip\",\n",
    "#     showlegend=True,\n",
    "#     name='P10-P90 ARIMA'\n",
    "# ))\n",
    "\n",
    "# # Intervallo di confidenza ETS 95%\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=list(test_pred['ds']) + \n",
    "#     list(test_pred['ds'][::-1]),\n",
    "#     y=list(test_pred['AutoETS-hi-95']) + list(test_pred['AutoETS-lo-95'][::-1]),\n",
    "#     fill='toself',\n",
    "#     fillcolor='rgba(144, 228, 44, 0.3)',  \n",
    "#     line=dict(color='rgba(255,255,255,0)'),\n",
    "#     hoverinfo=\"skip\",\n",
    "#     showlegend=True,\n",
    "#     name='P10-P90 ETS'\n",
    "# ))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15)))\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa697f-b845-418b-ae39-5c3108e4e137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf.predict(h=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6f2f45-42f3-4765-8e48-e2e53ddca729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9c023f-922c-4bc3-b86a-e67ed0ca49f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 12 obs.\n",
    "df_xgb['covid'] = (df_xgb['ds'] > pd.Timestamp('2020-02-01')).astype(int)\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070d1b39-4e98-4b41-b217-5e075d7ffc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.9)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9fa67ff-1613-4d3d-a399-3338e4b91c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e633ad2-721c-4ef4-af57-f976aa2c3e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "078b2e30-fb97-4173-9aca-b2561efb4166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7b9ec2-c36c-4895-b8ad-4fe44d68279f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d77825-82e5-453c-be97-9c7df6cd2fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af224c10-b548-40f4-9364-ed742f481d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e149c26b-d8e8-4337-a095-e3a3277bea8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14f8053-5109-4eb2-ae20-07631e23227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label='Oil Price - XGB, CV')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7643cfb6-5ac7-49ff-bfc6-6ec526ccc118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora facciamo training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b0696eb-3b12-4ec1-84b1-e08ae30410fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "import time\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f938a6-6d90-4ec6-977b-78734db3267c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2bef908-3218-4128-abd4-39b9ddf88aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIRECT MULTI-STEP FORECAST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c543c2-0425-405b-b2cf-1ce692437294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon, freq='MS') #dopo modificare freq = M\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92736c58-e36e-4790-a43b-f8089f881317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Forecasting\n",
    "# forecast_xgb = direct_multistep_forecast(train_df_xgb,\n",
    "#                                          feature_cols, \n",
    "#                                          target_col='y',\n",
    "#                                          horizon=len(test_df_xgb)) #posso predire anche oltre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f29a5e-249e-4d11-bc74-bbd0c316d4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RECURSIVE FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c82aa7da-5001-4445-aca6-cccb8bfe48a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a028da87-9c06-44d3-a5e8-66231d717a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a48339f-6cf1-4aed-b9dc-f054500c3377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82140023-185b-4bee-a061-3fa4c1cffded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8840b94d-7cc0-4afe-a586-ef9b89a1e6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3193e802-2525-4356-bced-86525454eae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'],modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79146225-68b5-4bba-ad93-e89b69d7738c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8dc796-6e0f-4cce-be86-d7e45d67b10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df_lstm) * 0.9)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "559d4d69-e0a1-46c3-9b58-276ebc04011b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_lstm.copy()\n",
    "test_df_original = test_df_lstm.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_lstm[['ds', 'unique_id']]\n",
    "test_meta = test_df_lstm[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_lstm.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_lstm.index\n",
    "test_idx = test_df_lstm.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_lstm = pd.DataFrame(scaler_x.fit_transform(train_df_lstm[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_lstm = pd.DataFrame(scaler_x.transform(test_df_lstm[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_lstm = pd.concat([train_df_lstm, train_meta], axis=1)\n",
    "test_df_lstm = pd.concat([test_df_lstm, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b492d435-e180-4d1e-8909-1a112479f93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [12, 24, 36])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=12,  # nel tuning\n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=100,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='MS')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=12,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1312b48b-7255-48a6-9c8b-2f6b42733f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe00cd5c-9c82-4983-a334-df8d72ae8030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982cccb2-2043-4e08-b7cf-29bc9744e56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestpars_OilPrice.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6addcb8-187f-46ca-bb64-43a6c7768dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b583d034-37f3-4d5c-80a8-34ed4c52ca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_lstm,\n",
    "    step_size=len(test_df_lstm),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a17f89-8e29-4ebe-b58f-7ac0e582495f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8a9a5a-24d6-4f19-9b3c-9a0666abe4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e580f9a-17ad-4bd6-a1c2-e2359a8cee8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico i best params\n",
    "with open('pickles/LSTM_bestpars_OilPrice.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3caa7f69-5357-4b4f-946d-697122e4a710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='MS')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d23449-30e1-4d9b-999b-6f3daf595ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora faccio predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c866e74-17d6-48b8-8f39-3b40c708212c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34e7926-5bd8-49f6-92fb-adeb41ea3ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_lstm['LSTM'] = Y_hat_df_lstm['LSTM'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-median'] = Y_hat_df_lstm['LSTM-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-lo-95'] = Y_hat_df_lstm['LSTM-lo-95'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-hi-95'] = Y_hat_df_lstm['LSTM-hi-95'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756d1398-a48a-40a0-9420-206e946d1c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Intervallo di confidenza 95%\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_lstm['ds']) + \n",
    "    list(Y_hat_df_lstm['ds'][::-1]),\n",
    "    y=list(Y_hat_df_lstm['LSTM-hi-95']) + list(Y_hat_df_lstm['LSTM-lo-95'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73e82dbb-67be-4e8c-a538-645039c37d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_original['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5956272-6944-48bf-82f8-46ad0f6fd926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65420adc-84ae-4f5a-91b3-25e05eab3785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np.drop(columns=['unique_id'])\n",
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e52ba89-4955-443c-a8b8-230321bfb6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4704c2b0-0f8e-423e-81a6-c568bb29d063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d83952-45ad-4bca-a137-5402fa030dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['percentChange', 'change', 'lag_1', 'lag_2', 'lag_4', 'lag_12',\n",
    "       'diff_12', 'month', 'quarter', 'year', 'rolling_mean_3',\n",
    "       'rolling_mean_6', 'rolling_mean_12', 'rolling_std_3', 'rolling_std_6',\n",
    "       'rolling_std_12']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703982ab-5f42-4d29-a0fb-7cd52b9f1dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dddd29e0-6861-47d0-951a-fd85dc172006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Reset risultati\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = np_df_train.iloc[:train_end]\n",
    "    test_window = np_df_train.iloc[test_start:test_end]\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    for reg in regressors:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"MS\", epochs=150)\n",
    "\n",
    "    # Previsione\n",
    "    forecast = model.predict(test_window)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot\n",
    "results_df_np.plot(x='Fold', y=['MAE_nprophet', 'RMSE_nprophet'], marker='o', title='TimeSeriesCV Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010901be-7b45-4027-a67f-ac5006848c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a56aa8-6c93-45a3-9d19-c1c42da1ffb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6fa4d6b-41db-4060-a8df-ef36d84e6c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19c2479-bf0d-4567-b0d7-5223566f1de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['percentChange', 'change', 'lag_1', 'lag_2', 'lag_4', 'lag_12',\n",
    "       'diff_12', 'month', 'quarter', 'year', 'rolling_mean_3',\n",
    "       'rolling_mean_6', 'rolling_mean_12', 'rolling_std_3', 'rolling_std_6',\n",
    "       'rolling_std_12']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e482c6af-015b-4760-9a82-44125aec3ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definisco il modello\n",
    "\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "149959b5-7f11-45ea-bbcd-aaa56d9c862f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "#fit \n",
    "metrics_df = neuralprophet.fit(np_df_train, \n",
    "                               freq=\"MS\",\n",
    "                               epochs = 200)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3206710e-1f3f-49ca-8175-c9f2ba5b4c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d7d5729-496e-4bef-8846-10355fc9b3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(np_df_test)\n",
    "forecast_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d8dabf-106a-4667-9fec-317c11a58c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # previsioni OUT-OF-SAMPLE, per ora non le utilizzo\n",
    "# #utilizzando solo il mese, il trimestre e l'anno -->> non preciso\n",
    "# new_neuralprophet = NeuralProphet(\n",
    "#     quantiles=[0.025, 0.975],    \n",
    "#     learning_rate=0.001, \n",
    "#     batch_size=64,\n",
    "#     daily_seasonality=True,\n",
    "#     weekly_seasonality=True,\n",
    "#     loss_func='Huber'\n",
    "# )\n",
    "# new_neuralprophet.add_future_regressor(\"month\")\n",
    "# new_neuralprophet.add_future_regressor(\"quarter\")\n",
    "# new_neuralprophet.add_future_regressor(\"year\")\n",
    "\n",
    "# # Addestra il nuovo modello con solo le feature che hai disponibili per il futuro\n",
    "# new_neuralprophet.fit(\n",
    "#     df=np_df_train[['ds', 'y','month', 'quarter','year']],\n",
    "#     freq='MS',\n",
    "#     epoches=300\n",
    "# )\n",
    "\n",
    "# # Ora puoi fare previsioni senza la temperatura\n",
    "# future_dates = pd.date_range(\n",
    "#     start=df_np['ds'].max() + pd.Timedelta(days=1),\n",
    "#     periods=36,\n",
    "#     freq='MS'\n",
    "# )\n",
    "\n",
    "# future_regressors = pd.DataFrame({\n",
    "#     'ds': future_dates,\n",
    "#     'month': future_dates.month,\n",
    "#     'quarter': future_dates.quarter,\n",
    "#     'year': future_dates.year\n",
    "# })\n",
    "\n",
    "# future = new_neuralprophet.make_future_dataframe(\n",
    "#     df=df_np[['ds', 'y', 'month', 'quarter','year']],\n",
    "#     periods=36,\n",
    "#     regressors_df=future_regressors\n",
    "# )\n",
    "# forecast_future = new_neuralprophet.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac10f6c2-698a-42d7-b672-d11726354e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba282d6-82b5-4896-b868-5e1be53c5f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# # Out-of-sample forecast (future)\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=forecast_future['ds'], \n",
    "#     y=forecast_future['yhat1'],\n",
    "#     mode='lines', \n",
    "#     line=dict(color='green'),\n",
    "#     showlegend=False,  # Evita duplicati nella legenda\n",
    "#     name=\"Out-of-sample\"\n",
    "# ))\n",
    "\n",
    "#Intervallo di confidenza al 95%\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(forecast_test['ds']) + \n",
    "    list(forecast_test['ds'][::-1]), \n",
    "    y=list(forecast_test['yhat1 2.5%']) + list(forecast_test['yhat1 97.5%'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ea01d6-d450-4627-82ec-064b552e344e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'], train_df_original['y'],modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4574f3b6-1544-4c07-964e-55c3ec6e98de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ada4777c-5b17-4a8a-a758-e47e546ca176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "train_size = int(len(df_tft) * 0.9)\n",
    "train_df_tft = df_tft.iloc[:train_size]\n",
    "test_df_tft = df_tft.iloc[train_size:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "63d500a9-4d9d-48a5-b651-566bcb24ada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9886813-2624-446c-a990-9afe639e0cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (TROPPO LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a2afbe7-2246-4117-99a6-a38e967ab681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(1.5*h), int(2*h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=40, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='MS')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "176b220b-9f27-40c3-b02b-3c55c212ab82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "h = len(test_df_tft)\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        TFT(\n",
    "            h=h,\n",
    "            input_size=h,\n",
    "            hidden_size=60,  # deve essere divisibile per 4\n",
    "            grn_activation=\"ReLU\", #prima era ELU\n",
    "            rnn_type=\"lstm\",\n",
    "            n_rnn_layers=4,  #consigliato fra 2 e 5; \n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "            learning_rate=0.005, \n",
    "            futr_exog_list=train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "            max_steps=50, #aumentare su databricks\n",
    "            val_check_steps=10,\n",
    "            batch_size=96, \n",
    "            #early_stop_patience_steps=15,  \n",
    "            scaler_type=\"robust\",\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",  # Utilizza GPU se disponibile\n",
    "        ),\n",
    "    ],\n",
    "    freq='MS',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8d0f04db-3d94-4042-97f3-9ac0d79beef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft,\n",
    "    step_size=h,\n",
    "    n_windows=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3396b24-9fe3-4b6a-9d00-40a1a0c4765d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3f353fe7-a1ad-4939-b03c-e61724ee0b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "#series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "plt.plot(train_df_tft['ds'], train_df_tft['y'], label='Serie unica')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_TFT'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_TFT'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dae1547-96e7-441c-af92-eaf8799481c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "03143ea2-f8e8-4baf-bf49-94532e49a00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model fit, LENTO\n",
    "h = len(test_df_tft)\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        TFT(\n",
    "            h=h,\n",
    "            input_size=h,\n",
    "            hidden_size=60,  # deve essere divisibile per 4\n",
    "            grn_activation=\"ReLU\", #prima era ELU\n",
    "            rnn_type=\"lstm\",\n",
    "            n_rnn_layers=4,  #consigliato fra 2 e 5; \n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "            learning_rate=0.005, \n",
    "            futr_exog_list=train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "            max_steps=200, #aumentare su databricks\n",
    "            val_check_steps=10,\n",
    "            batch_size=96, \n",
    "            early_stop_patience_steps=15,  \n",
    "            scaler_type=\"robust\",\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",  # Utilizza GPU se disponibile\n",
    "        ),\n",
    "    ],\n",
    "    freq='MS',\n",
    ")\n",
    "import time\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=2*h)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d8f57bda-ce16-45b0-ba3b-2f21a7ebb478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2741ce78-6539-43cc-94d1-8cbc4067a114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_pred_OilPrice.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(Y_hat_df_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c8955b4b-7ea6-40d3-87ac-75d9b4788c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Rescaling dei dati\n",
    "# target_col_position = feature_cols.index('y')\n",
    "# min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "# max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# # Trasformazione inversa manuale\n",
    "# Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "# Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "# Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "# Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c164a4-76ab-49b1-95ec-c2f1ef95d378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni:\n",
    "with open('pickles/TFT_pred_OilPrice.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9862ab47-858a-48e2-aeae-fae477ff1d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bacdc5-9f96-4601-bb31-8efb91c509a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ffb1e03-7696-4c10-95f2-487cf62375cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le metriche cv di TFT (fatte su DBR)\n",
    "with open('pickles/TFT_cv_metrics_df_tft.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca629161-cc70-4a8c-8ac2-cfa7ce77e397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32451478-1ac3-4516-93cf-48f2031970ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_tft = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc242625-6792-4a61-8d2d-f0f524d11fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo il modello (ero in DBR)\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_cv_metrics_df_tft.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(cv_metrics_df_tft, f)\n",
    "# salvo il modello\n",
    "# torch.save(model_tft, 'TFT_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f50c05-139e-4ebf-922c-6824c7db662b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico il modello\n",
    "model_tft = torch.load('pickles/TFT_model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a6500d-e48a-4c63-ba95-a749c880b09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = model_tft.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36ef48b-081c-4461-8a61-01e3be7adf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(model_tft, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc25a28f-45fe-41b9-a116-987c7605c42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(nf.models[0], plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0025f519-e539-4279-81d4-6a389350fa10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(nf.models[0], plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29c2b56-8d6d-4d1b-a0d9-708dc9ebd1e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = model_tft.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a80376f-9105-4e20-9ffb-1854c499f1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b323a4a-2bd9-4ed5-a61e-101c1a921a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfd250f-c2b7-477b-bff1-0393a492a9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b81285a-cedf-47a5-bee3-f7de57e57644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58213652-4381-4484-986f-27389173659f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "064b7876-42b4-4c93-adfc-9848da7d5298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4205355d-55a8-4202-a286-897eb3b36ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2308fc34-39ac-4f8d-96b4-973036f175d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e242caa-6962-4852-ad8d-34427f7949f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cac1f41d-5521-49a9-baf2-2c67d9c29009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37dc9cf3-4ccb-4e3c-b070-5edefee5986b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nf.models[0].feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f74fdac-a966-4f76-a5c7-af2a01dc3bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64ad676f-8a74-4a3d-b4ac-3cb949240383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "item_id, timestamp, target (Chronos calcola tempo di fitting in automatico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e7b7c9-1fd7-43b9-a2eb-7dd60e3a26a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb5eab9a-a4a0-481d-8983-6133fa97dbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ef47fa-a8a8-466c-a9e6-a720037f04be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8c7945d-a820-439f-a9d3-55c13ac5de67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(days=1),\n",
    "        periods=h,\n",
    "        freq=\"MS\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"MS\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(\n",
    "        train_df_chronos,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    "        )\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'Fold': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('Fold',inplace=True)\n",
    "print(\"\\n=== TimeSeriesCV Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "608c6f2f-b654-44d4-8a9d-9bdcedfb90cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e0e92e-9964-4bfe-98c2-7a8aef9c5bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='Fold', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d2f529-824d-4008-b007-dc51636deb6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b73049-651e-4484-9e97-d424ea24453a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bdc1337-3510-473c-8f8b-93f5ec54a2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"robust\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c1a82d-474b-4654-9256-dd405d72f795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI CLASSICHE\n",
    "# col miglior modello e col 0-shot\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2954fdb-1f42-4939-a295-c71cce3bdc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # PREVISIONI MULTI STEP ????\n",
    "# # Separa le covariate in due gruppi\n",
    "# known_ahead_covariates = ['month', 'quarter', 'year']  # Queste sono realmente conosciute in anticipo\n",
    "\n",
    "# lagged_covariates = [\n",
    "#     'lag_1', 'lag_2', 'lag_4', 'lag_12',\n",
    "#     'diff_12', 'change', 'percentChange',\n",
    "#     'rolling_mean_3', 'rolling_mean_6', 'rolling_mean_12',\n",
    "#     'rolling_std_3', 'rolling_std_6', 'rolling_std_12'\n",
    "# ]\n",
    "# # Configurazione del predictor\n",
    "# predictor = TimeSeriesPredictor(\n",
    "#     prediction_length=len(test_df_chronos),  # Ad esempio 6 per 6 mesi\n",
    "#     eval_metric=\"MASE\",\n",
    "#     target=\"target\",\n",
    "#     known_covariates_names=known_ahead_covariates\n",
    "# )\n",
    "\n",
    "# test_known_covariates = test_df_chronos[known_ahead_covariates]\n",
    "\n",
    "# # Fit del modello\n",
    "# predictor.fit(\n",
    "#     train_df_chronos,\n",
    "#     hyperparameters={\n",
    "#         \"Chronos\": [\n",
    "#             {\n",
    "#                 \"model_path\": \"bolt_small\",\n",
    "#                 \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "#             },\n",
    "#             {\n",
    "#                 \"model_path\": \"bolt_small\",\n",
    "#                 \"covariate_regressor\": \"CAT\",\n",
    "#                 \"target_scaler\": \"robust\",\n",
    "#                 \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "#             },\n",
    "#         ],\n",
    "#     },\n",
    "#     time_limit=600,\n",
    "#     enable_ensemble=True,\n",
    "# )\n",
    "\n",
    "# # Previsioni appropriate al contesto\n",
    "# # Per un multi-step puro senza feedback\n",
    "# predictions = predictor.predict(\n",
    "#     train_df_chronos,\n",
    "#     known_covariates=test_known_covariates  # Solo le covariate realmente note in anticipo\n",
    "# )\n",
    "# predictions.reset_index(inplace=True)\n",
    "\n",
    "# predictions_0shot = predictor.predict(\n",
    "#     train_df_chronos,\n",
    "#     known_covariates=test_known_covariates,\n",
    "#     model=\"ChronosZeroShot[bolt_small]\"\n",
    "# )\n",
    "# predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d98c1b0d-6d6d-41f1-8c8c-bd63306f9c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad9bdaf-b64d-4cf5-bc23-33612b1099a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['0.5'], mode='lines', \n",
    "    name='P50', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressor', line=dict(color='lightgreen')))\n",
    "# QUANTILI CHRONOS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(test_df_chronos['timestamp']) + \n",
    "    list(test_df_chronos['timestamp'][::-1]),\n",
    "    y=list(predictions['0.9']) + list(predictions['0.1'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# PREVISIONI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7b80b3-6b9c-43d3-a99c-e9c3e42930ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec45a3e-c943-4592-90dd-bf5c4e957154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d5282a-9f95-4749-9f79-b1ec6767cd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_tft).join(metriche_chronos).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295ec253-0db5-4537-89e5-6622cb0e4762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR OIL PRICE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=forecast_test['ds'], \n",
    "                         #y=Y_hat_df_tft['TFT'], \n",
    "                         y=Y_hat_df_tft,\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    "# PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS 0-shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5e3dd3-398c-4065-89e2-3376e40a1f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sistemo i vari dataset prima del merge\n",
    "#results_df_chronos.drop(columns=(['level_0', 'index']),inplace=True)\n",
    "#cv_metrics_df_tft.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE_AutoARIMA','RMSE_AutoARIMA']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE_AutoETS','RMSE_AutoETS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0d0f51-f193-41ad-9ff5-39ede5e81d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = results_df_np.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_tft, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_ets,\n",
    "                            on=\"Fold\", how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_lstm', 'RMSE_lstm', 'MAE_TFT', 'RMSE_TFT',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f62ed1e5-3b86-48a0-a4c5-b8837d3a9c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos.round(4)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8759e99a-4b4c-4937-a812-d36d117ac47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **OIL PRICE (multistep)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdda3d7b-9152-4080-a566-8274d5497bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "H=12 mesi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b25e13c-8438-44de-b5eb-b0d017aad540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "oil_prices.dropna(inplace=True)\n",
    "df = oil_prices.reset_index().rename(columns={'date': 'ds','price':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dda2450-56db-4071-857a-91a11059e9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    # features\n",
    "    lags = [1, 2, 4, 12]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    df['diff_12'] = (df[target_col] - df[target_col].shift(12)).shift(1)\n",
    "    \n",
    "    df['change'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['percentChange'] = df[target_col].pct_change().shift(1)\n",
    "\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_3'] = df[target_col].rolling(window=3).mean().shift(1) #ultimo trimestre\n",
    "    df['rolling_mean_6'] = df[target_col].rolling(window=6).mean().shift(1) #ultima metà dell'anno\n",
    "    df['rolling_mean_12'] = df[target_col].rolling(window=12).mean().shift(1) #ultimo anno\n",
    "    df['rolling_std_3'] = df[target_col].rolling(window=3).std().shift(1)\n",
    "    df['rolling_std_6'] = df[target_col].rolling(window=6).std().shift(1)\n",
    "    df['rolling_std_12'] = df[target_col].rolling(window=12).std().shift(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12a7f081-b106-48af-8327-4c8adbff5463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "h = 12\n",
    "future_features = ['ds', 'y','month', 'quarter', 'year']\n",
    "futr_exog_list = ['month', 'quarter', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3946041-0167-420a-9f76-1ea03f0eac79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "437e9f89-9262-4738-ab37-ac3b30f57bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "hist_exog_features = df_xgb.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 12 obs.\n",
    "df_xgb['covid'] = (df_xgb['ds'] > pd.Timestamp('2020-02-01')).astype(int)\n",
    "\n",
    "# STEP 2: Prepara DMatrix per previsione\n",
    "df_xgb = df_xgb[future_features]\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb99b773-6594-4621-a947-d0625ab960c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# train tutto tranne le ultime 12 osservazioni\n",
    "train_df_xgb = df_xgb.iloc[:-h]\n",
    "test_df_xgb = df_xgb.iloc[-h:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36deecc0-26be-4305-aab8-ff4ebd01a99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=df_xgb['ds'], y=y_train_xgb, mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=y_test_xgb, mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50e826eb-0e89-474d-8701-3f9e01178043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bdf93fd-0e96-4874-937a-873d5f5bf7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c85eac-95dc-460f-939c-2a556a92cc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  \n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd6e294e-36a7-4348-b1ef-819a556e4ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63923eff-f38c-485c-ab38-5e89027511a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729266fe-c741-4b3b-8429-1af85271035d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d7146a-cb13-4618-8367-57e4a14d6988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a92343c-aa51-483a-834b-2cdfb2642090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label='Oil Price - XGB, CV')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56122b91-1ec7-430d-b49e-8e5c92778484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora facciamo training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c22d319-bb16-48cb-821b-8703e4bccbdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c6c509-584b-4ebd-972e-b0034dab4600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig, ax = plt.subplots()\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=10, ax=ax)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "618d86f5-e1a9-4520-8dd6-afc689961262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95589c23-2a32-433e-92d9-5c5fe555588c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2638e95-1420-4540-9ddc-eae9e025ac3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb248b6-740b-46ec-b523-fe55f0800523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af00c3ef-36f7-4bbc-8062-46b70c135263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa04f888-883a-492f-b3fb-4631375aa505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'],modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "744f0fde-fefc-42b9-b355-01624696c2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92097f10-b970-482b-9e6e-695390102d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_df_lstm = df_lstm.iloc[:-h]\n",
    "test_df_lstm = df_lstm.iloc[-h:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700d5590-caf5-4e32-bc5b-abd869fec7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_lstm.copy()\n",
    "test_df_original = test_df_lstm.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_lstm[['ds', 'unique_id']]\n",
    "test_meta = test_df_lstm[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_lstm.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_lstm.index\n",
    "test_idx = test_df_lstm.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_lstm = pd.DataFrame(scaler_x.fit_transform(train_df_lstm[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_lstm = pd.DataFrame(scaler_x.transform(test_df_lstm[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_lstm = pd.concat([train_df_lstm, train_meta], axis=1)\n",
    "test_df_lstm = pd.concat([test_df_lstm, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab7a1f3-6063-48cd-8c44-d068ea21a9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [12, 24, 36])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=len(test_df_lstm), \n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=100,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='MS')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=len(test_df_lstm),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0d9978-2cfe-4cf5-a970-cd9550e23371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57422443-3185-4590-80a0-5f484a61fe82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46908e94-7f91-4fd2-b742-2fda4b379328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestpars_OilPrice_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade56fb0-c045-48eb-b565-8aa1f85923f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e08c032-b27b-40a2-94b3-695e8ace65be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestpars_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a338d7-f189-47ca-9a15-467da0acbf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24d4b72-bbe7-4834-892e-8da2d7ec52d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_lstm,\n",
    "    step_size=len(test_df_lstm),\n",
    "    n_windows=5\n",
    ")\n",
    "with open('pickles/LSTM_cv_OilPrice_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)\n",
    "# with open('pickles/LSTM_cv_OilPrice_multistep.pkl', 'rb') as file:\n",
    "#     best_params = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1dbf7e-3425-4f18-b801-4751cac2da61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4f41ac-40ac-44db-9f52-5ea78169a4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='MS')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28db6906-253d-4d57-9e87-5f7367748abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora faccio predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc4489c-5f1f-416c-8b48-2e9d510e62a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87889e3c-72f0-44c6-ad70-e90f8b023881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_lstm['LSTM'] = Y_hat_df_lstm['LSTM'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-median'] = Y_hat_df_lstm['LSTM-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-lo-95'] = Y_hat_df_lstm['LSTM-lo-95'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-hi-95'] = Y_hat_df_lstm['LSTM-hi-95'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "716c40fd-69e2-4590-bb8c-d69db71d5cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Intervallo di confidenza 95%\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_lstm['ds']) + \n",
    "    list(Y_hat_df_lstm['ds'][::-1]),\n",
    "    y=list(Y_hat_df_lstm['LSTM-hi-95']) + list(Y_hat_df_lstm['LSTM-lo-95'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9d6260-480c-4e1c-9a53-874441cd697c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_original['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b32c28-6fe7-429c-b561-47d16aaf7cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2591360c-f5bf-47bd-9137-084eb82ead1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "737f6a40-350c-4e1c-9275-ace46b91209e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f079de6b-72f1-4df1-a001-691bff6bd989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1dceb1c-28dd-4c5e-8db8-65644864579a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "initial_train_size = len(train_df_np) - 5 * len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = train_df_np.iloc[:train_end].copy()\n",
    "    test_window = train_df_np.iloc[test_start:test_end].copy()\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello NeuralProphet\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    # Aggiungi regressori\n",
    "    for reg in futr_exog_list:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"MS\", epochs=100)\n",
    "\n",
    "    # Predizione\n",
    "    future_df = test_window.copy()  # Deve contenere anche i regressori futuri\n",
    "    forecast = model.predict(future_df)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati CV\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# with open('pickles/NP_cv_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(results_df_np, file)\n",
    "with open('pickles/NP_cv_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)\n",
    "\n",
    "# Plot Errori\n",
    "results_df_np.plot(\n",
    "    x='Fold',\n",
    "    y=['MAE_nprophet', 'RMSE_nprophet'],\n",
    "    marker='o',\n",
    "    title='TimeSeriesCV Errors',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b3b11a2-cbb6-410c-bcf1-8f1727aa24ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0becf161-34db-4453-864c-edbd2fd2c74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a1bf27b-867d-4043-846b-54509c1aeda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f1e8900-0e21-49d6-9dee-336138543ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "deterministic_features = ['month','quarter','year']\n",
    "for reg in deterministic_features:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"MS\",epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff9ec8d-fb04-4eaa-b2f3-e6ec6b00b632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0f5a9b-5d1e-4f07-ae32-19d65c44bb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast = neuralprophet.predict(test_df_np[future_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eee505be-4029-4247-946d-5cd13f1d1fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast['yhat1'] = forecast['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast['yhat1 2.5%'] = forecast['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast['yhat1 97.5%'] = forecast['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d959245-d68f-41d7-b4ab-873caf69c208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast['ds'], \n",
    "    y=forecast['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# #Intervallo di confidenza al 95%\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=list(forecast['ds']) + \n",
    "#     list(forecast['ds'][::-1]), \n",
    "#     y=list(forecast['yhat1 2.5%']) + list(forecast_test['yhat1 97.5%'][::-1]),\n",
    "#     fill='toself',\n",
    "#     fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "#     line=dict(color='rgba(255,255,255,0)'),\n",
    "#     hoverinfo=\"skip\",\n",
    "#     showlegend=True,\n",
    "#     name='P10-P90'\n",
    "# ))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68cc5890-f002-4b2a-aa34-6d5669b71303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast['ds'], 'forecast': forecast['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'], train_df_original['y'],modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ef31fb-bd56-417d-a200-5eb274df4745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a8ca627-1521-4b4d-91ce-d02f69b080c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "train_df_tft = df_tft.iloc[:-h]\n",
    "test_df_tft = df_tft.iloc[-h:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b110fce-12d1-460a-9552-b3229f6aa13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c62e59-9675-4729-85ac-227495e1e7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (TROPPO LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4207356-b8eb-4f49-83a7-74adbeae40d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h*6), int(4*h), int(2*h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft=TFT(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            n_rnn_layers=n_rnn_layers,\n",
    "            grn_activation=grn_activation, #prima era ELU\n",
    "            rnn_type=\"lstm\",\n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "            learning_rate=learning_rate, \n",
    "            hist_exog_list=hist_exog_features,\n",
    "            futr_exog_list=futr_exog_list, #queste dovranno essere nel test\n",
    "            max_steps=30, \n",
    "            val_check_steps=10,\n",
    "            batch_size=batch_size, \n",
    "            #early_stop_patience_steps=15,  \n",
    "            scaler_type=\"robust\",\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",  \n",
    "        )\n",
    "    \n",
    "    # nf\n",
    "    nf = NeuralForecast(models=[tft], freq='MS')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f49b2fd-33fe-44e8-9ea9-8ec74a549d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61993d93-c95c-4d13-8556-39dd1de60573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9a88b94-86fc-4b1c-8fc0-4d43b8641095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo i best params\n",
    "# with open('pickles/TFT_bestpars_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278f3a76-0a5a-4676-8ef0-47b092cfb7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico i best params\n",
    "with open('pickles/TFT_bestpars_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be122384-373a-4e11-9522-863ee858c4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"MS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1822191a-7b82-4259-9c29-545c62f3a433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_tft,\n",
    "#     step_size=len(test_df_tft),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# # salvo le cv\n",
    "# with open('pickles/TFT_cv_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(df_cv, file)\n",
    "\n",
    "# carico le cv\n",
    "with open('pickles/TFT_cv_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_tft': mae,\n",
    "        'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c375ba-5191-4bfa-958b-201b59356b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_tft['unique_id'].unique():\n",
    "    series_data = train_df_tft[train_df_tft['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_tft'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_tft'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c2ca3b-90e6-46c0-be3e-f7e101357942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a98505-2e59-478b-b907-9473060b4bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training with best params\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"MS\")\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=best_params['input_size'])\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")\n",
    "tft_model = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06149887-64c6-4b67-a9a7-797dd7cda2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # training senza best params\n",
    "# h = len(test_df_tft)\n",
    "# nf = NeuralForecast(\n",
    "#     models=[\n",
    "#         TFT(\n",
    "#             h=h,\n",
    "#             input_size=h*5,\n",
    "#             hidden_size=60,  # deve essere divisibile per 4\n",
    "#             grn_activation=\"ReLU\", #prima era ELU\n",
    "#             rnn_type=\"lstm\",\n",
    "#             n_rnn_layers=4,  #consigliato fra 2 e 5; \n",
    "#             one_rnn_initial_state=False,\n",
    "#             loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "#             learning_rate=0.005, \n",
    "#             hist_exog_list=hist_exog_features,\n",
    "#             futr_exog_list=futr_exog_list, #queste dovranno essere nel test\n",
    "#             max_steps=50, \n",
    "#             val_check_steps=10,\n",
    "#             batch_size=96, \n",
    "#             early_stop_patience_steps=15,  \n",
    "#             scaler_type=\"robust\",\n",
    "#             enable_progress_bar=True,\n",
    "#             accelerator=\"auto\",  \n",
    "#         ),\n",
    "#     ],\n",
    "#     freq='MS',\n",
    "# )\n",
    "\n",
    "# start_time = time.time()\n",
    "# #fit \n",
    "# nf.fit(df=train_df_tft, val_size=2*h)\n",
    "\n",
    "# end_time = time.time()\n",
    "# training_time = end_time - start_time\n",
    "# print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d7cfc4-740f-42a4-8542-a2ef611c7d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft[['unique_id','ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e6a21a9-f5a6-40a0-8fdf-eb905233ff0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo le prediction\n",
    "# with open('pickles/TFT_pred_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ae822ef-e9f3-4ac4-aaa5-f076adea96d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le prediction\n",
    "with open('pickles/TFT_pred_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4819d70a-a413-4ad6-9197-5e7b6a41cb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbdbc733-879b-452d-ae4c-3f7ad8eee157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475019f4-a16d-4927-99e2-dd76e58db50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "459665e0-2772-4c62-a973-cb01b2f16777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "488d4f8d-59e6-4535-99ea-7257cdbd6f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo il modello tft\n",
    "# with open('pickles/TFT_model_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(tft_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c8ca27-2e2f-45cc-b573-2b238a1090db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico il modello tft\n",
    "with open('pickles/TFT_model_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    tft_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab2b70f-3cb1-40d7-9901-490b5fb58fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = tft_model.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542f5fc0-e5d9-4395-8279-577e0fee907b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(tft_model, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85e8a0c-27ca-491d-b9dd-a4b1b0db83ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(tft_model, plot=4)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a2302d-f067-4ef2-9b1a-4bc61a87fcb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(tft_model, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888383cf-496a-4478-a007-cec0da472546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = tft_model.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7336aa59-3a57-47cd-91c2-2637142aeb20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22a07cf-eaa3-4013-aa70-feed83fc0d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d9988d-1901-4741-a213-0bf46b5191c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be447cda-5e85-42e9-87a6-8fa403451ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd9b4ed-b8d1-4646-8e31-9f2ae50a57d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5590468-2058-48c0-86a6-8f617a68ff48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6757439e-543c-426e-8ca4-ab53f3fcd9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f94eb476-a3a1-4c9f-b15e-55e7021441ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62cddf8-4122-4da9-8c42-c7cfa4dc2fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    tft_model\n",
    "    .attention_weights()[tft_model.input_size :, :]\n",
    "    .mean(axis=0)[: tft_model.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d64df814-3cf9-4382-96c1-9199668c9880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_importances4.shape)  # (n_time_steps, n_variables)\n",
    "print(mean_attention.shape)   # Deve essere (n_variables,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4ea55d-a0ac-47c4-9a5d-743d243ee0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "\n",
    "# Estrai l'importanza delle future variables\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "\n",
    "# Estrai tutte le attention weights mediate nel tempo\n",
    "full_attention = tft_model.attention_weights()[tft_model.input_size:, :].mean(axis=0)\n",
    "\n",
    "# Estrai solo la parte relativa ai 12 time steps futuri\n",
    "n_future_steps = df_importances4.shape[0]\n",
    "mean_attention_future = full_attention[-n_future_steps:]\n",
    "\n",
    "# Applica la ponderazione riga per riga (sul tempo)\n",
    "df_importances4 = df_importances4.multiply(mean_attention_future, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    ax.bar(\n",
    "        np.arange(1, len(df_importances4) + 1), \n",
    "        df_importances4[col].values, \n",
    "        width=0.6, \n",
    "        label=col, \n",
    "        bottom=bottom\n",
    "    )\n",
    "    bottom += df_importances4[col].values\n",
    "\n",
    "# Titoli e assi\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Future time steps\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Linea media di attenzione\n",
    "ax.plot(\n",
    "    np.arange(1, len(df_importances4) + 1),\n",
    "    mean_attention_future,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8329ada-cc35-4777-b242-d34ef1d7877e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tft_model.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a97d2c5-ce78-4df2-85f4-44537a3bf6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f69700a-9e94-44b4-9412-ead1eb372fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "item_id, timestamp, target (Chronos calcola tempo di fitting in automatico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff94989-d75c-4b9d-bf18-3dda1985a64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0d2f59-4edf-4e38-a45f-2968be6d19c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd7c9a4-7764-4fa9-8194-111d9de2a5b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c6805682-ab2f-44a8-b5ed-cc14ee217796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(days=1),\n",
    "        periods=h,\n",
    "        freq=\"MS\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"MS\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(\n",
    "        train_df_chronos,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    "        )\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'Fold': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('Fold',inplace=True)\n",
    "print(\"\\n=== TimeSeriesCV Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9be523ed-147e-46a2-852c-7068ec67ff7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ff818f88-df41-401b-af2a-363d1c0c8431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='Fold', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbc17a2c-c131-4b46-b1b8-9ed0ca001027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b34081-3de7-4f59-8429-366f6c36be7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ec86d6-c90c-4795-b470-d5b3e3376507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparo dataset per training\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list\n",
    ")\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133ea18a-da11-4f46-ad00-2a303ce295f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"robust\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb24937-d8c5-45dc-a185-93ebb55b4555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI MULTI STEP \n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates  # Solo le covariate realmente note in anticipo\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542c38f3-40c2-4b74-bf00-b87c88c73118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "104dde22-3573-4131-b1af-0b76d8f368ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['0.5'], mode='lines', \n",
    "    name='P50', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressor', line=dict(color='lightgreen')))\n",
    "# QUANTILI CHRONOS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(test_df_chronos['timestamp']) + \n",
    "    list(test_df_chronos['timestamp'][::-1]),\n",
    "    y=list(predictions['0.9']) + list(predictions['0.1'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# PREVISIONI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec8c647c-f28d-4e4c-bc77-714528419890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed6398f-a659-4abc-a2ba-31273c2b8981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos_0shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25426ac-70a1-43bf-bfb6-5b379137c401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68ee8114-3a90-4cfd-9664-af5eb27395ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_tft).join(metriche_chronos).join(metriche_chronos_0shot).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02fc14cc-12ea-42b7-a5e3-7d753bd6c8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR OIL PRICE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=forecast['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], \n",
    "                         y=Y_hat_df_tft['TFT'], \n",
    "                         #y=Y_hat_df_tft_TFT,\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS 0-shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3976411-c564-49c8-9e8e-38078bf30978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)\n",
    "df_arima.rename(columns={'MAE':'MAE_AutoARIMA','RMSE':'RMSE_AutoARIMA'}, inplace=True)\n",
    "df_ets.rename(columns={'MAE':'MAE_AutoETS','RMSE':'RMSE_AutoETS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0cd3cd-d1af-432b-98ca-dfd595e1a291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = df_ets.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_tft', 'RMSE_tft', 'MAE_lstm', 'RMSE_lstm',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'tft': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb75de44-0201-40a3-a623-255fcff32695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96109063-8fa3-455d-a1f3-8d34cbd40897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **OUTLIERS data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3c397e-301e-4883-b0ce-99af8fd56b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = out.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "164cd7ad-fb06-4647-9ccd-26bd08f0fc65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    # Lag features (1-7)\n",
    "    for lag in range(1, 8):\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    # Differencing features, 1 e 7\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(7)).shift(1)\n",
    "    \n",
    "    # Caratteristiche temporali\n",
    "    df['dayofweek'] = df['ds'].dt.dayofweek\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_3'] = df[target_col].rolling(window=3).mean().shift(1) #ultimoi 3 giorni\n",
    "    df['rolling_mean_7'] = df[target_col].rolling(window=7).mean().shift(1) #ultima settimana\n",
    "    df['rolling_std_3'] = df[target_col].rolling(window=3).std().shift(1)\n",
    "    df['rolling_std_7'] = df[target_col].rolling(window=7).std().shift(1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a16066-2254-4732-b7b2-29cb1f99be8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "003c616d-f9e1-4782-98ab-7a6327a8b771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_arima = df[['y','ds','unique_id']].iloc[8:]\n",
    "train_size = int(len(df_arima) * 0.9)\n",
    "train_df = df_arima.iloc[:train_size]\n",
    "test_df = df_arima.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 7 # for monthly data = 12 \n",
    "horizon = len(test_df) # number of predictions\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bab9a837-356c-4787-8e34-6e3fe05ceb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c9ca50-bad4-4451-86eb-e3fdd56cc9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definisco i modelli\n",
    "models = [\n",
    "    AutoARIMA(season_length=season_length),\n",
    "    AutoETS(season_length=season_length)\n",
    "]\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='D')\n",
    "import time\n",
    "start_time = time.time()\n",
    "# fit\n",
    "sf.fit(train_df)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4118af44-f3a6-4c98-add0-6296d52b57d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a526e7c4-c2bb-47f2-843b-052db74a6eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arima_string(sf.fitted_[0,0].model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90520e3f-d0dd-4567-985c-d1e41772ebf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4194709-ccd3-42e6-9634-36bd82719509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast\n",
    "Y_hat_df = sf.forecast(df=train_df, h=len(test_df), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ba64dd-dfe5-4c3f-9ad9-2f5e3a622eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "crossvalidation_df = sf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=horizon, \n",
    "    step_size=len(test_df),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = crossvalidation_df['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == fold]\n",
    "    \n",
    "    for model in ['AutoARIMA', 'AutoETS']:\n",
    "        # Filtra i dati per il modello corrente\n",
    "        model_data = fold_data[['y', f'{model}']]\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        # Calcola MAE e RMSE\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        \n",
    "        # Aggiungi i risultati\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': model,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_arima_ets = pd.DataFrame(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1528a214-cae5-4c3c-9d31-84291a43ec52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_df_arima_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827baf11-ec8a-4af1-9c79-d9fe8e24d5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_arima_ets)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_arima_ets.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics[['Model', 'MAE', 'RMSE']])\n",
    "\n",
    "# Ora creiamo un grafico che combina la serie temporale con le metriche di errore per visualizzare dove si verificano gli errori maggiori\n",
    "\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df['unique_id'].unique():\n",
    "    series_data = train_df[train_df['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3a0627-d4ed-4d8d-a3ab-7fae0d6aa0ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_intervals #EXPANDING WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8800f425-9caa-4da7-802c-90676cd32f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's forecast on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc80d04f-277e-4afc-88fb-b3ffc66e05b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "test_pred = sf.forecast(df=train_df, h=len(test_df), level=[95], fitted=True)\n",
    "pred = test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "699b42b6-82df-4ec4-8c8a-4e59b483a91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_arima = calcola_metriche(pred['y'],pred['AutoARIMA'],train_df['y'], \n",
    "                                modelname=\"ARIMA\").round(10)\n",
    "metriche_ets = calcola_metriche(pred['y'],pred['AutoETS'],train_df['y'], \n",
    "                                modelname=\"ETS\").round(10)\n",
    "metriche_arima, metriche_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "963b2975-d639-42cc-a401-1777c0f5fd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(pred, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15)))\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c837db-a3ee-4283-97ec-1048b938185b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b111947d-cf9c-4c8d-93d2-2fbf430f12d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 7 obs.\n",
    "# print(df_xgb.columns)\n",
    "# df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f519eb08-2fc4-4e05-a9eb-9bcf89d4c350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.9)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a2afad-e049-4ddf-8804-c27f7b049359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce56fdb3-d6d5-4ef1-9ade-07e39507c628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce178ce5-7195-41ab-86ea-e305b7a292f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69cb4731-792f-4e70-aa00-fc09317770ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) #aumentare in databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6a34dd-ed6a-49ec-8d46-8cf22d2abe83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74fc664-6aaa-4fd9-83a0-c757b4c37b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c70092-7437-4f6a-b9b9-bffc2da54948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc88624-2fb7-4ce0-9d5f-1509780dc936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8936ea8-a5f2-436c-918d-027c0205c46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d814e82c-b206-44da-8de5-d2ae7ab863d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feecba44-b1a3-484d-b681-9ac3d94f4e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf78bcd5-5fd8-41ff-8ea0-32275ba9a213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MULTI-STEP FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3243d5da-c595-4f01-832e-826802a5f5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon, freq='D')\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f3799f-d877-4bca-8796-72a7f318bb05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Forecasting\n",
    "# forecast_xgb = direct_multistep_forecast(train_df_xgb, feature_cols, target_col='y',horizon=len(test_df_xgb))\n",
    "# print(f\"Previsioni per i prossimi {(len(test_df_xgb))} step:\", forecast_xgb.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af19b223-869f-4aa4-960a-0ed4bdab8355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RECURSIVE FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec137b56-23cc-4935-908a-e8f64b62a609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896086d5-4b7c-4db6-a8d9-4ccf5a9da966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2eb819-c5e1-41bb-8ffc-9f089ecd852e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdfb706d-e595-4141-b214-d841935f1db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615aecb8-a5ef-44b9-bc9e-17aa8b6e1d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74fd906b-ab07-4a2b-bea0-1048ce81d304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'],modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f756bfb9-324c-43f9-aff1-854c05c3f3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM (neuralforecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e1af54-fb19-4e2e-8c73-708bb102e517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df_lstm) * 0.9)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05ff7d08-0bad-4954-8627-084dd63bf601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c1f83b-2dcb-48e7-b9a1-54a2e0e5f821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "h = len(test_df_lstm)\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    cv_metrics = []\n",
    "    # Hyperparametri da ottimizzare\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(2.5*h)] )\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 3)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    # Fisso il seed per riproducibilità\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=h,  # nel tuning\n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, #aumentare a 150\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='D')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739ceb1f-544a-4edf-b905-35b77918e373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a77f5e-dfcd-4e85-ac44-e85e8ddcae19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040040aa-b81e-4645-be29-dc35f46f5967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo i best params trovati da Optuna\n",
    "# with open('pickles/LSTM_bestPar_Outliers.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf21ffd-0aba-43fc-b02b-431dc36c8e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea1c695-7d57-4a9f-bdc0-099f49534027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_Outliers.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89614d64-b567-4967-a475-6da5376e3f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=100,\n",
    "    #early_stop_patience_steps=15,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "940366b3-9354-488c-ba82-9c3f897591d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm,\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_Outliers.pkl', 'wb') as file:\n",
    "#     pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_Outliers.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c0ed1a-c200-4549-b4d8-bc86eb295e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40581a7f-94ac-4143-841f-a07155883d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=100, #aumentare a 150\n",
    "    early_stop_patience_steps=15,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='D')\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88cda4df-fe4f-4d62-ab86-bcccbe4fcf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65c754d-b4a4-4695-8a3f-2eacf539a670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)\n",
    "# salvo previsioni in pickle\n",
    "#Y_hat_df_lstm.to_pickle('pickles/LSTM_solarEnergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49af9551-1958-4a4f-b1dd-41426a9accca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni\n",
    "Y_hat_df_lstm = pd.read_pickle('pickles/LSTM_solarEnergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ee6d82-d114-4f08-bff3-4fc8b436da83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "#     name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c34c91-1a7b-47b3-88b9-905752474076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_lstm['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ed3a79-9153-4ed7-82a5-d2a2077eb855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7088b9b-7db6-4771-8ab0-1a1454a9af44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.drop(columns=['unique_id'])\n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b5a1a7-c55e-4060-92cc-66e1fbc08e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23138ab-40c8-480b-8dbb-3e6595dc0366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a84bb30-0fa4-4a15-b969-0d64bbb03e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7d90ad-c799-4f45-9106-116b0f035785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6',\n",
    "       'lag_7', 'diff_1', 'dayofweek', 'month', 'quarter', 'day_sin',\n",
    "       'day_cos', 'month_sin', 'month_cos', 'rolling_mean_3', 'rolling_mean_7',\n",
    "       'rolling_std_3', 'rolling_std_7']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.001, \n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3574173c-3e25-4404-b563-ed9ae5a1136d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Verifica\n",
    "required_length = initial_train_size + h * n_windows\n",
    "if len(np_df_train) < required_length:\n",
    "    raise ValueError(\"Dataset troppo corto per questo schema di cross-validation.\")\n",
    "\n",
    "# Reset risultati\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = np_df_train.iloc[:train_end]\n",
    "    test_window = np_df_train.iloc[test_start:test_end]\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=32,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "        global_normalization=True,\n",
    "        unknown_data_normalization=True\n",
    "    )\n",
    "\n",
    "    for reg in regressors:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"15min\", epochs=100)\n",
    "\n",
    "    # Previsione\n",
    "    forecast = model.predict(test_window)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'split': i+1, 'MAE_np': mae, 'RMSE_np': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot\n",
    "results_df_np.plot(x='split', y=['MAE_np', 'RMSE_np'], marker='o', title='Backtesting Errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d853ff-a194-48f3-8138-640f9ea2c904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b04ba3-ea1d-47fd-97e5-15d7f8ad0c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ae3267-3fee-4e49-96cb-e5ae96802887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV PLOT\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b532566-3d19-489c-adaf-d1fab2cae925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b2df86-024d-4f73-8817-c46164669787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definisco il modello\n",
    "\n",
    "# Fisso i seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, #0.005\n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8496fcbd-2341-4bee-a1fe-d5aa02501dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"D\", epochs=300)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ea4ffd-2cf5-4fa6-b0b0-8a48fcab7f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33368cb-1914-40e1-a8a4-8d59b8fa4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(np_df_test)\n",
    "forecast_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd9edb4-cae0-44d9-8967-e38787118feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e7f841-ab64-4e2a-890a-57b47536acc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_original['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_original['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Out-of-sample forecast (future)\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=forecast_future['ds'], \n",
    "#     y=forecast_future['yhat1'],\n",
    "#     mode='lines', \n",
    "#     line=dict(color='green'),\n",
    "#     showlegend=False,  # Evita duplicati nella legenda\n",
    "#     name=\"Out-of-sample\"\n",
    "# ))\n",
    "\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8abedd84-8dfb-4ae8-a7ca-6c1a45f616b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'], \n",
    "                               train_df_original['y'], modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e35e92-1230-4ac1-b48b-8aa94757a0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9610a095-d435-41ed-8b70-cab1418b9a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.iloc[8:]\n",
    "# 2. Suddivisione in train/test\n",
    "train_size = int(len(df_tft) * 0.9)\n",
    "train_df_tft = df_tft.iloc[:train_size]\n",
    "test_df_tft = df_tft.iloc[train_size:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "843048b7-cee4-47ef-8268-d7f3ec228ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds','unique_id']]\n",
    "test_meta = test_df_tft[['ds','unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds','unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c94435-77fc-4ebb-b0a2-db60e6938a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  TimeSeriesCV - fine tuning Optuna (LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b05971eb-8a48-4015-84f9-3ed726bba41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(1.5*h), int(2*h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, \n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='D')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "574c28c9-52c4-4747-a18e-252c7b73f799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a00898-f68e-4701-a1ef-72483e47bb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params\n",
    "# #salvo i best params in pickle\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_bestPars_Outliers.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c569264d-71cd-41c7-8437-790682d28059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico i best params\n",
    "with open('pickles/TFT_bestPars_Outliers.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4e0211e-0e21-47a5-9966-69fd04d8f69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La CV è troppo lenta, si blocca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fdadbe-747e-48ff-b6fc-a3776c11f676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(h/3), #best_params['input_size']\n",
    "        hidden_size=4, #best_params['hidden_size']\n",
    "        n_rnn_layers=1, #best_params['n_rnn_layers']\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=int(best_params['batch_size'] / 4),\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"D\")\n",
    "\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft[200:],\n",
    "    step_size=int(h/2),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24048d4d-20be-4f85-bd99-9ff9c32df247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/cv_metrics_df_tft.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(cv_metrics_df_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da9aac22-9019-47be-ad75-c152905a97f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le metriche di CV\n",
    "with open('pickles/Outliers_cv_metrics_df_tft.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e937acc5-9427-4f6e-b485-5bdf8381dc13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_tft['ds'], train_df_tft['y'], label=f'Serie unica')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_TFT'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_TFT'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b7ef86-4a43-4a1d-a998-96dd495293e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18c372e7-4cc4-4286-9e53-302bd52f043d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=100, \n",
    "        val_check_steps=25,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64        \n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"D\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=len(test_df_tft))\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e6ea87-d6e4-4abc-85c8-68bbc85d9cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)\n",
    "#salvo in pickle\n",
    "# with open('pickles/TFT_Outliers.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ac8ff3-d68f-4231-977a-db69b9504584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni:\n",
    "with open('pickles/TFT_Outliers.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "56fb3a12-55c7-4cc5-bfc5-cc0ecceba622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "77f67cf2-f090-4a8c-80dc-a248133cb1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9beb54-473e-44a6-91c0-c40e0e975114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b5b32f-7df8-4151-8e64-0bb25f28bf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d1ccd8-2606-406e-880e-4bfbf40ce052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo modello non me lo faceva scaricare perchè troppo pesante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458eafbc-b21f-4332-8ae3-7352feb0c196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_tft = nf.models[0]\n",
    "# #salvo il modello (ero in DBR)\n",
    "# torch.save(model_tft, 'TFT_model_Outliers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7fbab4e-abf6-4ca0-b406-03392cf00038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico il modello\n",
    "# model_tft = torch.load('pickles/TFT_model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0df3782-47e7-4375-8f34-7953633ca937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = nf.models[0].attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d0eb40-686a-4d06-9aac-84d43780ce39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(nf.models[0], plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382b3be0-26ce-4e14-af67-9623c5d16a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = nf.models[0].feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc24ef2-ca79-4ba5-b796-b9c314e02952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2c4802-dab8-4896-a7f4-41781c7daaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb69ae6-8ed9-4690-98cf-c15aa4519077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27f83356-240d-488e-a890-66c3c0ffa0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33e64fa-ff60-47b1-ab0b-356f6c522ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827810a5-ce05-443b-8a67-fb0062583789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91950100-1b7a-4200-a0a9-11ddd99bea3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a9eaa3-b93a-4b9e-96f9-9d388ad5f7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a978c9e-44fc-4870-8306-4d4666fa5226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4976a55f-46f3-41a0-a248-1d1af01de1e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    nf.models[0]\n",
    "    .attention_weights()[nf.models[0].input_size :, :]\n",
    "    .mean(axis=0)[: nf.models[0].input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c6a068-663b-4dd7-b766-0657936de8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nf.models[0].feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f561da0f-5396-4f22-a98e-f6bebc430a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2936e7d1-6cee-49c6-90d3-f10e8a911214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74cc7d4f-1c94-4989-ba50-a735e6bb7064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27d64e5f-de29-4f6a-9e9e-77029828e97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a9672d-70c0-42bf-8817-b7a134898e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(days=1),\n",
    "        periods=h,\n",
    "        freq=\"D\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"D\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(train_df_fold,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"standard\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "                {\n",
    "                    \"model_path\": \"bolt_base\",\n",
    "                    \"covariate_regressor\": \"MLP\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"MLP_Regressor\"},\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,)\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'split': fold+1, 'MAE': mae, 'RMSE': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('split',inplace=True)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e27214-d6b0-4ab3-817e-00cf22a81d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "445133bd-920f-4440-b0c3-1f69b89ac8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='split', y=['MAE','RMSE'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fbc10b3-2c49-49e1-86e4-9557c1e32e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1681a5f-4828-4ef2-a1fd-ca307a1207f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq = \"D\"\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": \n",
    "            [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\",\n",
    "                \"target_scaler\": \"robust\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,  \n",
    ")\n",
    "\n",
    "# Valutazione del modello\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2aa34fd-bdac-4c73-83d2-34bc9a24954f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI CLASSICHE\n",
    "# col miglior modello e col 0-shot\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b1d083-6723-4e3d-8bd2-fd9d55cfd0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.1'] = predictions_0shot['0.1'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18eda6c5-8863-4e0e-b399-3aeb6578dd1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['0.5'], mode='lines', \n",
    "    name='P50', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressor', line=dict(color='lightgreen')))\n",
    "# QUANTILI CHRONOS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(test_df_chronos['timestamp']) + \n",
    "    list(test_df_chronos['timestamp'][::-1]),\n",
    "    y=list(predictions['0.9']) + list(predictions['0.1'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# PREVISIONI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "# QUANTILI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(test_df_chronos['timestamp']) + \n",
    "    list(test_df_chronos['timestamp'][::-1]),\n",
    "    y=list(predictions_0shot['0.9']) + list(predictions_0shot['0.1'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90 0shot'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d2fa292-b049-4bf8-b759-18ed87ce5774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9073676e-a16e-4487-8d9e-fb9e05dcdedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0-shot\")\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04634d37-abc2-4746-9323-b265a761baff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Il modello Chronos 0 shot funziona meglio, perchè? \n",
    "1) Il modello 0 shot funziona molto bene con dataset piccoli (il regressor CAT ha poco su cui lavorare)\n",
    "2) Il modello 0 shot funziona molto bene con dataset regolari ad alta stagionalità\n",
    "3) Le covariate sono deboli e/o altamente correlate fra di loro\n",
    "Il gap di performance è basso, questo a dimostrazione del fatto che i due modelli performano in maniera abbastanza simile, ma le covariate portano leggermente fuori strada il modello ChronosWithRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36610f2f-473c-4eea-8841-3ce9b30e10f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2175ff-3563-4d06-856c-ad92fea0fe90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b447b6a8-88d1-4ce5-9a53-4e5fc9b302a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PLOT for outlier data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGB', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet ', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "    \n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=forecast_test['ds'], \n",
    "                         y=Y_hat_df_tft['TFT'],\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressor', line=dict(color='lightgreen')))\n",
    "\n",
    "# PREVISIONI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='Chronos 0 shot', line=dict(color='green')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd82f0ba-4935-475b-8538-ca65e5d9a8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sistemo i vari dataset prima del merge\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "df_arima.rename(columns={'MAE':'MAE_AutoARIMA','RMSE':'RMSE_AutoARIMA'}, inplace=True)\n",
    "df_ets.rename(columns={'MAE':'MAE_AutoETS','RMSE':'RMSE_AutoETS'}, inplace=True)\n",
    "# aggiusto i valori minimi e massimi (senza outlier)\n",
    "jan_2017 = train_df_lstm[(train_df_lstm['ds'].dt.year == 2017) & (train_df_lstm['ds'].dt.month == 1)]\n",
    "# Calcola il valore massimo del target in gennaio 2017\n",
    "max_val = jan_2017['y'].max()\n",
    "min_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62e8ad7b-6826-4af9-ae3d-fa81347c5b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = results_df_np.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos,\n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_ets,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft.drop(columns='Model'), \n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_np', 'RMSE_np', 'MAE_TFT', 'RMSE_TFT',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'np': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e77073-fd3c-4d98-ab78-d19ee472eba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos.round(4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95793e12-de31-484b-8a48-6e32e775c53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **OUTLIERS DATA (multistep)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6c79eb-a3dd-443c-98d1-c34c3dfe270a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "H = 90 gg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f498fe-9234-4a4a-a8ac-9b40e98f5f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = out.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf8097b-3d14-404e-9b3b-e9d00d1680a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    # Lag features (1-7)\n",
    "    for lag in range(1, 8):\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    # Differencing features, 1 e 7\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(7)).shift(1)\n",
    "    \n",
    "    # Caratteristiche temporali\n",
    "    df['dayofweek'] = df['ds'].dt.dayofweek\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_3'] = df[target_col].rolling(window=3).mean().shift(1) #ultimoi 3 giorni\n",
    "    df['rolling_mean_7'] = df[target_col].rolling(window=7).mean().shift(1) #ultima settimana\n",
    "    df['rolling_std_3'] = df[target_col].rolling(window=3).std().shift(1)\n",
    "    df['rolling_std_7'] = df[target_col].rolling(window=7).std().shift(1)\n",
    "    \n",
    "    df['covid'] = (df['ds'] > pd.Timestamp('2020-02-01')).astype(int)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4acdd53d-ba32-43ac-8484-22c1d702973d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "h = 90\n",
    "futr_exog_list = ['dayofweek', 'month', 'quarter', 'day_sin',\n",
    "       'day_cos', 'month_sin', 'month_cos', 'covid']\n",
    "future_features = ['ds', 'y'] + futr_exog_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70bc84ec-03f8-489c-ac9b-1dec17c4efc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed848654-a3e2-4c8e-bac3-5902b00063e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "hist_exog_features = df_xgb.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 12 obs.\n",
    "# STEP 2: Prepara DMatrix per previsione\n",
    "df_xgb = df_xgb[future_features]\n",
    "print(df_xgb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90564145-a940-4c54-89df-c6f5b5eb507d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# train tutto tranne le ultime 12 osservazioni\n",
    "train_df_xgb = df_xgb.iloc[:-h]\n",
    "test_df_xgb = df_xgb.iloc[-h:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eceaf9e1-77aa-48db-b169-bfaacf328189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=df_xgb['ds'], y=y_train_xgb, mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=y_test_xgb, mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bf6b2a-3737-4581-96ee-bce1180539ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "001e8506-88e2-490c-bbd4-681f12917d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26fad748-6b2c-4518-bd8e-e2332ca864e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  \n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8cb548-4cd3-458b-b898-1519feb2ab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68eb250d-4e2b-4d65-95ab-64573ce6a711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5e4b50-0e82-4204-9939-3729384bfebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853928af-f614-43e8-8cdb-eeedfde70be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc0e779-f380-4f95-b8e6-342c71906057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label='Oil Price - XGB, CV')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41cc4a3-0200-4305-a56f-9f06cf86df6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora facciamo training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af1a57a-75c7-4c6a-a067-f80c67e1677d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "import time\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6cdfd03-0ddf-4102-a022-2b9dc642f6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig, ax = plt.subplots()\n",
    "xgb.plot_importance(model, importance_type='gain', max_num_features=10, ax=ax)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9fc604-20a0-46cd-a276-7c7a24bb463e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da95dd5a-35b8-4402-8c70-6129902b9c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd761464-d3ac-4add-a0c9-3e21769fa605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a602a83a-c1dd-4630-82dd-c8d5117d2460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a0b8c9-6b8f-4944-a077-0d4cc7b71c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0748e0c2-f0d5-4125-a90d-e0f975c71cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],train_df_xgb['y'],modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7813522-8b4d-49e6-a4db-78be0bf29c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ffa2e2-ae38-4509-9caa-0b026805fb7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_df_lstm = df_lstm.iloc[:-h]\n",
    "test_df_lstm = df_lstm.iloc[-h:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7528c3-c8b0-4154-82c0-735b2071cf3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_lstm.copy()\n",
    "test_df_original = test_df_lstm.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_lstm[['ds', 'unique_id']]\n",
    "test_meta = test_df_lstm[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_lstm.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_lstm.index\n",
    "test_idx = test_df_lstm.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_lstm = pd.DataFrame(scaler_x.fit_transform(train_df_lstm[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_lstm = pd.DataFrame(scaler_x.transform(test_df_lstm[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_lstm = pd.concat([train_df_lstm, train_meta], axis=1)\n",
    "test_df_lstm = pd.concat([test_df_lstm, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9bdd14-fc84-4f60-8526-f74def199b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h), int(h*2), int(h*5)])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=len(test_df_lstm), \n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='D')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=len(test_df_lstm),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7809a30a-f8e8-4be1-93c0-f50ccf82db9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ecdc80-8761-4853-abcb-23569a71062f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50677b71-6aec-430a-8311-5a42305e35b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestpars_OutliersData_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f01691ad-476f-4365-9a1e-fff77288a6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75753ce6-2cdd-4cbf-ad78-91cdc5eb9b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestpars_OutliersData_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c201c4bb-1be9-4d92-a8f4-d9c9da5ba139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f093a45a-9ca7-4d71-a71d-432830a97272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm,\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_cv_OilPrice_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)\n",
    "with open('pickles/LSTM_cv_OilPrice_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "910d77a1-2c18-41b5-a5b0-a43af6d2dbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6344443-015f-427b-b93d-8eb24eba7b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=len(test_df_lstm),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=150,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='D')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204f9c3b-2395-4d6f-985d-10b18207c06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ora faccio predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7550ff6-574d-4b12-85b1-795deefb1785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957ca247-3603-44e6-b7ac-2c929435cdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo in pickle\n",
    "with open('pickles/LSTM_pred_Outliers_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d52cb3-972c-452f-8c88-3b4d4cfdf1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico da pickle\n",
    "with open('pickles/LSTM_pred_Outliers_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e519008-f29a-4d23-a21d-faa9b0970571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_lstm['LSTM'] = Y_hat_df_lstm['LSTM'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-median'] = Y_hat_df_lstm['LSTM-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-lo-95'] = Y_hat_df_lstm['LSTM-lo-95'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_lstm['LSTM-hi-95'] = Y_hat_df_lstm['LSTM-hi-95'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eea21ae-24c7-44c7-b9a7-de374a37eaef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Intervallo di confidenza 95%\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_lstm['ds']) + \n",
    "    list(Y_hat_df_lstm['ds'][::-1]),\n",
    "    y=list(Y_hat_df_lstm['LSTM-hi-95']) + list(Y_hat_df_lstm['LSTM-lo-95'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79a5039-7aed-4210-8895-42a3d19c15af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_original['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd04af6b-9742-4af8-aab6-0b0e51afbfb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203aaa39-7fec-4871-9b94-f5ed3a9f3372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44492e49-6ebe-4a58-8017-7b7225949845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d7c0a8f-ee05-4dfc-90f4-c264b841071d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249650f4-bcfc-461c-bc13-a457652da5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "initial_train_size = len(train_df_np) - 5 * len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = train_df_np.iloc[:train_end].copy()\n",
    "    test_window = train_df_np.iloc[test_start:test_end].copy()\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello NeuralProphet\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    # Aggiungi regressori\n",
    "    for reg in futr_exog_list:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"D\", epochs=100)\n",
    "\n",
    "    # Predizione\n",
    "    future_df = test_window.copy()  # Deve contenere anche i regressori futuri\n",
    "    forecast = model.predict(future_df.drop(columns=['covid']))\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati CV\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "with open('pickles/NP_cv_Outliers_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(results_df_np, file)\n",
    "with open('pickles/NP_cv_Outliers_multistep.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)\n",
    "\n",
    "# Plot Errori\n",
    "results_df_np.plot(\n",
    "    x='Fold',\n",
    "    y=['MAE_nprophet', 'RMSE_nprophet'],\n",
    "    marker='o',\n",
    "    title='TimeSeriesCV Errors',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d77f50-de9f-4ea2-9fe9-6d830127f772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in train_df_np.columns:\n",
    "        start_date = train_df_np['ds'].iloc[test_start]\n",
    "        end_date = train_df_np['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba433b5-1c49-4ab3-9ff6-9baaec52a8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac9be49-d3c7-472c-b40a-8d316adc8151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec824f0b-a5ce-4198-813c-c7fb14fa9044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"D\",epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5430d132-fa18-4f90-88e9-d3a3aa383049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a43487d-121b-4f19-a95b-1dacc65f8faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast = neuralprophet.predict(test_df_np[future_features].drop(columns=['covid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda32181-d0b4-4a68-a80e-0657db5e8181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast['yhat1'] = forecast['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast['yhat1 2.5%'] = forecast['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast['yhat1 97.5%'] = forecast['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d404cb-5f1a-4e6c-8e57-e0404d4a1157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast['ds'], \n",
    "    y=forecast['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "#Intervallo di confidenza al 95%\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(forecast['ds']) + \n",
    "    list(forecast['ds'][::-1]), \n",
    "    y=list(forecast['yhat1 2.5%']) + list(forecast['yhat1 97.5%'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2168182b-24ff-449e-ba10-0333d7761f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast['ds'], 'forecast': forecast['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'], train_df_original['y'],modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31b1cb7-3363-4355-89ab-21c7856a601a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5428be84-7c2d-490f-aeea-4263f5a7f8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "train_df_tft = df_tft.iloc[:-h]\n",
    "test_df_tft = df_tft.iloc[-h:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffc09b1-0364-4bc9-88cb-b489361bd27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc4ba24-3ad6-4cb4-a94d-b6294d081bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (TROPPO LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3191758-0b74-420b-b4b0-7bd0bab51895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h/2), int(h)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16, 32, 64, 128])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft=TFT(\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            n_rnn_layers=n_rnn_layers,\n",
    "            grn_activation=grn_activation, #prima era ELU\n",
    "            rnn_type=\"lstm\",\n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "            learning_rate=learning_rate, \n",
    "            hist_exog_list=hist_exog_features,\n",
    "            futr_exog_list=futr_exog_list, #queste dovranno essere nel test\n",
    "            max_steps=40, \n",
    "            val_check_steps=10,\n",
    "            batch_size=batch_size, \n",
    "            #early_stop_patience_steps=15,  \n",
    "            scaler_type=\"standard\",\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",  \n",
    "        )\n",
    "    \n",
    "    # nf\n",
    "    nf = NeuralForecast(models=[tft], freq='D')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2332f5-bf6f-4788-8f63-ae1e041fc3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a829e559-51ad-4b69-8b4d-be4f3886f8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d1838c-10cc-4ad8-8987-b8fa66aebbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo i best params\n",
    "with open('pickles/TFT_bestpars_Outliers_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3189c65d-df2d-4b74-a30f-ae0e3aba32a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico i best params\n",
    "with open('pickles/TFT_bestpars_Outliers_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7426808-4b08-4fa7-be5e-adbbfa488ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=100, #aumentare a 100\n",
    "        val_check_steps=25,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59125be-3496-4e66-83a6-95671d3894e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_tft,\n",
    "#     step_size=len(test_df_tft),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# # salvo le cv\n",
    "# with open('pickles/TFT_cv_Outliers_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(df_cv, file)\n",
    "\n",
    "# carico le cv\n",
    "with open('pickles/TFT_cv_Outliers_multistep.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_tft': mae,\n",
    "        'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9d153c9-2428-48a3-9519-a79cbececac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_tft['unique_id'].unique():\n",
    "    series_data = train_df_tft[train_df_tft['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_tft'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_tft'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6629c42-181d-4e56-8ee4-93c8221e531d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "557a5c58-d4c8-45f4-9926-b40676dc6c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64de4b8e-d117-4db4-b45b-99d7b1c59fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final training with best params\n",
    "import time\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(best_params['input_size']*2),\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=2, #best_params['n_rnn_layers']\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=0.01, #best_params['learning_rate'], \n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=80, #aumentare a 100\n",
    "        val_check_steps=25,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=16\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"D\")\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=best_params['input_size'])\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")\n",
    "tft_model = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39ea105-f0f2-4091-a1c8-8f1248e255f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # training senza best params\n",
    "# h = len(test_df_tft)\n",
    "# nf = NeuralForecast(\n",
    "#     models=[\n",
    "#         TFT(\n",
    "#             h=h,\n",
    "#             input_size=h*5,\n",
    "#             hidden_size=60,  # deve essere divisibile per 4\n",
    "#             grn_activation=\"ReLU\", #prima era ELU\n",
    "#             rnn_type=\"lstm\",\n",
    "#             n_rnn_layers=4,  #consigliato fra 2 e 5; \n",
    "#             one_rnn_initial_state=False,\n",
    "#             loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "#             learning_rate=0.005, \n",
    "#             hist_exog_list=hist_exog_features,\n",
    "#             futr_exog_list=futr_exog_list, #queste dovranno essere nel test\n",
    "#             max_steps=50, \n",
    "#             val_check_steps=10,\n",
    "#             batch_size=96, \n",
    "#             early_stop_patience_steps=15,  \n",
    "#             scaler_type=\"robust\",\n",
    "#             enable_progress_bar=True,\n",
    "#             accelerator=\"auto\",  \n",
    "#         ),\n",
    "#     ],\n",
    "#     freq='MS',\n",
    "# )\n",
    "\n",
    "# start_time = time.time()\n",
    "# #fit \n",
    "# nf.fit(df=train_df_tft, val_size=2*h)\n",
    "\n",
    "# end_time = time.time()\n",
    "# training_time = end_time - start_time\n",
    "# print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7387959-f492-49e2-bbc9-f082f58350af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft[['unique_id','ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f03cdc-9cc3-401c-9142-fe698110d8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo le prediction\n",
    "# with open('pickles/TFT_pred_Outliers_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d88835e-4baa-4b3c-99c8-658c7dec7d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd47bd8e-3782-4bc6-99d8-485985f90e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico le prediction, MASE = 0.6893\n",
    "with open('pickles/TFT_pred_Outliers_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf3ee31-0448-4817-b477-f1b0ef66b30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04ec3041-cdb1-49cb-8177-e23f4cb5806c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c98bf9-3ace-4364-828f-fe9f7b99b031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e56d17-045b-48f4-896e-b15a6ca20637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo il modello tft\n",
    "# with open('pickles/TFT_model_Outliers_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(tft_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe5594e-9442-413e-bd4d-0e35165e9bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico il modello tft\n",
    "with open('pickles/TFT_model_Outliers_multistep.pkl', 'rb') as file:\n",
    "    tft_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b9e9482-f054-43e0-a85d-9ab96f8f5ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = tft_model.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd07f07e-ee3e-4717-bd8b-48fc9ace2f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(tft_model, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acdb8eb1-5ed8-4cca-affc-85777c69313c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(tft_model, plot=4)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3579f6f4-47d5-4bf5-bc60-584eeb3b5d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(tft_model, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa4ab96-7ba8-4773-be9a-cb5e17889583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = tft_model.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0bcce62-ced8-4aa3-bf63-787ceaff8b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8decd9f4-a712-4e94-9f12-b0057877f1e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb0d0c5-11ba-4518-94f3-374930b43f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "009073cb-e939-441f-b09c-776cf879abf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33a60c0-8675-4aa2-9963-429d74059010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ceedbc6-642b-4eee-94ee-7753190eb43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a9ec2c-05b5-45e8-9059-888891102b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07157c0-283d-4e8b-bf19-97092509c249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4562efd9-91e6-48f1-bd82-a253bc316579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    tft_model\n",
    "    .attention_weights()[tft_model.input_size :, :]\n",
    "    .mean(axis=0)[: tft_model.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87729471-6d81-455e-896c-7ab1df643af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_importances4.shape)  # (n_time_steps, n_variables)\n",
    "print(mean_attention.shape)   # Deve essere (n_variables,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab66d27-5bb7-43ca-9f0b-3d1c2f7f4fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "\n",
    "# Estrai l'importanza delle future variables\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "\n",
    "# Estrai tutte le attention weights mediate nel tempo\n",
    "full_attention = tft_model.attention_weights()[tft_model.input_size:, :].mean(axis=0)\n",
    "\n",
    "# Estrai solo la parte relativa ai 12 time steps futuri\n",
    "n_future_steps = df_importances4.shape[0]\n",
    "mean_attention_future = full_attention[-n_future_steps:]\n",
    "\n",
    "# Applica la ponderazione riga per riga (sul tempo)\n",
    "df_importances4 = df_importances4.multiply(mean_attention_future, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    ax.bar(\n",
    "        np.arange(1, len(df_importances4) + 1), \n",
    "        df_importances4[col].values, \n",
    "        width=0.6, \n",
    "        label=col, \n",
    "        bottom=bottom\n",
    "    )\n",
    "    bottom += df_importances4[col].values\n",
    "\n",
    "# Titoli e assi\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Future time steps\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Linea media di attenzione\n",
    "ax.plot(\n",
    "    np.arange(1, len(df_importances4) + 1),\n",
    "    mean_attention_future,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36cae623-27a1-4039-aeeb-093bc869f945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tft_model.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc8fd488-0872-4893-8ad1-7296ecb56d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e598ff-a161-47b2-89cc-7399e0e28e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "item_id, timestamp, target (Chronos calcola tempo di fitting in automatico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56ef2be5-8b80-4afb-8371-4213dbf15b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56301049-0592-4154-b27f-6f35e5ae537e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eda02f5-1e21-43a7-8bea-141faa80f3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b907b71-47eb-44ba-9506-15f085efef69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(days=1),\n",
    "        periods=h,\n",
    "        freq=\"D\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"D\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(\n",
    "        train_df_chronos,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"robust\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    "        )\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'Fold': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('Fold',inplace=True)\n",
    "print(\"\\n=== TimeSeriesCV Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0086ac-e51f-4a6d-93ea-fa952c0937e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "931df9fa-1b74-477a-9477-982a05c3e505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='Fold', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a496c899-da70-42a1-b957-ed770c74fd05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f020ac-da89-4f54-8ea0-69e42ef1d1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparo dataset per training\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"D\"\n",
    ")\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae0d3143-b714-49b7-a21f-4fc351c79cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,\n",
    ")\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b890bb59-03b5-4cfa-9f68-fa16eb7c1b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI MULTI STEP \n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,  # Solo le covariate realmente note in anticipo\n",
    "    model=\"ChronosWithRegressor[bolt_mini]\"\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,\n",
    "    #model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "848b1c0d-998c-4bc0-a260-1f1ac0f14ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9e0552-234e-4083-9fff-5c5cb8289a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['0.5'], mode='lines', \n",
    "    name='P50', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressor', line=dict(color='lightgreen')))\n",
    "# QUANTILI CHRONOS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(test_df_chronos['timestamp']) + \n",
    "    list(test_df_chronos['timestamp'][::-1]),\n",
    "    y=list(predictions['0.9']) + list(predictions['0.1'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# PREVISIONI CHRONOS 0 SHOT\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d4892a-4847-4e37-a883-00fe2e3aa257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed1a15f-a389-4c9e-bfdc-5df50821e939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "#test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos_0shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e89787b-db0f-4c3b-bdc3-2babbeeabaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Il modello 0 shot è il migliore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f13dc6e-70c3-48b9-b4a4-dc3f7602aa94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a167b15a-43dd-445f-b95a-05ee5780ea6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_xgb.join(metriche_lstm).join(metriche_np).join(metriche_tft).join(metriche_chronos).join(metriche_chronos_0shot).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa4d1681-b26d-4f0e-8d32-09a1ad0d257c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR OIL PRICE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=forecast['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], \n",
    "                         y=Y_hat_df_tft['TFT'], \n",
    "                         #y=Y_hat_df_tft_TFT,\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS 0-shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca84c13-cd79-4daf-b50d-f1890119d149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f0a5613-d1c5-4cbc-84d8-1eef04bfbdca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = cv_metrics_df_lstm.merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_tft', 'RMSE_tft', 'MAE_lstm', 'RMSE_lstm',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'tft': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45bd10a9-fc3c-48b2-853d-707683a7ff2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos_0shot.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27d5b409-403b-4a06-a413-65a94c4a6aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **MISSING data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19411a7f-d44c-4f0e-82b1-ae3c15c4c2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = missdata.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df_np = create_time_series_features(df, target_col='y') \n",
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c6d818-2b8f-41ed-a857-9d0baf2d3b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ci servirà dopo per valutazione\n",
    "#Questo è il dataset vero, senza distorsioni (prima di creare i NaN)\n",
    "missing = loaded_data['favorita_train']\n",
    "missing = missing[['sales']]\n",
    "#sommo tutte le vendite di un singolo store per ogni giorno\n",
    "missing = missing.groupby(['date']).sum().reset_index()\n",
    "missing = missing.rename(columns={\"sales\": \"total_daily_sales\"})\n",
    "missing.set_index(\"date\", inplace=True)\n",
    "test_completo = missing[train_size:]\n",
    "train_completo = missing[:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "900518ef-367d-4d2d-8f20-b9a841f0db10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f05e3b9-63d2-40da-94c9-c0c3ff497a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6',\n",
    "       'lag_7', 'diff_1', 'dayofweek', 'month', 'quarter', 'day_sin',\n",
    "       'day_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    " \n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)    \n",
    "    \n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58f1a1f4-c40c-4050-baf1-859a3d9399ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Non si riesce a fare la cross validation con i NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "333cd78f-5c17-47e5-ab9f-249564a33bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40f0e03f-2eb7-4ff5-b475-06635b932f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"D\", epochs=150)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "019be002-c00d-4880-96d0-c0ad1ecd1ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74198b0c-6c42-4879-bc2a-f190388698d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(np_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e6410c-9862-44d2-8c92-254ea6f791de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_np['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_np['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab209b59-2844-4a74-a8bf-4a83439521a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_np[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "metriche_np = calcola_metriche(y_true = test_completo['total_daily_sales'], y_pred = merged_df['forecast'],\n",
    "                               y_train = train_completo['total_daily_sales'], modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f78f7858-9885-45d6-b8fa-62022d966f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nonostante i NaN fa un ottimo lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3063485c-436d-4766-a4d5-2ae472de575f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57bc53f-1244-47b9-8d5d-68cb672b10d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df = missdata.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos['item_id'] = \"serie_unica\"\n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "df_chronos = df_chronos.iloc[8:]\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cdb1abb-5f72-4199-ac9c-69d2f94f38d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"robust\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431a0515-f2cd-4e43-915f-b88262b3cc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI CLASSICHE\n",
    "# col miglior modello e col 0-shot\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_base]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7aa1117-c451-4d8a-b260-844ff40ac3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_chronos['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_chronos['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0 shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "358a2451-165f-47e9-bb12-9e48376701bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcola metriche\n",
    "# metriche di valutazione\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_chronos[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "\n",
    "test_completo2 = test_completo.iloc[1:] \n",
    "metriche_chronos = calcola_metriche(y_true = test_completo2['total_daily_sales'], \n",
    "                                    y_pred = merged_df_chronos['mean'],\n",
    "                                    y_train = train_completo['total_daily_sales'], \n",
    "                                    y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                    quantiles=[0.1, 0.5, 0.9], \n",
    "                                    modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eaaef32-c87f-411f-b62b-36b94035eeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcola metriche 0 shot\n",
    "# metriche di valutazione\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_chronos[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(y_true = test_completo2['total_daily_sales'], \n",
    "                                    y_pred = merged_df_chronos['mean'],\n",
    "                                    y_train = train_completo['total_daily_sales'], \n",
    "                                    y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                    quantiles=[0.1, 0.5, 0.9], \n",
    "                                    modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "573845a2-35bf-4f31-bac6-b9b42627cabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **MISSING DATA (multistep)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df15921a-43f8-4f7d-9641-4f77d9abccbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = missdata.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b617740-5947-4696-873d-68a61c1d1a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ci servirà dopo per valutazione\n",
    "#Questo è il dataset vero, senza distorsioni (prima di creare i NaN)\n",
    "missing = loaded_data['favorita_train']\n",
    "missing = missing[['sales']]\n",
    "#sommo tutte le vendite di un singolo store per ogni giorno\n",
    "missing = missing.groupby(['date']).sum().reset_index()\n",
    "missing = missing.rename(columns={\"sales\": \"total_daily_sales\"})\n",
    "missing.set_index(\"date\", inplace=True)\n",
    "test_completo = missing[-h:]\n",
    "train_completo = missing[:-h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4673150-a8fb-4b6d-afa5-cdb905c40d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86840817-22af-483d-83d9-3da7632ea988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    " \n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)    \n",
    "    \n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80819af6-db10-40a3-9e1f-64805a8bede7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Non si riesce a fare la cross validation con i NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3353884c-d3a1-4647-b997-f6b8389bb673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d783b2-ad1f-4258-b7b4-db3aefd24143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"D\", epochs=150)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01293067-aa97-457e-a474-2b018a0df0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "671c2d7b-6e81-4bf1-b255-3537b2a31ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(test_df_np[future_features].drop(columns=['covid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "711ba09a-0f55-4d73-9dcb-83074370a516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_np['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_np['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e114b956-fc7f-4e7f-8e13-01d88fc2f1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_np[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "metriche_np = calcola_metriche(y_true = test_completo['total_daily_sales'], y_pred = merged_df['forecast'],\n",
    "                               y_train = train_completo['total_daily_sales'], modelname=\"NeuralProphet\")\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c8231b-0f55-4ae2-ae39-3c3cc6b11f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nonostante i NaN fa un ottimo lavoro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f80d581-00af-4e70-8bea-f2a9fd2fed9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4340f7c4-665d-4de0-9ddb-c5a5a090e531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df = missdata.reset_index().rename(columns={'date': 'ds','total_daily_sales':'y'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos['item_id'] = \"serie_unica\"\n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "df_chronos = df_chronos.iloc[8:]\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73936aad-f62e-4fd1-addd-186e332ea429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"robust\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,  # Attiva ensemble per migliorare l'accuratezza\n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d54c35-5c5c-49f2-b095-7978cd17f15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI CLASSICHE\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]\n",
    "# col miglior modello e col 0-shot\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_known_covariates\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_known_covariates,\n",
    "    model=\"ChronosZeroShot[bolt_base]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc9ab909-8c09-4b7c-b827-a940784244c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_chronos['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_chronos['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='ChronosWithRegressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0 shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546216f8-f16e-40dc-bcdc-25a6888f57bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcola metriche\n",
    "# metriche di valutazione\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_chronos[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(y_true = test_completo['total_daily_sales'], \n",
    "                                    y_pred = merged_df_chronos['mean'],\n",
    "                                    y_train = train_completo['total_daily_sales'], \n",
    "                                    y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                    quantiles=[0.1, 0.5, 0.9], \n",
    "                                    modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c3b594-ce9c-4fd8-a918-5de0fa1319a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calcola metriche 0 shot\n",
    "# metriche di valutazione\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_chronos[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(y_true = test_completo['total_daily_sales'], \n",
    "                                    y_pred = merged_df_chronos['mean'],\n",
    "                                    y_train = train_completo['total_daily_sales'], \n",
    "                                    y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                    quantiles=[0.1, 0.5, 0.9], \n",
    "                                    modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5fa4d2-7308-4c74-9223-8a08f70e5e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **ELECTRICITY DEMAND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "89cc6acf-1fbe-459d-b83a-b3dc2a59ba3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df_elec.reset_index().rename(columns={'DATE_TIME': 'ds','Demand':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97c0be7d-b0f9-42de-824e-7a8385b46dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 2, 4, 24, 36, 48, 96]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features, 1, 4, 48\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_4'] = (df[target_col] - df[target_col].shift(4)).shift(1)\n",
    "    df['diff_48'] = (df[target_col] - df[target_col].shift(48)).shift(1)\n",
    "    df['diff_96'] = (df[target_col] - df[target_col].shift(96)).shift(1)\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['mese_del_anno']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['mese_del_anno']/12)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_4'] = df[target_col].rolling(window=4).mean().shift(1) #ultime 2 ore\n",
    "    df['rolling_mean_24'] = df[target_col].rolling(window=24).mean().shift(1) #ultime 12 ore\n",
    "    df['rolling_mean_48'] = df[target_col].rolling(window=48).mean().shift(1) #ultimo giorno\n",
    "    df['rolling_mean_96'] = df[target_col].rolling(window=96).mean().shift(1) #ultimi 2 giorni\n",
    "    df['rolling_std_4'] = df[target_col].rolling(window=4).std().shift(1)\n",
    "    df['rolling_std_24'] = df[target_col].rolling(window=24).mean().shift(1) \n",
    "    df['rolling_std_48'] = df[target_col].rolling(window=48).std().shift(1)\n",
    "    df['rolling_std_48'] = df[target_col].rolling(window=96).mean().shift(1) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e1cee8-6925-4ab1-bf54-a794ce41489c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d1f34ee-102b-421b-8c64-7a69ff3eb3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_arima = df[['y','ds','unique_id']].iloc[97:]\n",
    "train_size = int(len(df_arima) * 0.9)\n",
    "train_df = df_arima.iloc[:train_size]\n",
    "test_df = df_arima.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 48 # i have half hourly data \n",
    "horizon = len(test_df) # number of predictions\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62949ef8-c7d0-47d2-b9b0-040652279c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa2c9d3b-e72c-45d0-8b0c-d24aaee285c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definisco i modelli\n",
    "models = [\n",
    "    AutoARIMA(), # ci dovrebbe andare season_length=season_length, ma ci mette 30min+\n",
    "    AutoETS() #season_length=season_length\n",
    "]\n",
    "import time\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='30min')\n",
    "start_time = time.time()\n",
    "# fit\n",
    "sf.fit(train_df)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a0fba1-08f9-49e0-a92c-0a9ee3dd24b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd47f5b7-865f-4110-a785-ed29d5c25e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arima_string(sf.fitted_[0,0].model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0345861f-0a3c-4107-815b-5de80f613110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd53fa0-2b7c-4496-b977-2051faa39d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast\n",
    "Y_hat_df = sf.forecast(df=train_df, h=len(test_df), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "#salvo i valori fittati nel training in un pickle\n",
    "# with open('pickles/ARIMAETS_fit_elecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(values, file)\n",
    "values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd440f0-81d7-40ac-9910-a145047fef93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d59da1b-b2fd-43b7-b887-43236c281af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico i valori fittati se ne avessi bisogno\n",
    "# with open('ARIMAETS_fit_elecDemand.pkl', 'rb') as file:\n",
    "#     values = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a1f4481-d255-4393-a744-62e76fad6393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58876569-0983-47fe-a9a1-1e4e600fcb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "crossvalidation_df = sf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=horizon, \n",
    "    step_size=len(test_df),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = crossvalidation_df['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == fold]\n",
    "    \n",
    "    for model in ['AutoARIMA', 'AutoETS']:\n",
    "        # Filtra i dati per il modello corrente\n",
    "        model_data = fold_data[['y', f'{model}']]\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        # Calcola MAE e RMSE\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        \n",
    "        # Aggiungi i risultati\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': model,\n",
    "            f'MAE_{model}': mae,\n",
    "            f'RMSE_{model}': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_arima_ets = pd.DataFrame(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41735f3-a862-4d3e-9799-c5d8a13a03a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_arima_ets)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_arima_ets.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)\n",
    "\n",
    "# Ora creiamo un grafico che combina la serie temporale con le metriche di errore per visualizzare dove si verificano gli errori maggiori\n",
    "\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df['unique_id'].unique():\n",
    "    series_data = train_df[train_df['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data[f'MAE_{model}'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data[f'RMSE_{model}'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56874f3e-62e4-4307-a0d9-379ab138ab47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_intervals #EXPANDING WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7154640c-d022-4a89-8e6d-92fe57b93227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's forecast on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6c9fb83-a5e9-4599-bdb0-70763c49f362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "test_pred = sf.forecast(df=train_df, h=len(test_df)+96, level=[95])\n",
    "Y_hat_df = test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5891a7b2-fefa-4054-94b1-8453a7b9ad7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_arima = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoARIMA'],train_df['y'], \n",
    "                                modelname=\"ARIMA\").round(5)\n",
    "metriche_ets = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoETS'],train_df['y'], \n",
    "                                modelname=\"ETS\").round(5)\n",
    "metriche_arima, metriche_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06512102-93f0-4250-8b2c-9cd6f89f102d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(Y_hat_df, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# # Intervallo di confidenza AUTOARIMA \n",
    "# if 'AutoARIMA-lo-95' in forecast_df_etsA.columns and 'AutoARIMA-hi-95' in forecast_df_etsA.columns:\n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-lo-95'], \n",
    "#         mode='lines', line_color='green', showlegend=False))\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoARIMA-hi-95'],\n",
    "#         mode='lines', fill='tonexty', fillcolor='rgba(200,200,200,0.2)',\n",
    "#         line_color='green', name='95% CI AutoARIMA'\n",
    "#     ))\n",
    "\n",
    "# # Intervallo di confidenza AUTOETS \n",
    "# if 'AutoETS-lo-95' in forecast_df_etsA.columns and 'AutoETS-hi-95' in forecast_df_etsA.columns:\n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-lo-95'], \n",
    "#         mode='lines', line_color='orange', showlegend=False))\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(x=forecast_df_etsA['ds'], y=forecast_df_etsA['AutoETS-hi-95'],\n",
    "#         mode='lines', fill='tonexty', fillcolor='rgba(255,165,0,0.15)',\n",
    "#         line_color='orange', name='95% CI AutoETS'))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c388e462-03a8-4354-a24b-32a87288ef92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609fd060-35a4-493d-9fb8-ad6ed3d88c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag\n",
    "print(df_xgb.columns)\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418c8ba4-a768-4408-9f00-e48c8217a4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.9)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "331120c6-f963-4c50-8444-4ec9e4f47778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a76eee-246e-4765-82bc-7ec276671089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55231b94-622b-4d41-a814-64e2cefd52c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "562652ed-fcf0-4cc7-bc8f-1499ead28183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2814da7d-e6cc-4325-8b97-ffcb19f1ce1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5032f430-c8b4-462f-bcf2-b961cd9bb61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79380323-f47f-4d88-98d5-50ed9c68d62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f66c9bc-9c86-4f70-ba84-899dfdfb1007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0949655-178e-4a8e-811d-3810a7e743b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d829e592-06eb-43f7-9a58-aadacb4faf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeea2f7f-c91d-46ab-90a2-b02212897404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32fa1869-3dc2-4892-ac87-91b9fe7b402a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MULTI-STEP FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb8a284-cf8b-488d-bd6d-e5caf1ab1b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(minutes=30), periods=horizon, freq='30min') #dopo modificare\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=100,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b30f9d8-ad81-4494-a865-c25c914c0892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Forecasting, took 8/9 minutes\n",
    "# forecast_xgb = direct_multistep_forecast(train_df_xgb, feature_cols, target_col='y',horizon=len(test_df_xgb)+96)\n",
    "# with open('pickles/XGB_elecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(forecast_xgb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa4627a7-e423-49ab-a2d7-beb799d067c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico le previsioni\n",
    "# with open('pickles/XGB_elecDemand.pkl', 'rb') as file:\n",
    "#     forecast_xgb = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c09e6349-79c8-43e3-9857-4923f37f3678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RECURSIVE FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9220649-1962-434a-978d-26fc5414ccc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706b26d7-6b73-4f0a-98d5-c35ba8b20ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1771113c-0480-425c-832a-4c5986697494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b97ca5-b29c-4121-ac7a-4c04a0e11b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "# Forecast dirette\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d3bd198-b5ad-495f-9ce8-98d3a8e062b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ffe1dab-41e8-4d83-94c8-5a290d61d46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],\n",
    "                                train_df_xgb['y'], modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1cb72f-e365-4b04-ad72-13a897393359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM (neuralforecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e954c681-5310-4fa3-892c-2ef8ea37cad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df_lstm) * 0.9)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32461a62-509c-4269-819d-f2dd41a31d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf608839-e503-445c-9526-4e39e9ef6652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "h = len(test_df_lstm)\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    cv_metrics = []\n",
    "    # Hyperparametri da ottimizzare\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(2.5*h)] )\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 128])\n",
    "\n",
    "    # Fisso il seed per riproducibilità\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=h,  # nel tuning\n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=20, \n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='30min')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "287d16fc-eacf-46d2-b3f3-46769aedb83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c0e9e9b-40b2-4249-8e09-63747d0003df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0432136a-71aa-485c-88d5-7405d6eb0bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo i best params trovati da Optuna\n",
    "# with open('pickles/LSTM_bestPar_ElecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d344ed36-46f2-4c88-931b-c48292efdc82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a84529c-5ad8-4ff3-9006-1c793c3a98a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_ElecDemand.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91402297-8b3e-4520-942d-df97a81d4a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='30min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "643e0ada-6028-4062-9490-c12f1f121017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm,\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_ElecDemand.pkl', 'wb') as file:\n",
    "#      pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_ElecDemand.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccf9711-71af-4eb3-93d5-09813f6ef50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ed15a2-7a95-4bbd-980c-122adec80bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50, #aumentare a 150\n",
    "    early_stop_patience_steps=15,\n",
    "    batch_size=256,\n",
    "    learning_rate=best_params['learning_rate'], \n",
    "    recurrent=False\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='30min')\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0ba7b3-aa8d-4202-bee2-ee62937ed37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67f731f3-96cd-40bc-be40-f48e0a66d27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "#  Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)\n",
    "#salvo in pickle\n",
    "# with open('pickles/LSTM_elecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84cdb7c9-d47d-4761-9287-9ba61c394968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni:\n",
    "with open('pickles/LSTM_elecDemand.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)\n",
    "#queste previsioni sono state generate con 'max_steps' = 50, ma ci mette 15 minuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8c71ce-6e73-4641-a7ee-e17ed3c0abb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='LSTM MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "#     name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153c8dba-c9fb-4777-89e6-8c7f6156250e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_lstm['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b53193-a4e8-4227-91e7-a7ef855482d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22eb7ec-867b-4273-b881-9637f7950e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np.drop(columns=['unique_id'])\n",
    "df_np['settimana_del_anno'] = df_np['settimana_del_anno'].astype(int)\n",
    "df_np.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721b0ca1-0f75-405f-bedb-c2ecc41a631e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bdf622b-a3d1-4fcd-bfe3-93ee356c547e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87b63993-1abd-4f33-84d6-a5c16b779a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['Workday', 'Temperature', 'giorno_settimana', 'ora_del_giorno',\n",
    "       'settimana_del_mese', 'settimana_del_anno', 'giorno_del_mese',\n",
    "       'mese_del_anno', 'lag_1', 'lag_2', 'lag_4', 'lag_24', 'lag_36',\n",
    "       'lag_48', 'lag_96', 'diff_1', 'diff_4', 'diff_48', 'diff_96', 'day_sin',\n",
    "       'day_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "       'rolling_mean_4', 'rolling_mean_24', 'rolling_mean_48',\n",
    "       'rolling_mean_96', 'rolling_std_4', 'rolling_std_24', 'rolling_std_48']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.001, \n",
    "    batch_size=32,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9897059-03aa-4cb7-ae4d-6459ac9921be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Reset risultati\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = np_df_train.iloc[:train_end]\n",
    "    test_window = np_df_train.iloc[test_start:test_end]\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.001,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "        global_normalization=True,\n",
    "        unknown_data_normalization=True\n",
    "    )\n",
    "\n",
    "    for reg in regressors:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"30min\", epochs=100)\n",
    "\n",
    "    # Previsione\n",
    "    forecast = model.predict(test_window)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'split': i+1, 'MAE_np': mae, 'RMSE_np': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot\n",
    "results_df_np.plot(x='split', y=['MAE_np', 'RMSE_np'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f802714d-7409-487c-ad13-d07f3a218670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvo results df CV\n",
    "# with open('pickles/NP_CV_ElecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(results_df_np, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "558d51c1-a009-4c16-ad85-81e615b4cda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f472d8b1-b7dd-4c44-b99d-ea27d94074f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carico i risultati della CV\n",
    "with open('pickles/NP_CV_ElecDemand.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)\n",
    "results_df_np.plot(x='split', y=['MAE_np', 'RMSE_np'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ad3e37-1f85-44e7-9117-69192c9c6b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2bf0616-285f-40bd-9584-da903487689f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV PLOT\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f68f8c-579a-4e33-a90f-708f5095e622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a95dcfb0-f8f2-4bb9-ba49-b126d496e975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['Workday', 'Temperature', 'giorno_settimana', 'ora_del_giorno',\n",
    "       'settimana_del_mese', 'settimana_del_anno', 'giorno_del_mese',\n",
    "       'mese_del_anno', 'lag_1', 'lag_2', 'lag_4', 'lag_24', 'lag_36',\n",
    "       'lag_48', 'lag_96', 'diff_1', 'diff_4', 'diff_48', 'diff_96', 'day_sin',\n",
    "       'day_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "       'rolling_mean_4', 'rolling_mean_24', 'rolling_mean_48',\n",
    "       'rolling_mean_96', 'rolling_std_4', 'rolling_std_24', 'rolling_std_48']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "    \n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber',\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcf6d7e1-7e53-42f4-8daa-72b0622597a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"30min\", epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24f646d-ecf5-4d5b-a01e-8a044f219639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adb69ffc-74f1-4bf8-89c9-db5ace6dee80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast_test = neuralprophet.predict(np_df_test)\n",
    "# #salvo le prediction sul test in locale\n",
    "# with open('pickles/NP_elecDemand.pkl', 'wb') as file:\n",
    "#     pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f60f0b-3c81-4b65-a6ae-8ba918866822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134fb21b-141e-4f0d-8d60-643310729149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni del test\n",
    "with open('pickles/NP_elecDemand.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9baa3a8c-92ce-4a07-b098-654c093df3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e4fba0-5329-454f-9111-1d8798a42841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1967ba84-3881-4b75-b3ea-5c11b7ffbdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba0f4b0-ab4b-4254-9b9b-fefb38b82894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3535e9dd-9977-4e7a-8ac3-772ef00279cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "train_size = int(len(df_tft) * 0.9)\n",
    "train_df_tft = df_tft.iloc[:train_size]\n",
    "test_df_tft = df_tft.iloc[train_size:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a0280a28-ca0c-4990-ad1c-7cf6b4c428ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c943f93a-22d2-4783-ab9d-09a0b0c5dc20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  TimeSeriesCV - fine tuning Optuna (TROPPO LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8a839a62-3e01-4c15-954d-6033cf0f1ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(1.5*h), int(h/2)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=int(h/5),  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, \n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=128\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='30min')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft[5000:],\n",
    "        step_size=int(h/5),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "61f8b90b-0b36-431a-8852-d965e018a17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533e2432-f5ae-46a4-ab00-7c45f6234781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params\n",
    "#salvo i best params in pickle\n",
    "with open('pickles/TFT_bestPars_ElecDemand.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa78800-0018-4792-b962-00fb91672761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico i best params\n",
    "# with open('pickles/TFT_bestPars_ElecDemand.pkl', 'rb') as file:\n",
    "#     best_params = pickle.load(file)\n",
    "# with open('pickles/TFT_ElecDemand_best.pkl', 'rb') as file:\n",
    "#     best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b0015df-9576-4631-93e2-c7f78f87bd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le metriche di CV (fatte in colab)\n",
    "cv_metrics_df_tft = pd.read_pickle(\"pickles/TFT_ElecDemand_metrics_cv.pkl\")\n",
    "cv_metrics_df_tft.plot(x='Fold', y=['MAE_TFT', 'RMSE_TFT'], marker='o', title='Backtesting Errors')\n",
    "cv_metrics_df_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f73aa3-b82b-4768-b78e-eeeab196f7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La CV è troppo lenta, si blocca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99607f43-c224-45f7-af05-c1c96f8b370c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"30min\")\n",
    "\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft,\n",
    "    step_size=len(test_df_tft),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0684dbfc-d789-4a53-8580-970a66ee811b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71bd306f-0466-47fe-8eb5-ba40b0610507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8c036192-37da-4791-ba15-8aa5e407c246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Final training con best_params\n",
    "# best_tft = TFT(\n",
    "#         h=h,  \n",
    "#         input_size=best_params['input_size'],\n",
    "#         hidden_size=best_params['hidden_size'],\n",
    "#         n_rnn_layers=best_params['n_rnn_layers'],\n",
    "#         rnn_type='lstm',\n",
    "#         grn_activation=best_params['grn_activation'],\n",
    "#         loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "#         learning_rate=best_params['learning_rate'],\n",
    "#         futr_exog_list=futr_exog_list,\n",
    "#         max_steps=50, #aumentare a 100\n",
    "#         val_check_steps=25,\n",
    "#         batch_size=best_params['batch_size'],\n",
    "#         early_stop_patience_steps=-1,\n",
    "#         scaler_type=\"robust\",\n",
    "#         enable_progress_bar=True,\n",
    "#         accelerator=\"auto\",\n",
    "#         one_rnn_initial_state=False,\n",
    "#         windows_batch_size=64        \n",
    "# )\n",
    "# nf = NeuralForecast(models=[best_tft],freq=\"30min\")\n",
    "# nf.fit(df=train_df_tft, val_size=len(test_df_tft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b559b05e-a726-4f31-840a-e6a36db4ef0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TRAINING FATTO SU GPT T4 GOOGLE COLAB\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "h = len(test_df_tft)\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "\n",
    "# Configurazione conservativa per la memoria\n",
    "best_tft = TFT(\n",
    "    h=h,  \n",
    "    input_size=168,\n",
    "    hidden_size=16, \n",
    "    n_rnn_layers=2,\n",
    "    rnn_type='lstm',\n",
    "    grn_activation='ReLU',\n",
    "    loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "    learning_rate=0.005,\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=200,\n",
    "    val_check_steps=50,\n",
    "    batch_size=32,  \n",
    "    early_stop_patience_steps=-1,\n",
    "    scaler_type=\"standard\",\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    one_rnn_initial_state=False,\n",
    "    windows_batch_size=16,  \n",
    "    start_padding_enabled=True\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft], freq=\"30min\")\n",
    "\n",
    "# Pulire la memoria GPU prima di iniziare\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=len(test_df_tft))\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b85c89-6cfb-480a-a3f7-39cfc8662d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  | Name                    | Type                     | Params | Mode \\\n",
    "-----------------------------------------------------------------------------\\\n",
    "0 | loss                    | DistributionLoss         | 3      | train\\\n",
    "1 | padder_train            | ConstantPad1d            | 0      | train\\\n",
    "2 | scaler                  | TemporalNorm             | 0      | train\\\n",
    "3 | embedding               | TFTEmbedding             | 1.1 K  | train\\\n",
    "4 | temporal_encoder        | TemporalCovariateEncoder | 136 K  | train\\\n",
    "5 | temporal_fusion_decoder | TemporalFusionDecoder    | 4.3 K  | train\\\n",
    "6 | output_adapter          | Linear                   | 51     | train\\\n",
    "-----------------------------------------------------------------------------\\\n",
    "141 K     Trainable params\\\n",
    "3         Non-trainable params\\\n",
    "141 K     Total params\\\n",
    "0.566     Total estimated model params size (MB)\\\n",
    "584       Modules in train mode\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d7b32197-428b-4e54-8593-d0f95696a58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "908e4dcd-49f6-488c-a17c-72ab3fbac535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_tft = pd.read_csv('pickles/TFT_ElecDemand.csv')\n",
    "Y_hat_df_tft['ds'] = pd.to_datetime(Y_hat_df_tft['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "77498c23-7c71-46bc-a25f-a6224db146fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "21a6697d-8cff-4a68-9ad5-d2bceb7eaefb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f4930c96-1f45-4062-9b44-028df94b9b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b34ab9-e4b5-4fac-9dbc-978afd582712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1104c0-93a8-4560-9d75-f88f2f3284b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft = torch.load('pickles/best_tft_ElecDemand_model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1105cc73-8402-420e-ae12-04c4878f1d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = best_tft.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d25070-51e7-45af-91ff-3ccacbd466df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(best_tft, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce96003-0996-4f31-8ad3-8a21d21a6b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(best_tft, plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d80d947-1ffc-4ede-9c07-72126fefe4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(best_tft, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42cdcd10-b96b-4441-94ef-77a2db585767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = best_tft.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99741ac4-0121-44af-a9ea-4fe4f7dd3d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41cdb50-49b1-4018-a535-33d6cc1894a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "387aed0d-16af-4e57-a3f8-826ac0ec1dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90a60235-398c-4330-ba50-9960bd455f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528892d7-eaee-4843-94bb-883c4805982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfca6aaa-40b1-4098-8ed1-46972c5b524d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4037372-4923-4daf-b168-f6d2e0f0c155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47bc554f-7f63-4cf1-a9ea-0c5ef7106914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83ba7638-ad56-40b0-a630-ee2f8cb74acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "757de751-f6e9-410f-890b-29c168215d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d7feecf-7a0b-4863-b444-5aa6009cd0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f71660-b9b7-4132-be2b-d33ee8d7468f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95bd858c-5ddc-4f8b-868d-2b16722246c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_chronos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bb595a-7c2e-4bb4-b617-2783f992d954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definizione delle covariate note\n",
    "known_covariates_names = [\n",
    "'Workday', 'Temperature', 'giorno_settimana',\n",
    "       'ora_del_giorno', 'settimana_del_mese', 'settimana_del_anno',\n",
    "       'giorno_del_mese', 'mese_del_anno', 'lag_1', 'lag_2', 'lag_4', 'lag_24',\n",
    "       'lag_36', 'lag_48', 'lag_96', 'diff_1', 'diff_4', 'diff_48', 'diff_96',\n",
    "       'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "       'rolling_mean_4', 'rolling_mean_24', 'rolling_mean_48',\n",
    "       'rolling_mean_96', 'rolling_std_4', 'rolling_std_24', 'rolling_std_48']\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb4f7a1-5b80-4789-8616-dfc34f12f99f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd310a7f-ce71-48cf-b290-e463414d5c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7489e59-8c76-4b5c-9702-8d7ea2c9c0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(\"30min\"),\n",
    "        periods=h,\n",
    "        freq=\"30min\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"30min\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(train_df_fold,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"standard\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,)\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'split': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('split',inplace=True)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "#salvo results_df_chronos\n",
    "results_df_chronos.to_pickle('pickles/CHRONOS_CV_ElecDemand.pkl')\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1457830-417a-4ee9-859b-e1f2bd4c1565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico results_df_chronos\n",
    "results_df_chronos = pd.read_pickle('pickles/CHRONOS_CV_ElecDemand.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab1764e-389c-4bd2-9b04-65112ad71cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "n_splits = 5\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# creo dataframe finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eea199ed-e38a-4252-935d-f97e8574fe04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd0eae6-2403-4197-aca3-3f2ffe6b4dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='split', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc37946-8178-42c8-b058-2df0daedb3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a2fab70-0e3f-448e-8bc1-1bdf0e59d83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq=\"30min\"\n",
    ")\n",
    "\n",
    "train_df_chronos = TimeSeriesDataFrame(\n",
    "    train_df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True, \n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fa73f93-6e04-4b60-bc25-d47b2dac18c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI 1 CLASSICHE\n",
    "#train_df_chronos.set_index(['item_id','timestamp'],inplace=True)\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "#prediction 0 shot\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_base]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce2e3f4a-7378-4416-a958-01d73e6af076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7196d983-f93c-4d92-b80a-3514b6bea28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156658ec-e6ac-4d27-908d-45a3eb9c9308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0ede76-be3a-4108-861a-9d23f4e67062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cbc769b-3428-4a33-aecb-a27eb18ae743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c27f76ad-7e7d-4a7a-ad80-1aeb32b88dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd2e35f5-0031-40b8-a452-b8176dfd0556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR OIL PRICE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=forecast_test['ds'], \n",
    "                         #y=Y_hat_df_tft['TFT'], \n",
    "                         y=Y_hat_df_tft,\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    "# PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS 0-shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for Electricity demand\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dde8889-9755-465c-9af6-5594bec0eeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE_AutoARIMA','RMSE_AutoARIMA']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE_AutoETS','RMSE_AutoETS']]\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)\n",
    "df_arima.rename(columns={'MAE':'MAE_AutoARIMA','RMSE':'RMSE_AutoARIMA'}, inplace=True)\n",
    "df_ets.rename(columns={'MAE':'MAE_AutoETS','RMSE':'RMSE_AutoETS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b825ba-1d34-4d07-ba92-c9537f52161b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = df_ets.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_np', 'RMSE_np', 'MAE_TFT', 'RMSE_TFT', #'MAE_lstm', 'RMSE_lstm', #'MAE_AutoETS', 'RMSE_AutoETS',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'np': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c902ec1c-77d5-48a2-a631-e020fdfcc93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc8b93f-dd5b-4ab1-9530-c19e6c0d1a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f9fdbd4-82d8-4c59-a3eb-c9abb24b5c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **ELECTRICITY DEMAND (MULTISTEP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb8853b-17f7-4d55-9e3a-a5a7cf7e7e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futr_exog_list = ['Workday', 'ora_del_giorno', 'giorno_settimana', 'settimana_del_mese',\n",
    "                  'settimana_del_anno','giorno_del_mese','mese_del_anno',\n",
    "                  'day_sin','day_cos','hour_sin','hour_cos', 'month_sin','month_cos',]\n",
    "future_features = ['ds', 'y'] + futr_exog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a86e2e-adf5-4377-8531-78be9b332571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df_elec.reset_index().rename(columns={'DATE_TIME': 'ds','Demand':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "h = 336 #la settimana successiva (24*2*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca8a17f5-aad5-40c4-9d7c-c7273a3239c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 2, 4, 24, 36, 48, 96]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features, 1, 4, 48\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_4'] = (df[target_col] - df[target_col].shift(4)).shift(1)\n",
    "    df['diff_48'] = (df[target_col] - df[target_col].shift(48)).shift(1)\n",
    "    df['diff_96'] = (df[target_col] - df[target_col].shift(96)).shift(1)\n",
    "    \n",
    "    # Caratteristiche cicliche per rappresentare meglio la stagionalità\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['giorno_settimana']/7)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['ora_del_giorno']/24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['mese_del_anno']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['mese_del_anno']/12)\n",
    "    \n",
    "    # rolling means \n",
    "    df['rolling_mean_4'] = df[target_col].rolling(window=4).mean().shift(1) #ultime 2 ore\n",
    "    df['rolling_mean_24'] = df[target_col].rolling(window=24).mean().shift(1) #ultime 12 ore\n",
    "    df['rolling_mean_48'] = df[target_col].rolling(window=48).mean().shift(1) #ultimo giorno\n",
    "    df['rolling_mean_96'] = df[target_col].rolling(window=96).mean().shift(1) #ultimi 2 giorni\n",
    "    df['rolling_std_4'] = df[target_col].rolling(window=4).std().shift(1)\n",
    "    df['rolling_std_24'] = df[target_col].rolling(window=24).mean().shift(1) \n",
    "    df['rolling_std_48'] = df[target_col].rolling(window=48).std().shift(1)\n",
    "    df['rolling_std_48'] = df[target_col].rolling(window=96).mean().shift(1) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ee6b6af-aa17-43ee-8d48-bb4049d3dd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be28d791-5081-4c06-96dc-6d069bee1fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "hist_exog_features = df_xgb.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag, quindi le prime 12 obs.\n",
    "# STEP 2: Prepara DMatrix per previsione\n",
    "df_xgb = df_xgb[future_features]\n",
    "print(df_xgb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57f19488-c05c-4a99-a435-4088e27cb5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# train tutto tranne le ultime 12 osservazioni\n",
    "train_df_xgb = df_xgb.iloc[:-h]\n",
    "test_df_xgb = df_xgb.iloc[-h:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=df_xgb['ds'], y=y_train_xgb, mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=y_test_xgb, mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd5b75a8-d97f-4fb9-8298-6162666ecdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4afd15f-c74f-4aa9-9bfb-8c8c919f96ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7606fe6f-ea3a-4073-9b3a-19cab968f6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  \n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE_XGB': mae,\n",
    "            'RMSE_XGB': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  \n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff8d96a-5c83-45e7-9005-b587de256686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "438917af-3435-4d15-93ca-20a32b3fda75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1d9194-a597-4ab7-81ad-6bbfa0147fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5498e5e-9d61-4c98-896c-2e47dd24feb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52c557e4-0372-444a-8c02-ac4b015aee3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_XGB'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_XGB'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8532a62-11c1-4df3-b8c0-ef02817677b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d39b3f5-acd3-4a77-9de7-7af943243ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo il modello con i best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c1d14f5-4877-426d-a383-43057b122d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0102d6b7-98e3-4ab2-88b1-1e5e77ed911e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05a7911e-b5e4-4520-a680-92ae09ca21d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977033ab-bbbc-4bc6-ab8c-7e111a9180aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad07a6f-70a2-4dee-8e55-5590e17b6f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "# Forecast dirette\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4835af26-1da7-4b35-8640-271a57defbf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f21a02-9201-4fa0-8778-6810cea47da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "merged_df.dropna(inplace=True)\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],\n",
    "                                train_df_xgb['y'], modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e919f7c1-6cf4-4cb4-b27c-025ee4cf3ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da05114-16fd-4b58-bf2f-183105f1b1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "\n",
    "train_df_lstm = df_lstm.iloc[:-h]\n",
    "test_df_lstm = df_lstm.iloc[-h:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0879ce11-a536-4840-99bd-4a8283dbd03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e050a0b0-c681-425f-a8e3-2fc705436acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 64, 128, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h), int(h*2), int(h*5)])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=len(test_df_lstm), \n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='30min')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=len(test_df_lstm),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff235bf6-0ea0-4919-9137-f5ebe19f9539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d64646f-9a39-423a-b19f-f3b9aa5f3f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22ec187-0aa8-434a-b136-cc8e30ccbb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo i best params trovati da Optuna\n",
    "with open('pickles/LSTM_bestPar_ElecDemand_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086740a1-eef3-498e-9b4a-755ca77fc8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7c3d60-bcd7-45a7-9a09-f3424c317ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0073e619-ffe7-44d0-94bf-f678b19f9f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "best_lstm = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=best_params['input_size'],\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='30min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e98c484e-d7a3-45b2-9734-f5f7792d7202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm,\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_ElecDemand_multistep.pkl', 'wb') as file:\n",
    "#      pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "cv_metrics_df_lstm.iloc[1:2,2:] = cv_metrics_df_lstm.iloc[1:2,2:]/1000\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68be1473-c2d1-4df1-a525-e116b977f843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda74be7-f6ee-4eb1-aec7-8d299a6c96b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=len(test_df_lstm),\n",
    "    input_size=int(best_params['input_size']*5),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='standard',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=200, #aumentare a 150\n",
    "    early_stop_patience_steps=25,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.032, #best_params['learning_rate']\n",
    "    recurrent=True\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[final_model], freq='30min')\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_lstm, val_size=len(test_df_lstm))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd02de79-8154-4a78-a2d8-d0ef3b36e0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dceabd6-77dd-4ee9-8047-ed9d076a01f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])\n",
    "#salvo in pickle\n",
    "with open('pickles/LSTM_elecDemand_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b032b252-8a85-4408-8eef-576884ff2a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni:\n",
    "with open('pickles/LSTM_elecDemand_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d69d0563-f43e-4704-ab98-225cffb8fa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='LSTM MEAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5183d4-74d5-4a76-97bd-70d0a03640d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "merged_df_lstm.dropna(inplace=True)\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_lstm['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418c6d93-61d7-48de-9bbb-ab3b5dc02b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "028fbdb5-a6e8-47cc-9249-73d487d70547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9906fbd2-3cd4-48e8-9acf-ee3b7a440d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac4872d-6d00-4d45-86ab-13132e46a032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "initial_train_size = len(train_df_np) - 5 * len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = train_df_np.iloc[:train_end].copy()\n",
    "    test_window = train_df_np.iloc[test_start:test_end].copy()\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello NeuralProphet\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    # Aggiungi regressori\n",
    "    for reg in futr_exog_list:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"30min\", epochs=50)\n",
    "\n",
    "    # Predizione\n",
    "    future_df = test_window.copy()  # Deve contenere anche i regressori futuri\n",
    "    forecast = model.predict(future_df)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati CV\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot Errori\n",
    "results_df_np.plot(\n",
    "    x='Fold',\n",
    "    y=['MAE_nprophet', 'RMSE_nprophet'],\n",
    "    marker='o',\n",
    "    title='TimeSeriesCV Errors',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48280e9-afb6-4e6a-b506-537ca31b0308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Salvo results df CV\n",
    "with open('pickles/NP_CV_ElecDemand_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(results_df_np, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cec1b6-a40d-4542-9945-006b682e14ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carico i risultati della CV\n",
    "with open('pickles/NP_CV_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)\n",
    "    results_df_np = results_df_np.reset_index(drop=True)\n",
    "results_df_np.plot(x='Fold', y=['MAE_nprophet', 'RMSE_nprophet'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224bd5ce-a24e-4788-a6c5-7258346e579c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6fec72f-752a-4063-a994-815458afa46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.02, \n",
    "    batch_size=128,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"30min\",epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0320327-3616-4f41-b72b-8f2ef57956d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "358fe1b8-2e1c-4dd4-acd7-621b6b154616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(test_df_np[future_features])\n",
    "#salvo le prediction sul test in locale\n",
    "with open('pickles/NP_elecDemand_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2405b227-496f-4dd6-a420-b069917cb70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c42a3f-0c7b-453f-b2a7-6619a4322cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni del test\n",
    "with open('pickles/NP_elecDemand_multistep.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6132ad3-f972-4b0b-9b60-7b9ecdda5c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9e6ecf-083b-4d76-8ca5-ff83969a7047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b970161-d62f-4c7e-8e46-f92ff033fb31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c019317-f40a-4919-8cd4-1e7eb652fa3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "05b531dc-704a-41ed-9249-845af12eaa8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "hist_exog_features = df_tft.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist()\n",
    "# 2. Suddivisione in train/test\n",
    "train_df_tft = df_tft.iloc[:-h]\n",
    "test_df_tft = df_tft.iloc[-h:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "27d941fd-c525-494a-803a-69d4b9aeec16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40dcf6aa-93e2-485d-a0ef-c6077309ae84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  TimeSeriesCV - fine tuning Optuna (TROPPO LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2663e5d9-258d-4033-b735-82ba8f3549ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(h/2)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=32\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='15min')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "18d907d1-4f17-42e2-bada-b271bc3a8f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)  #aumentare a 50 su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4693977f-8075-459b-9475-60eaec268b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad8f8e5-8270-4dfe-9069-4990256b723f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_bestPars_ElecDemand_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd0aef3-e2e7-46ad-ad46-6681859f6f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico i best params\n",
    "with open('pickles/TFT_bestPars_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "efdfc593-19a5-47e3-8f46-abde05781730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size'], \n",
    "        hidden_size=32,\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list = hist_exog_features,\n",
    "        max_steps=200, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=8,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=8\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"30min\")\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft,\n",
    "    step_size=len(test_df_tft),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb22689b-c9fb-47b4-9796-9241bb29531f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_CV_ElecDemand_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(cv_metrics_df_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c45bd00-ee45-4fee-848b-65d8246a8fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico la CV\n",
    "with open('pickles/TFT_CV_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fc3f466-2cc7-4a0f-a33d-806f17eb394a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_df_tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033e01b5-5f9e-4ae6-8736-19abcac1d8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_tft['unique_id'].unique():\n",
    "    series_data = train_df_tft[train_df_tft['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_tft['Model'] = 'TFT'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_TFT'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_tft['Model'].unique():\n",
    "    model_data = cv_metrics_df_tft[cv_metrics_df_tft['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_TFT'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b24412-baf1-499c-9f9c-8a35b4416754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bbc94374-9adb-4a0e-b7fe-93bb9e8736e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=best_params['input_size']*6,\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        n_rnn_layers=best_params['n_rnn_layers'],\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=best_params['grn_activation'],\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=200, \n",
    "        val_check_steps=50,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        early_stop_patience_steps=25,\n",
    "        scaler_type=\"robust\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"30min\")\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, val_size=len(test_df_tft))\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "80f08172-1843-4005-bad3-ee8a9490adce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft[['unique_id','ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df9f410-cf22-4644-b64f-fe93804e9be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo le pred in pickle\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_pred_ElecDemand_multistep.pkl\"\n",
    "# with open(save_path, 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d7d8aa-54af-47e1-853d-7eecc2c97c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le pred\n",
    "with open(\"pickles/TFT_pred_ElecDemand_multistep.pkl\", 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e503b880-0931-44ad-85a9-ed99875b2744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4c99c836-25e5-49f2-93d3-348af2d132ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88898b68-0b92-4675-bfdb-2d18d3f5c6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c85c362-57b2-4dca-8586-4a0d7c571bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a511dcf5-4a23-434c-b869-92a6f4382706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f951b63-0701-48e0-85e4-6dce77140ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # NON MI FA SALVARE IL MODELLO PERCHE' TROPPO PESANTE\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_model_ElecDemand_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ad8447-317d-41f7-aadb-843f34d7cc98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # carico il modello tft\n",
    "# with open('pickles/TFT_model_ElecDemand_multistep.pkl', 'rb') as file:\n",
    "#     best_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802ad2f9-5d75-43f6-9209-721808c606b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = best_tft.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e95358-d7c4-4e80-8f6c-7aa3eb9d96d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(best_tft, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be6954c-1a79-4134-be6d-9920491cb8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(best_tft, plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0009aaea-c019-4f73-82a2-1a0e87850903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(best_tft, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23a99cda-5747-467b-b20e-8963cac65516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = best_tft.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2c2069-9512-4a95-8e0e-1089abb48507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e13ff83-643e-4800-9a6c-93db3ce0a495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9683cbb2-9bcf-4a05-baff-d97e4c2ba8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9573ce7-51bc-4e09-8475-3fe5376e067d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8bc63d-8d1e-480f-b279-13f9b0329fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b93bc2b-21a9-455c-80ea-9d4fa48de7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6739033a-3723-4f43-87ea-723c9e1c5dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04978605-9f2a-48d5-89d1-df663333ff2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2364911-9f54-420d-a557-ef96da02368c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "582b172e-480e-4b44-b64d-f1b18da7fef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97c3616-4f1c-424d-8563-dde0c734f60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7001cc-fb9c-4aad-af1f-5b24da5c2aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1d4e2f-f6c6-4cc6-8bdf-7a9aeffaf61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c382e636-9680-4913-a283-96baf3f37d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae4c517-36a9-409f-b527-a3155ae83e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8c9b13-9c45-4746-a111-fb7420fdade9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(\"30min\"),\n",
    "        periods=h,\n",
    "        freq=\"30min\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"30min\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(train_df_fold,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"standard\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,)\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'split': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('split',inplace=True)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "#salvo results_df_chronos\n",
    "results_df_chronos.to_pickle('pickles/CHRONOS_CV_ElecDemand_multistep.pkl')\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0698c19f-197f-49ce-9504-1393da5e67de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico results_df_chronos\n",
    "results_df_chronos = pd.read_pickle('pickles/CHRONOS_CV_ElecDemand_multistep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5395d95-c0ce-41cc-89a5-53010d900bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "n_splits = 5\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# creo dataframe finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame (before conversion):\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a47a7c2-81fd-4adf-b17e-9d448466a6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='split', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c43d2e1-5402-4c84-a7db-77d59afd096e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4293606-7037-4254-a9c1-62da5d9ebaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparo dataset per training\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"30min\"\n",
    ")\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,\n",
    ")\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a3f82b-e7fd-4efc-8bd3-daf2b0e0fa42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI MULTI STEP \n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates  # Solo le covariate realmente note in anticipo\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efa62daf-786d-4645-ab93-d2ca6b4d2f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a63c332-a891-4388-a567-3f5a4404d2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e6409f-182a-4df3-b790-260ab25a79f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4da9b6b8-4487-4d2d-92a5-d7611b0b9da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2080832-8386-4e43-b72f-caf81a3047c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127273b0-1461-41cb-967b-536d5142f124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_xgb.join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft).round(4)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c46a3e4c-f09f-483e-8f77-144221e593a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR OIL PRICE\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=forecast_test['ds'], \n",
    "                         #y=Y_hat_df_tft['TFT'], \n",
    "                         y=Y_hat_df_tft,\n",
    "                         mode='lines', \n",
    "    name='TFT', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    "# PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS 0-shot', line=dict(color='lightgreen')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for Electricity demand\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = cv_metrics_df_lstm.merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_TFT', 'RMSE_TFT', #'MAE_lstm', 'RMSE_lstm',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0428706-ca54-485d-8316-9f5792268fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "424cc9e1-cd27-424f-a31b-34d3100b3bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **F1 RACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b1af69-84b9-4ffb-a955-9e85d705c4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = f1[f1['Driver']=='LEC'] # prendiamo i dati di un solo pilota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b47f7e5-1f9e-4ce0-927e-4685343470be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 8]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_8'] = (df[target_col] - df[target_col].shift(8)).shift(1)  # circa 1 secondo prima\n",
    "    #df['diff_700'] = df[target_col] - df[target_col].shift(700) #circa il giro prima \n",
    "    \n",
    "    # Caratteristiche temporali dal timestamp\n",
    "    df['millisecond'] = df['ds'].dt.microsecond / 1000  # millisecondi\n",
    "    df['second'] = df['ds'].dt.second\n",
    "    df['minute'] = df['ds'].dt.minute\n",
    "    \n",
    "    # Caratteristiche cicliche per il tempo\n",
    "    df['second_sin'] = np.sin(2 * np.pi * df['second']/60)\n",
    "    df['second_cos'] = np.cos(2 * np.pi * df['second']/60)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['minute']/60)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['minute']/60)\n",
    "    \n",
    "    # progressione proporzionale lungo il giro\n",
    "    df['lap_position'] = (df['ds'] - df.groupby('LapNumber')['ds'].transform('min')) / \\\n",
    "                          (df.groupby('LapNumber')['ds'].transform('max') - df.groupby('LapNumber')['ds'].transform('min'))\n",
    "    # Permette di confrontare facilmente punti simili della pista tra giri diversi;\n",
    "    # aiuta a identificare pattern che dipendono dalla posizione nella pista\n",
    "    \n",
    "    # Rolling statistics (medie mobili)\n",
    "    window_sizes = [4, 8]\n",
    "    for window in window_sizes:\n",
    "        df[f'rolling_mean_{window}'] = df[target_col].rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        df[f'rolling_std_{window}'] = df[target_col].rolling(window=window, min_periods=1).std().shift(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe01e14-ac71-47a4-8371-3e6248f1d021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df.reset_index().rename(columns={'Date': 'ds','Speed':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.drop(columns=['Driver','Status'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a736e3-4d07-4a86-90bc-876f254fc97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### ARIMA + ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7dcff29-7f10-492a-9151-c3f36b96995a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_arima = df[['y','ds','unique_id']].iloc[9:]\n",
    "\n",
    "df_arima = TimeSeriesDataFrame(\n",
    "    df_arima,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_arima = df_arima.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_arima.reset_index(inplace=True)\n",
    "df_arima.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "\n",
    "train_size = int(len(df_arima) * 0.9)\n",
    "train_df = df_arima.iloc[:train_size]\n",
    "test_df = df_arima.iloc[train_size:]\n",
    "# queste due ci serviranno dopo\n",
    "season_length = 700 # \n",
    "horizon = len(test_df) # number of predictions\n",
    "train_df = pd.DataFrame(train_df)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023fb71f-c2aa-44a7-baa4-01ee01973200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf88722-1dc8-4668-91d1-e781f9de7450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# definisco i modelli\n",
    "models = [\n",
    "    AutoARIMA(), # ci dovrebbe andare season_length=season_length, ma ci mette 30min+\n",
    "    AutoETS() #season_length=season_length\n",
    "]\n",
    "# creo il forecaster\n",
    "sf = StatsForecast(models=models, freq='131ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca3cfae-2a95-408f-9e51-fc0138a4ab39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "import time\n",
    "start_time = time.time()\n",
    "sf.fit(train_df)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c79c0535-9758-48f6-81a1-94d9a3eb9837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ARIMA\n",
    "result=sf.fitted_[0,0].model_\n",
    "print(result.keys())\n",
    "print(\"Parametri ARMA:\",result['arma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ed07004-47a3-4416-a9a0-f8ede86a556b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arima_string(sf.fitted_[0,0].model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96f2fa9e-c83a-43c3-bb2b-a83026f89bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#summary of the model ETS\n",
    "result2 = sf.fitted_[0,1].model_\n",
    "print(result2.keys())\n",
    "print(\"Modello:\",result2['method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ce22ba-a7ea-4209-92ab-749cd332f20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e8323aa-6197-4368-a0cc-de8615780d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "crossvalidation_df = sf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=horizon, \n",
    "    step_size=len(test_df),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = crossvalidation_df['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == fold]\n",
    "    \n",
    "    for model in ['AutoARIMA', 'AutoETS']:\n",
    "        # Filtra i dati per il modello corrente\n",
    "        model_data = fold_data[['y', f'{model}']]\n",
    "        model_data = model_data.dropna()\n",
    "        \n",
    "        # Calcola MAE e RMSE\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data[f'{model}'])\n",
    "        \n",
    "        # Aggiungi i risultati\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': model,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_arima_ets = pd.DataFrame(cv_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5153a3e8-1749-4893-a28d-c833f3b6c70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_arima_ets)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_arima_ets.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics[['Model', 'MAE', 'RMSE']])\n",
    "\n",
    "# Ora creiamo un grafico che combina la serie temporale con le metriche di errore per visualizzare dove si verificano gli errori maggiori\n",
    "\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df['unique_id'].unique():\n",
    "    series_data = train_df[train_df['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_arima_ets['Model'].unique():\n",
    "    model_data = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "516a1e50-0ca6-4bdd-b48a-e39711dcaf51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fold_intervals #EXPANDING WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f759a69-0a1f-4d40-8c5b-10bf45b5eaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d938b876-5ff1-4e9a-a8d9-b88f18b8c7d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's forecast\n",
    "Y_hat_df = sf.forecast(df=train_df, h=len(test_df), fitted=True, level=[95])\n",
    "#see fitted values vs true values\n",
    "values=sf.forecast_fitted_values() #qui viene aggiunta anche la vera y\n",
    "#salvo i valori fittati nel training in un pickle\n",
    "# with open('pickles/ARIMAETS_fit_f1.pkl', 'wb') as file:\n",
    "#     pickle.dump(values, file)\n",
    "values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1865ad0b-bd0f-4cb5-b2e9-05a9d3ad3227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico i valori fittati se ne avessi bisogno \n",
    "# with open('pickles/ARIMAETS_fit_f1.pkl', 'rb') as file:\n",
    "#      values = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a68a123-dfa4-4ab8-853a-038f8f7423fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # prediction on test set\n",
    "test_pred = sf.forecast(df=train_df, h=len(test_df), level=[95])\n",
    "Y_hat_df = test_df.merge(Y_hat_df, how='inner', on=['unique_id', 'ds'])\n",
    "# with open('pickles/ARIMAETS_pred_f1.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eee2eb7-e57c-4469-995a-bd701a846763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea87e3fc-4616-42d6-9ef2-bb6457af11dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df = pd.read_pickle('pickles/ARIMAETS_pred_f1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38bafce3-ab92-49e7-b94d-90f3ba102cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "metriche_arima = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoARIMA'],train_df['y'], \n",
    "                                modelname=\"ARIMA\").round(5)\n",
    "metriche_ets = calcola_metriche(Y_hat_df['y'],Y_hat_df['AutoETS'],train_df['y'], \n",
    "                                modelname=\"ETS\").round(5)\n",
    "metriche_arima, metriche_ets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c74f39-c54f-4a72-b03a-0a8ecfaedc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "# Creiamo un dataframe completo che include training, test e previsioni future\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# Aggiungiamo le previsioni al dataframe completo\n",
    "forecast_df_etsA = full_df.merge(Y_hat_df, how='outer', on=['unique_id', 'ds'])\n",
    "\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df['ds'], y=train_df['y'], mode='lines', name='Training Data'))\n",
    "\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoARIMA'], mode='lines', \n",
    "    name='AutoARIMA Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(x=test_pred['ds'], y=test_pred['AutoETS'], mode='lines', \n",
    "    name='AutoETS Forecast', line=dict(color='orange', dash='dot')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"Forecast Comparison: AutoARIMA vs AutoETS\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf32136b-5ea8-41b5-afc2-1ddc8fd52efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73539b7e-b8dd-498b-a415-8ea68e96f64b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag\n",
    "df_xgb = TimeSeriesDataFrame(\n",
    "    df_xgb,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_xgb = df_xgb.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_xgb.reset_index(inplace=True)\n",
    "df_xgb.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "print(df_xgb.columns)\n",
    "# df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecd0587c-6929-4747-9295-92df688c4d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_size = int(len(df_xgb) * 0.9)\n",
    "train_df_xgb = df_xgb.iloc[:train_size]\n",
    "test_df_xgb = df_xgb.iloc[train_size:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48006c5-2f8c-46a3-9780-4bcf9016c7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e258212f-4bc5-4fa5-8a41-8875fd895eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd4b2b4-3633-4813-bf26-0357350487fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  # Puoi modificarlo se usi più modelli\n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e51802bf-f5b1-4f92-9544-8ae41860d6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=2))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54c626e4-3b12-433f-b5c3-590f2e3fa9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a4bf95-3d8b-46ed-a3d8-49e25fdbecb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a056a7c9-d967-4352-af98-f3f2858df2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b1eda2-6c8b-45c5-88fd-01bffff805d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label='Serie unica')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefc0f46-53ac-4a79-8ed6-2d7a9b6a327a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22156afe-7317-407b-ac0c-1f22c07d714d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo i best params\n",
    "# best_params = study.best_params\n",
    "# with open ('pickles/XGB_bestparams_f1.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c854b4e-f49e-4ee9-b819-e812c67d91da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico i best_params\n",
    "best_params = pd.read_pickle('pickles/XGB_bestparams_f1.pkl')\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 22})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17007776-c6e4-4a01-9233-c45ccdbdb94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f94fe92-3dd8-4d40-83a1-63860c614739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MULTI-STEP FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc621f7-59c9-4a5a-806e-e391834f918c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def direct_multistep_forecast(train_data, feature_cols, target_col, horizon=None):\n",
    "    \"\"\"\n",
    "    Addestra modelli separati per ogni orizzonte temporale futuro\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    models = []\n",
    "    \n",
    "    # Crea dataframe per date future\n",
    "    last_date = train_data['ds'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta('131ms'), periods=horizon, freq='131ms') #dopo modificare\n",
    "    \n",
    "    # Per ogni step futuro, addestra un modello dedicato\n",
    "    for h in range(1, horizon+1):\n",
    "        print(f\"Training model for horizon {h}\")\n",
    "        \n",
    "        # Prepara target con shift inverso per prevedere h passi avanti\n",
    "        df_horizon = train_data.copy()\n",
    "        df_horizon[f'y_horizon_{h}'] = df_horizon[target_col].shift(-h)\n",
    "        df_horizon = df_horizon.dropna()\n",
    "        \n",
    "        # Prendi features e target per questo orizzonte\n",
    "        X = df_horizon[feature_cols]\n",
    "        y = df_horizon[f'y_horizon_{h}']\n",
    "        \n",
    "        # Split train/validation\n",
    "        train_size = int(len(X) * 0.95)\n",
    "        X_train, X_val = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        \n",
    "        # Addestra modello\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        params = best_params\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            num_boost_round=30,\n",
    "            early_stopping_rounds=4,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "    # Crea input per la previsione (l'ultimo punto noto)\n",
    "    last_point = xgb.DMatrix(train_data.iloc[[-1]][feature_cols])\n",
    "    \n",
    "    # Prevedi ciascun orizzonte con il modello dedicato\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict(last_point)[0]\n",
    "        forecasts.append(pred)\n",
    "    return pd.DataFrame({'ds': future_dates, 'forecast': forecasts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1020ab-ca65-4610-93db-ac08839e7174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Forecasting, took 20/30 minutes\n",
    "# forecast_xgb = direct_multistep_forecast(train_df_xgb, feature_cols, target_col='y',horizon=len(test_df_xgb)+700)\n",
    "# with open('pickles/XGB_pred_f1.pkl', 'wb') as file:\n",
    "#     pickle.dump(forecast_xgb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "569436bb-654a-499c-9e85-65dd339ca412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico le previsioni\n",
    "# with open('pickles/XGB_pred_f1.pkl', 'rb') as file:\n",
    "#     forecast_xgb = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1718be6-bc09-457f-9cd7-cc2fed85ee83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RECURSIVE FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ac1b5de-ca1a-4b07-8c45-14db0560f9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04ee53a-234b-45b2-bce7-29bade00fc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e478cd3b-8797-4fc3-b18b-a3e8dfbaa5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e6b3f7-ea38-4654-b004-b51b849dfb2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "# Forecast dirette\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a205d515-c77c-4b24-bd3d-f424ce0935f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0dff29e-8327-449b-8a41-b950812d20e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],\n",
    "                                train_df_xgb['y'], modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6835c1c4-a8dc-4a0d-9743-efb8f318d7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9112c2-cca5-41d0-b6e0-d4a412551aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "df_lstm = TimeSeriesDataFrame(\n",
    "    df_lstm,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_lstm = df_lstm.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_lstm.reset_index(inplace=True)\n",
    "df_lstm.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "\n",
    "train_size = int(len(df_lstm) * 0.9)\n",
    "train_df_lstm = df_lstm.iloc[:train_size]\n",
    "test_df_lstm = df_lstm.iloc[train_size:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1377faf9-efe0-4111-80a4-049b1104adb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0841ede-b327-40a2-9893-47873cf52e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CV da lanciare solo su Databricks, qui si blocca tutto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ec1b97-dc1d-4c4d-9f6b-95d749c444b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "futr_exog_list = train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "folds = pd.DataFrame()\n",
    "h = len(test_df_lstm)\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    cv_metrics = []\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128,1024])\n",
    "\n",
    "    # Fisso il seed per riproducibilità\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=h,  # nel tuning\n",
    "        input_size=h,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=20, \n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='160ms')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=h,\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be8ad98-74f5-4a19-8cae-8413063d4856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0781d44-c2c1-4441-880b-6a19234b49ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "457af881-680c-46cb-bc0c-b73b0c840ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo i best params trovati da Optuna\n",
    "with open('pickles/LSTM_bestPar_F1.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0179567d-1442-4000-a168-0b375652faa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2249359e-bb3c-428a-8d60-44a3795d38ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('pickles/LSTM_bestPar_F1.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66fc1ae9-4ec6-4d40-a67f-8b39cd927da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "h = len(test_df_lstm)\n",
    "best_lstm = LSTM(\n",
    "    h=h,\n",
    "    input_size=h,\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=2048,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='160ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72792e19-6a22-4eed-82a0-5f722118ec1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm[10000:], #la CPU non regge tutte le unità\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_F1.pkl', 'wb') as file:\n",
    "#      pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_F1.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15bd16aa-8981-4b08-9086-125490a04d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49581fb7-f4c8-4e11-9efc-a949dbc879b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "h = len(test_df_lstm)\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "final_model = LSTM(\n",
    "    h=h,\n",
    "    input_size=h,\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=best_params['encoder_n_layers'],\n",
    "    encoder_hidden_size=best_params['encoder_hidden_size'],\n",
    "    decoder_hidden_size=best_params['decoder_hidden_size'],\n",
    "    decoder_layers=best_params['decoder_layers'],\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=100,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=256,\n",
    "    learning_rate=0.01,\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[final_model], freq='160ms')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(\n",
    "    df=train_df_lstm[12000:], #la cpu non regge tutto il dataset\n",
    "    val_size=len(test_df_lstm)\n",
    "    )\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b1f1c1-a38f-4d87-b22e-063c9a9c7c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "598e1e75-e7e3-4f75-a26a-74dda68f2cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)\n",
    "#salvo in pickle\n",
    "with open('pickles/LSTM_F1.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe8a0f8-8e46-4308-a1a7-293500aba398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni:\n",
    "with open('pickles/LSTM_f1.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)\n",
    "#queste previsioni sono state generate con 'max_steps' = 50, ma ci mette 15 minuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5293373e-f9b2-48a4-9bf7-1aac5001ef67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='LSTM Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "#     name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ec5ad1-cbef-4519-92fb-9d18b4257f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "#merged_df_lstm.dropna()\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_lstm['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c35cb93-5de0-4748-86e6-c6db62c08a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e3c6fc-c56f-439f-9375-37c066c8348b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = TimeSeriesDataFrame(\n",
    "    df_np,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_np = df_np.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_np.reset_index(inplace=True)\n",
    "df_np.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "#metto come booleana la variabile brake\n",
    "df_np['Brake'] = df_np['Brake'].astype(int)\n",
    "df_np.drop(columns=\"unique_id\",inplace=True)\n",
    "df_np.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78bd8e6f-7f30-417e-a5d0-a0c439371559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(df_np) * 0.9)\n",
    "train_df_np = df_np.iloc[:train_size]\n",
    "test_df_np = df_np.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056b0e9e-0716-4197-9660-af9759e46a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd2b122-395f-4f08-8624-1289905708b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976a2839-240e-458f-af6e-ecfb327fbbdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['DistanceToDriverAhead', 'RPM', 'nGear', 'Throttle', 'Brake',\n",
    "       'DRS', 'LapNumber', 'lag_1', 'lag_8', 'diff_1', 'diff_8', 'millisecond',\n",
    "       'second', 'minute', 'second_sin', 'second_cos', 'minute_sin',\n",
    "       'minute_cos', 'lap_position', 'rolling_mean_4', 'rolling_std_4',\n",
    "       'rolling_mean_8', 'rolling_std_8']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.001, \n",
    "    batch_size=16,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d157d833-d34d-400c-8390-e8ba3766abce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS VALIDATION\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Verifica\n",
    "required_length = initial_train_size + h * n_windows\n",
    "if len(np_df_train) < required_length:\n",
    "    raise ValueError(\"Dataset troppo corto per questo schema di cross-validation.\")\n",
    "\n",
    "# Reset risultati\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = np_df_train.iloc[:train_end]\n",
    "    test_window = np_df_train.iloc[test_start:test_end]\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.001,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "        global_normalization=True,\n",
    "        unknown_data_normalization=True\n",
    "    )\n",
    "\n",
    "    for reg in regressors:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"15min\", epochs=100)\n",
    "\n",
    "    # Previsione\n",
    "    forecast = model.predict(test_window)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'split': i+1, 'MAE_np': mae, 'RMSE_np': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot\n",
    "results_df_np.plot(x='split', y=['MAE_np', 'RMSE_np'], marker='o', title='Backtesting Errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a14616-8106-4d96-bb42-0cd5c5fbb2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvo results df CV\n",
    "# with open('pickles/NP_CV_F1.pkl', 'wb') as file:\n",
    "#     pickle.dump(results_df_np, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e972fc-a6f4-432d-884c-53376bb7f972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carico i risultati della CV\n",
    "with open('pickles/NP_CV_F1.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a227fd0-5dd4-4dbb-bd33-1c0c060eaf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in np_df_train.columns:\n",
    "        start_date = np_df_train['ds'].iloc[test_start]\n",
    "        end_date = np_df_train['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = np_df_train.index[test_start]\n",
    "        end_date = np_df_train.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b811118-a6ec-4196-b40b-3835e515e1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "results_df_np.plot(x='split', y=['MAE_np', 'RMSE_np'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7bdbfe-e8cb-4a08-b989-6e1e614c05fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3a7beb7-e74e-48c2-a257-c9bf7f9511d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preparazione dataset\n",
    "np_df_train = train_df_np[['ds', 'y']].copy()\n",
    "np_df_test = test_df_np[['ds', 'y']].copy()\n",
    "\n",
    "# Aggiungi le colonne dei regressori\n",
    "regressors = ['DistanceToDriverAhead', 'RPM', 'nGear', 'Throttle', 'Brake',\n",
    "       'DRS', 'LapNumber', 'lag_1', 'lag_8', 'diff_1', 'diff_8', 'millisecond',\n",
    "       'second', 'minute', 'second_sin', 'second_cos', 'minute_sin',\n",
    "       'minute_cos', 'lap_position', 'rolling_mean_4', 'rolling_std_4',\n",
    "       'rolling_mean_8', 'rolling_std_8']\n",
    "\n",
    "for reg in regressors:\n",
    "    np_df_train[reg] = train_df_np[reg]\n",
    "    np_df_test[reg] = test_df_np[reg]\n",
    "    \n",
    "\n",
    "# Fisso il seed per riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Definisci il modello\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],\n",
    "    learning_rate=0.01,\n",
    "    batch_size=16,\n",
    "    loss_func='Huber'\n",
    ")\n",
    "\n",
    "# Aggiungi i regressori\n",
    "for reg in regressors:\n",
    "    neuralprophet.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9c5d7f-9116-459d-8299-db47248d9231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"160ms\", epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb1f909-5033-42ab-8658-f60d8e8221e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b8bc64-5dc5-4e7e-8e09-2c1e56d5f295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(np_df_test)\n",
    "# #salvo le prediction sul test in locale\n",
    "with open('pickles/NP_f1.pkl', 'wb') as file:\n",
    "    pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7307a7-6a28-4a72-9121-020bd44dc095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf8048c5-5be4-4eca-bd43-457d8ffc7577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni del test\n",
    "with open('pickles/NP_f1.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c74c6f3-03f8-452b-bbf3-dc004c28d695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a5d1800-b78a-45f0-838d-a23d85acf6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed66607-9936-4786-9c00-3001030652bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc498535-5040-47b3-b6c9-e93af7901679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2dabc15-6ae1-4fe8-9e39-3922ded554d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_tft = df_tft.dropna().reset_index(drop=True) \n",
    "df_tft['item_id'] = df_tft['unique_id'] \n",
    "df_tft.drop(columns=\"unique_id\", inplace=True) \n",
    "df_tft.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_tft = TimeSeriesDataFrame(\n",
    "    df_tft,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_tft = df_tft.convert_frequency(\"160ms\").interpolate()\n",
    "df_tft.reset_index(inplace=True)\n",
    "df_tft.rename(columns={'timestamp': 'ds','item_id':'unique_id','target':'y'}, inplace=True)\n",
    "train_size = int(len(df_tft) * 0.9)\n",
    "train_df_tft = df_tft.iloc[:train_size]\n",
    "test_df_tft = df_tft.iloc[train_size:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8219f15e-4a30-4e5b-9730-c6ccfbe8f466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5955e051-07da-4ec5-8c06-d60575777ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico la cv\n",
    "cv_metrics_df_tft = pd.read_pickle(\"pickles/cv_metrics_TFT_F1.pkl\")\n",
    "cv_metrics_df_tft.plot(x='Fold', y=['MAE_tft', 'RMSE_tft'], marker='o', title='Chronos Backtesting Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7f7bd2-9953-430b-92cd-b0e60b73ff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### MAIN Training (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729eacb6-7001-4a96-874d-548d7d3538f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training per GPU\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "h = len(test_df_tft)\n",
    "futr_exog_list = train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist()\n",
    "\n",
    "# Configurazione conservativa per la memoria\n",
    "best_tft = TFT(\n",
    "    h=h,  \n",
    "    input_size=800, #should be 800\n",
    "    hidden_size=8, \n",
    "    n_rnn_layers=1,\n",
    "    rnn_type='lstm',\n",
    "    grn_activation='ReLU',\n",
    "    loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "    learning_rate=0.01,\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=200,\n",
    "    batch_size=4,  \n",
    "    early_stop_patience_steps=-1,\n",
    "    scaler_type=\"standard\",\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    one_rnn_initial_state=False,\n",
    "    windows_batch_size=4,  \n",
    "    start_padding_enabled=True\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft], freq=\"160ms\")\n",
    "# Pulire la memoria GPU prima di iniziare IL FIT\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "start_time = time.time()\n",
    "\n",
    "#fit \n",
    "nf.fit(df=train_df_tft[15000:], # nel training scartato 15k unità per memoria GPU\n",
    "       val_size=len(test_df_tft)) \n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa4e6ba-5812-48cc-8cc5-229bb8237a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  | Name                    | Type                     | Params | Mode \\\n",
    "-----------------------------------------------------------------------------\\\n",
    "0 | loss                    | DistributionLoss         | 3      | train\\\n",
    "1 | padder_train            | ConstantPad1d            | 0      | train\\\n",
    "2 | scaler                  | TemporalNorm             | 0      | train\\\n",
    "3 | embedding               | TFTEmbedding             | 384    | train\\\n",
    "4 | temporal_encoder        | TemporalCovariateEncoder | 28.7 K | train\\\n",
    "5 | temporal_fusion_decoder | TemporalFusionDecoder    | 1.2 K  | train\\\n",
    "6 | output_adapter          | Linear                   | 27     | train\\\n",
    "-----------------------------------------------------------------------------\\\n",
    "30.3 K    Trainable params\\\n",
    "3         Non-trainable params\\\n",
    "30.3 K    Total params\\\n",
    "0.121     Total estimated model params size (MB)\\\n",
    "440       Modules in train mode\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bb799f-59b5-4bf5-8748-e2776f8cc7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76279496-9aa8-4ecc-ae38-863319135eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico le previsioni fatte da Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b0281f-1ef5-4b9b-b7de-a7f01e4d7d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_tft = pd.read_csv('pickles/TFT_F1_best.csv')\n",
    "Y_hat_df_tft['ds'] = pd.to_datetime(Y_hat_df_tft['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64755198-8eed-4523-8f76-9eca36fff9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e163ab0c-0505-42ff-9200-035454fee2b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f38adad-a5d5-428b-9ad3-f4b84bcfd650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "#merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba993814-4c85-48ea-acf3-d2f219452479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5276ad-e528-40e2-b9f9-7631434ab085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft = torch.load('pickles/best_tft_F1_model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d74506-65b9-459a-8c54-393ecd0aa6c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = best_tft.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf0bdf60-03b8-4ba7-8671-0c82b94c3507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(best_tft, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfefac1f-ca36-42f4-bff2-ee11c392c48a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(best_tft, plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39e9aa2d-e4ee-4ef2-a0d7-5a933502b6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(best_tft, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d35a96-06ee-4aac-a625-fb11994df1ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = best_tft.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc079a7-1ee8-4f6e-95e1-c21ffea9728c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf59a09-c200-4db5-ab4f-a9952a8474ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d47a4f0-3364-47c4-92c8-41f90586cdcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b9615b-f4fe-4700-90c9-9a00d61fd18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a45a39f-dddd-42c1-843f-10f033dcecf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb76fee-79d6-4b98-863f-8edb12ef26d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa5718fa-c596-4535-8101-8edb44dfbf9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a6cc404-f67b-4b6e-915d-6023c1f6280b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "397cdb28-eb81-4481-ad4d-1588e9d212ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adab2f02-6432-4fb8-b6ac-ab357ad7570b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89e5d070-7dab-4104-8a55-2eaa6101d27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "399ce2b1-aa1c-4adc-9548-f8f04c1c8573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fb440e-75c2-4fe6-b99c-637e1cd879ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_chronos = df_chronos.convert_frequency(\"160ms\").interpolate()\n",
    "df_chronos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f8566eb-c91f-4047-a861-ccf0be97548a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train e test\n",
    "known_covariates_names = [\n",
    "    'DistanceToDriverAhead', 'RPM', 'nGear', 'Throttle', 'Brake',\n",
    "       'DRS', 'LapNumber', 'lag_1', 'lag_8', 'diff_1', 'diff_8', 'millisecond',\n",
    "       'second', 'minute', 'second_sin', 'second_cos', 'minute_sin',\n",
    "       'minute_cos', 'lap_position', 'rolling_mean_4', 'rolling_std_4',\n",
    "       'rolling_mean_8', 'rolling_std_8']\n",
    "\n",
    "# Split train e test\n",
    "train_size = int(len(df_chronos) * 0.9)\n",
    "train_df_chronos = df_chronos.iloc[:train_size]\n",
    "test_df_chronos = df_chronos.iloc[train_size:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "351cbf13-82fa-4ef2-98c2-9955f21c09b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02a073e-b425-4041-abba-c260ca1ec9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35564943-8002-4fb5-9910-8c9bc81cfc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(\"160ms\"),\n",
    "        periods=h,\n",
    "        freq=\"160ms\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"160ms\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(train_df_fold,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"standard\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,)\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'split': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('split',inplace=True)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9c828a-ccca-46b6-b5a7-73131ab78c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee6426a-5005-4dff-bb78-3f05c9c3553a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='split', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "285380f0-d683-483e-a1cf-838e0cd2b2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9a83766-fb4e-472b-b705-2503701c6f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d96fde-68c8-452e-859a-f26e2313bc6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "\n",
    "# SOLO PER DATABRICKS\n",
    "# sys.modules['sklearn.metrics._regression'].mean_absolute_error = sklearn.metrics.mean_absolute_error\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq=\"160ms\"\n",
    ")\n",
    "\n",
    "train_df_chronos = TimeSeriesDataFrame(\n",
    "    train_df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True, \n",
    ")\n",
    "\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a619781-a818-469c-811b-6c95f2037d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI 1 CLASSICHE\n",
    "# train_df_chronos.set_index(['item_id','timestamp'],inplace=True)\n",
    "# test_df_chronos.set_index(['item_id','timestamp'],inplace=True)\n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "#prediction 0 shot\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model=\"ChronosZeroShot[bolt_base]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a37977-96ed-4a43-8378-e19f0af02e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cbb66b1-0e7f-43f1-9174-53385eea1d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a0c2a7-f7bd-4968-b2b8-070ba78f558f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab854dff-1090-42b6-86d1-f1434b3de63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55bbba7e-cdf1-44be-9c18-c61cae721e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f28a3a-ddf6-4b74-a72f-fb5a8f9bcad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_arima.join(metriche_ets).join(metriche_xgb).join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe93776e-2583-4404-8616-26367b36b734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR SOLAR ENERGY\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI AUTOARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoARIMA'], \n",
    "    mode='lines', \n",
    "    name='ARIMA', \n",
    "    line=dict(color='grey')\n",
    "))\n",
    "\n",
    "# PREVISIONI AUTOETS\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_df_etsA['ds'], \n",
    "    y=forecast_df_etsA['AutoETS'], \n",
    "    mode='lines', \n",
    "    name='ETS', \n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    " # PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='CHRONOS 0-SHOT', line=dict(color='lightgreen')))\n",
    "   \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9085f827-e102-4076-ba03-f672548d24a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "df_arima = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoARIMA'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "df_ets = cv_metrics_df_arima_ets[cv_metrics_df_arima_ets['Model'] == 'AutoETS'][['Fold',\n",
    "                                                                    'MAE','RMSE']]\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "# df_ets.rename\n",
    "# df_arima.rename\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB'}, inplace=True)\n",
    "df_arima.rename(columns={'MAE':'MAE_AutoARIMA','RMSE':'RMSE_AutoARIMA'}, inplace=True)\n",
    "df_ets.rename(columns={'MAE':'MAE_AutoETS','RMSE':'RMSE_AutoETS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ea6ad5-9048-486b-890c-bfdd927a5f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = df_ets.merge(cv_metrics_df_lstm, \n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(df_arima,\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_np', 'RMSE_np', 'MAE_tft', 'RMSE_tft',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'AutoARIMA': '#808080',      # Grey\n",
    "    'AutoETS': '#ff7f0e',        # Orange\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'np': '#7fffd4',         # Aquamarine\n",
    "    'tft': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb1076b-ab95-45e4-bea9-b7ab1821199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fd7f580-de3e-4b50-8435-814faefbb532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **F1 RACE (MULTISTEP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b242bab-ab56-41ae-98e2-c97349592c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "futr_exog_list = ['millisecond', 'second', 'minute', 'second_sin', \n",
    "                  'second_cos','minute_sin','minute_cos','lap_position']\n",
    "future_features = ['ds', 'y'] + futr_exog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da8a197-c341-4700-8284-a18a606852e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = f1[f1['Driver']=='LEC'] # prendiamo i dati di un solo pilota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e65fc79b-eea4-40ab-8d76-7216bcc4407d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione per creare feature\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    lags = [1, 8]\n",
    "    # Lag features (1-4)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Differencing features\n",
    "    df['diff_1'] = (df[target_col] - df[target_col].shift(1)).shift(1)\n",
    "    df['diff_8'] = (df[target_col] - df[target_col].shift(8)).shift(1)  # circa 1 secondo prima\n",
    "    #df['diff_700'] = df[target_col] - df[target_col].shift(700) #circa il giro prima \n",
    "    \n",
    "    # Caratteristiche temporali dal timestamp\n",
    "    df['millisecond'] = df['ds'].dt.microsecond / 1000  # millisecondi\n",
    "    df['second'] = df['ds'].dt.second\n",
    "    df['minute'] = df['ds'].dt.minute\n",
    "    \n",
    "    # Caratteristiche cicliche per il tempo\n",
    "    df['second_sin'] = np.sin(2 * np.pi * df['second']/60)\n",
    "    df['second_cos'] = np.cos(2 * np.pi * df['second']/60)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['minute']/60)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['minute']/60)\n",
    "    \n",
    "    # progressione proporzionale lungo il giro\n",
    "    df['lap_position'] = (df['ds'] - df.groupby('LapNumber')['ds'].transform('min')) / \\\n",
    "                          (df.groupby('LapNumber')['ds'].transform('max') - df.groupby('LapNumber')['ds'].transform('min'))\n",
    "    # Permette di confrontare facilmente punti simili della pista tra giri diversi;\n",
    "    # aiuta a identificare pattern che dipendono dalla posizione nella pista\n",
    "    \n",
    "    # Rolling statistics (medie mobili)\n",
    "    window_sizes = [4, 8]\n",
    "    for window in window_sizes:\n",
    "        df[f'rolling_mean_{window}'] = df[target_col].rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        df[f'rolling_std_{window}'] = df[target_col].rolling(window=window, min_periods=1).std().shift(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "01d5534e-2ed2-4d21-b42d-de94b12d1a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = df.reset_index().rename(columns={'Date': 'ds','Speed':'y'})\n",
    "df['unique_id'] = 'serie_1'\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.drop(columns=['Driver','Status'],inplace=True)\n",
    "h = 1500 #(i due giri successivi circa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542a2b9f-e396-4315-8ae3-cea48b9f4198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b036859-8e44-487d-9ddb-60b10594a16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_xgb\n",
    "df_xgb = create_time_series_features(df, target_col='y') \n",
    "df_xgb = df_xgb.dropna().reset_index(drop=True) #droppo i NaN creati dai lag\n",
    "df_xgb = TimeSeriesDataFrame(\n",
    "    df_xgb,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_xgb = df_xgb.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_xgb.reset_index(inplace=True)\n",
    "df_xgb.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "\n",
    "hist_exog_features = df_xgb.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist() #dovrebbe funzionare\n",
    "\n",
    "df_xgb.drop(columns=['unique_id'],inplace=True)\n",
    "df_xgb = df_xgb[future_features]\n",
    "print(df_xgb.columns)\n",
    "# df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ee1ccc-843a-4921-a084-6736a57e6609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_df_xgb = df_xgb.iloc[:-h]\n",
    "test_df_xgb = df_xgb.iloc[-h:]\n",
    "\n",
    "# Separazione features e target\n",
    "feature_cols = [col for col in df_xgb.columns if col not in ['y', 'ds']]\n",
    "X_train_xgb, y_train_xgb = train_df_xgb[feature_cols], train_df_xgb['y']\n",
    "X_test_xgb, y_test_xgb = test_df_xgb[feature_cols], test_df_xgb['y']\n",
    "X_train_xgb.shape, y_train_xgb.shape, X_test_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train e test plot\n",
    "fig = go.Figure()\n",
    "#train dat\n",
    "fig.add_trace(go.Scatter(x=df_xgb['ds'], y=y_train_xgb, mode='lines', name='Train'))\n",
    "#test data \n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=y_test_xgb, mode='lines', name='Test'))\n",
    "#layout\n",
    "fig.update_layout(template='plotly_dark', title='Train and Test Data Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e62479-32f1-4b9c-aae6-cbe6440d99c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "\n",
    "#features\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_xgb)\n",
    "X_test_scaled = scaler_x.transform(X_test_xgb)\n",
    "\n",
    "# target, non serve che scalo y_test\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_xgb.values.reshape(-1, 1))\n",
    "\n",
    "# Ricreo df con stessi indici e nomi\n",
    "X_train_xgb = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train_xgb.index)\n",
    "X_test_xgb = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test_xgb.index)\n",
    "y_train_xgb = pd.Series(y_train_scaled.flatten(), name='y', index=y_train_xgb.index)\n",
    "\n",
    "#aggiorno train_df_xgb\n",
    "train_df_xgb[feature_cols] = X_train_xgb\n",
    "train_df_xgb['y'] = y_train_xgb\n",
    "\n",
    "#controllo le shape\n",
    "X_train_xgb.shape, X_test_xgb.shape, y_train_xgb.shape, y_test_xgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8417e52-398c-446c-9604-269ce0fa74ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ottimizzazione Optuna (Bayesiana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc50c7b-7253-4e7f-bfe4-46ad822a97d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "cv_metrics_log = []  # List globale per salvare metriche fold per fold\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5,test_size=len(test_df_xgb))\n",
    "    y = train_df_xgb['y']\n",
    "    df_xgb_feature = train_df_xgb.drop(columns=['y','ds'])\n",
    "    all_rmse = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_xgb_feature)):\n",
    "        X_train, X_test = df_xgb_feature.iloc[train_idx], df_xgb_feature.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "        dtest = xgb.DMatrix(X_test.values, label=y_test.values)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=500,\n",
    "                          evals=[(dtest, 'eval')],\n",
    "                          early_stopping_rounds=30, verbose_eval=False)\n",
    "\n",
    "        preds = model.predict(dtest)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = calcola_mae(y_test, preds)  # tua funzione custom\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        # Recupera le date (se esiste colonna 'ds')\n",
    "        start_date = df_xgb.iloc[test_idx]['ds'].min() if 'ds' in df_xgb.columns else None\n",
    "        end_date = df_xgb.iloc[test_idx]['ds'].max() if 'ds' in df_xgb.columns else None\n",
    "\n",
    "        # Logga le metriche della fold\n",
    "        cv_metrics_log.append({\n",
    "            'Trial': trial.number,\n",
    "            'Fold': fold + 1,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'Model': 'XGBoost'  # Puoi modificarlo se usi più modelli\n",
    "        })\n",
    "    return np.mean(all_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b4835e-7f27-49c5-b9e9-50244f96876c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=2))\n",
    "study.optimize(objective, n_trials=50) #aumentare su databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pd.read_pickle('pickles/XGB_bestparams_f1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cc8f82-42ec-41d3-9984-d20fe3217d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best trial:\",study.best_trial.number)\n",
    "besttrial = study.best_trial.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1acf658e-ae9f-4a29-bea7-3e374924792b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_log = pd.DataFrame(cv_metrics_log)\n",
    "cv_metrics_log = cv_metrics_log[cv_metrics_log['Trial']==besttrial]\n",
    "cv_metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f84e61-a3ee-4279-94cf-c034d7b29936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vediamo come ha performato nel miglior trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f8f52b-166f-400f-8faf-f0dcd5ad2baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PLOT CV\n",
    "cv_metrics_df_xgboost = cv_metrics_log.copy()\n",
    "\n",
    "# Stampa le metriche per modello\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_xgboost)\n",
    "\n",
    "# Calcola e stampa le metriche medie per modello\n",
    "mean_metrics = cv_metrics_df_xgboost.mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Crea il DataFrame con gli intervalli delle fold\n",
    "fold_intervals_df = cv_metrics_df_xgboost[['Fold', 'start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Ora creiamo il grafico combinato\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# === PLOT 1: Serie temporale originale ===\n",
    "plt.subplot(2, 1, 1)\n",
    "#for unique_id in train_df_xgb['unique_id'].unique():\n",
    "series_data = train_df_xgb.copy()\n",
    "plt.plot(series_data['ds'], series_data['y'], label='Serie unica')\n",
    "\n",
    "# Evidenzia le fold con colori\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)],\n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'{row[\"Fold\"]}', ha='center', fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# === PLOT 2: Metriche MAE e RMSE ===\n",
    "plt.subplot(2, 1, 2)\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# MAE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# RMSE\n",
    "for model in cv_metrics_df_xgboost['Model'].unique():\n",
    "    model_data = cv_metrics_df_xgboost[cv_metrics_df_xgboost['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Legenda combinata\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carico le cv metrics\n",
    "cv_metrics_df_xgboost = pd.read_pickle('pickles/cv_metrics_df_xgboost_f1.pkl')\n",
    "print(cv_metrics_df_xgboost)\n",
    "cv_metrics_df_xgboost.plot(x='fold', y=['MAE', 'RMSE'], marker='o', title='XGBoost CV Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caaf8cf2-4718-4c57-ae6c-6ed3fb7c6ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training coi migliori parametri trovati da Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4269bd1-96c9-4542-973c-c9cda123645a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo i best params\n",
    "# best_params = study.best_params\n",
    "# with open ('pickles/XGB_bestparams_f1.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbec84ad-5d67-4251-b8a7-1074f351144f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train con best params\n",
    "best_params.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 42})\n",
    "\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "import time\n",
    "start_time = time.time()\n",
    "#fit \n",
    "model = xgb.train(best_params, dtrain_xgb, num_boost_round=500)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f62d664-e3c3-4e0a-becd-2a6ac12f39db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "ax = xgb.plot_importance(model, importance_type='gain', max_num_features=10)\n",
    "for text in ax.texts:\n",
    "    text.set_visible(False)\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd1354d8-5819-4ac0-a20d-74a74ab90229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "forecast_xgb = pd.DataFrame(model.predict(dtest_xgb),columns=['forecast'])\n",
    "forecast_xgb['ds'] = test_df_xgb['ds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "249afb06-aeaa-45f7-8938-01dab0462086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RE-Scaling dei dati\n",
    "forecast_xgb['forecast'] = scaler_y.inverse_transform(forecast_xgb[['forecast']])\n",
    "train_df_xgb['y'] = scaler_y.inverse_transform(train_df_xgb[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c43f109-050a-4fa4-811d-28fbf2e99be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "fig = go.Figure()\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(x=train_df_xgb['ds'], y=train_df_xgb['y'], mode='lines', name='Training'))\n",
    "# Test data\n",
    "fig.add_trace(go.Scatter(x=test_df_xgb['ds'], y=test_df_xgb['y'], mode='lines', name='Test'))\n",
    "# Forecast\n",
    "fig.add_trace(go.Scatter(x=forecast_xgb['ds'], y=forecast_xgb['forecast'], mode='lines', \n",
    "                         name='Forecast', line=dict(color='deepskyblue')))\n",
    "# Forecast dirette\n",
    "fig.update_layout(title='Forecast vs Real Data', template='plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fdb54ad-bd4b-41d2-a89c-24b8fa067d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Calcolo metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d767eb-6f37-475e-84d6-48cbdd5b5a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df = pd.merge(test_df_xgb[['ds', 'y']], forecast_xgb, on='ds', how='left')\n",
    "metriche_xgb = calcola_metriche(merged_df['y'],merged_df['forecast'],\n",
    "                                train_df_xgb['y'], modelname=\"XGBoost\")\n",
    "metriche_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1605439d-caa3-4bde-9a4b-a924011d36bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747376ef-7230-4633-92ec-92a88078a228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# divisione in train e test\n",
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)\n",
    "df_lstm = TimeSeriesDataFrame(\n",
    "    df_lstm,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_lstm = df_lstm.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_lstm.reset_index(inplace=True)\n",
    "df_lstm.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "\n",
    "train_df_lstm = df_lstm.iloc[:-h]\n",
    "test_df_lstm = df_lstm.iloc[-h:]\n",
    "print(train_df_lstm.columns)\n",
    "train_df_lstm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_lstm = pd.DataFrame()\n",
    "def objective(trial):\n",
    "    # Hyperparametri da ottimizzare\n",
    "    encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 1, 2,4)\n",
    "    encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [32, 256])\n",
    "    decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [32, 256])\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 2,4)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [int(h), int(h*2), int(h/2)])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64,128])\n",
    "\n",
    "    # Definizione modello LSTM\n",
    "    lstm = LSTM(\n",
    "        h=len(test_df_lstm), \n",
    "        input_size=input_size,\n",
    "        loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "        scaler_type='robust',\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        max_steps=50,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        recurrent=False\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[lstm], freq='160ms')\n",
    "\n",
    "    # cross-validation per tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_lstm,\n",
    "        step_size=len(test_df_lstm),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calcola MAE per ogni fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'LSTM']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'LSTM',\n",
    "            'MAE_lstm': mae,\n",
    "            'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "    # Media MAE su tutte le fold\n",
    "    cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_lstm['MAE_lstm'].mean()\n",
    "\n",
    "    return mean_mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b355bb00-a569-4076-ac53-afaddcf7b3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270cb17d-5895-4105-b4fb-4f6a90b9614f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d5ca73-d522-4429-8ae5-20acd861853c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo i best params trovati da Optuna\n",
    "with open('pickles/LSTM_bestPar_F1_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2530ccfa-6b58-4f68-9f88-abaa30032da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico i best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31699aa8-16c1-480e-bb62-dc175b77e88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico i best pars\n",
    "with open('pickles/LSTM_bestPar_F1_multistep.pkl', 'rb') as file:\n",
    "    best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3fc79b-5239-468c-abb5-ad2cd43e8793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "h = len(test_df_lstm)\n",
    "best_lstm = LSTM(\n",
    "    h=h,\n",
    "    input_size=h*2,\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=2,\n",
    "    encoder_hidden_size=64,\n",
    "    decoder_hidden_size=32,\n",
    "    decoder_layers=1,\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=50,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.01,\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='160ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a873733c-1787-45b7-9a6e-b550b35c09d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "# df_cv = nf.cross_validation(\n",
    "#     df=train_df_lstm[10000:], #la CPU non regge tutte le unità\n",
    "#     step_size=len(test_df_lstm),\n",
    "#     n_windows=5\n",
    "# )\n",
    "# with open('pickles/LSTM_CV_F1_multistep.pkl', 'wb') as file:\n",
    "#      pickle.dump(df_cv, file)\n",
    "\n",
    "with open('pickles/LSTM_CV_F1_multistep.pkl', 'rb') as file:\n",
    "    df_cv = pickle.load(file)\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'LSTM']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['LSTM'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE_lstm': mae,\n",
    "        'RMSE_lstm': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_lstm = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_lstm)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_lstm.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294a9751-37f0-41df-abeb-8d9dc0715c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "# Prima dobbiamo identificare gli intervalli temporali per ogni fold\n",
    "folds = df_cv['cutoff'].unique()\n",
    "fold_intervals = []\n",
    "for i, cutoff in enumerate(folds):\n",
    "    # Ottieni le date della fold corrente\n",
    "    fold_data = df_cv[df_cv['cutoff'] == cutoff]\n",
    "    start_date = fold_data['ds'].min()\n",
    "    end_date = fold_data['ds'].max()\n",
    "    fold_intervals.append({\n",
    "        'Fold': i+1,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    })\n",
    "\n",
    "fold_intervals_df = pd.DataFrame(fold_intervals)\n",
    "\n",
    "# Ora creiamo un grafico combinato: serie temporale + metriche di errore\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "for unique_id in train_df_lstm['unique_id'].unique():\n",
    "    series_data = train_df_lstm[train_df_lstm['unique_id'] == unique_id]\n",
    "    plt.plot(series_data['ds'], series_data['y'], label=f'Serie {unique_id}')\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot inferiore: metriche MAE e RMSE per ogni fold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Crea un grafico con due assi y\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "cv_metrics_df_lstm['Model'] = 'lstm'\n",
    "# Plot MAE sul primo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax1.plot(model_data['Fold'], model_data['MAE_lstm'], 'o-', label=f'MAE - {model}')\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('MAE', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot RMSE sul secondo asse\n",
    "for model in cv_metrics_df_lstm['Model'].unique():\n",
    "    model_data = cv_metrics_df_lstm[cv_metrics_df_lstm['Model'] == model]\n",
    "    ax2.plot(model_data['Fold'], model_data['RMSE_lstm'], 's--', label=f'RMSE - {model}')\n",
    "\n",
    "ax2.set_ylabel('RMSE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combina le legende\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Metriche di Errore per Fold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9c3831-5ec6-44ef-95d7-22af2192294c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "best_lstm = LSTM(\n",
    "    h=h,\n",
    "    input_size=int(h*2),\n",
    "    loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "    scaler_type='robust',\n",
    "    encoder_n_layers=2,\n",
    "    encoder_hidden_size=64,\n",
    "    decoder_hidden_size=32,\n",
    "    decoder_layers=2,\n",
    "    futr_exog_list=futr_exog_list,\n",
    "    max_steps=200,\n",
    "    #early_stop_patience_steps=20,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.025,\n",
    "    recurrent=False\n",
    ")\n",
    "nf = NeuralForecast(models=[best_lstm], freq='160ms')\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(\n",
    "    df=train_df_lstm[12500:], #la cpu non regge tutto il dataset\n",
    "    val_size=len(test_df_lstm)\n",
    "    )\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63651b63-a66f-44d8-8e6a-9c4cd73ace28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cec6fe2-0a48-4c5c-842b-a141a53f19fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])\n",
    "#salvo in pickle\n",
    "with open('pickles/LSTM_F1_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9381865c-bde1-4d52-8917-1d7303d640e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni:\n",
    "with open('pickles/LSTM_f1_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d966536-1897-4765-bc08-02e18225d026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_lstm['ds'], y=train_df_lstm['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_lstm['ds'], y=test_df_lstm['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIA LSTM\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM'], mode='lines', \n",
    "    name='LSTM Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIANA LSTM\n",
    "# fig.add_trace(go.Scatter(x=Y_hat_df_lstm['ds'], y=Y_hat_df_lstm['LSTM-median'], mode='lines', \n",
    "#     name='MEDIAN Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"LSTM forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_lstm['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46da4a6f-b637-411c-a68a-be51313a3a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_lstm = pd.merge(test_df_lstm[['ds', 'y']], Y_hat_df_lstm, on='ds', how='left')\n",
    "#merged_df_lstm.dropna()\n",
    "metriche_lstm = calcola_metriche(merged_df_lstm['y'],merged_df_lstm['LSTM'],train_df_lstm['y'],modelname=\"LSTM\")\n",
    "metriche_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbcac56c-043d-4165-b417-ed2a9c01d49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0bcfaf-cf98-4836-8852-9b63099f985a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_np = create_time_series_features(df, target_col='y') \n",
    "df_np = df_np.dropna().reset_index(drop=True)\n",
    "df_np = TimeSeriesDataFrame(\n",
    "    df_np,\n",
    "    id_column='unique_id',\n",
    "    timestamp_column='ds'\n",
    ")\n",
    "df_np = df_np.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "df_np.reset_index(inplace=True)\n",
    "df_np.rename(columns={'timestamp': 'ds', 'item_id':'unique_id'}, inplace=True)\n",
    "#metto come booleana la variabile brake\n",
    "df_np['Brake'] = df_np['Brake'].astype(int)\n",
    "df_np.drop(columns=\"unique_id\",inplace=True)\n",
    "df_np = df_np[future_features]\n",
    "train_df_np = df_np.iloc[:-h]\n",
    "test_df_np = df_np.iloc[-h:]\n",
    "df_np.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef4ec03c-72b2-4d07-87ac-19fc193211ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_np.copy()\n",
    "test_df_original = test_df_np.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_np['ds']\n",
    "test_meta = test_df_np['ds']\n",
    "\n",
    "feature_cols = [col for col in train_df_np.columns if col not in ['ds']]\n",
    "train_idx = train_df_np.index\n",
    "test_idx = test_df_np.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_np[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_np[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_np = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_np = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa61f704-f4ac-4300-a6da-05f4f17826a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "553e29ba-1206-4ef9-9ea7-3048e41d8b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CV\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "initial_train_size = len(train_df_np) - 5 * len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    print(f\"\\n--- Fold {i+1}/{n_windows} ---\")\n",
    "\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    train_window = train_df_np.iloc[:train_end].copy()\n",
    "    test_window = train_df_np.iloc[test_start:test_end].copy()\n",
    "\n",
    "    print(f\"Train: {train_window.shape}, Test: {test_window.shape}\")\n",
    "\n",
    "    # Modello NeuralProphet\n",
    "    model = NeuralProphet(\n",
    "        quantiles=[0.025, 0.975],\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        loss_func='Huber',\n",
    "    )\n",
    "\n",
    "    # Aggiungi regressori\n",
    "    for reg in futr_exog_list:\n",
    "        model.add_future_regressor(reg)\n",
    "\n",
    "    # Fit\n",
    "    _ = model.fit(train_window, freq=\"160ms\", epochs=50)\n",
    "\n",
    "    # Predizione\n",
    "    future_df = test_window.copy()  # Deve contenere anche i regressori futuri\n",
    "    forecast = model.predict(future_df)\n",
    "\n",
    "    # Metriche\n",
    "    y_true = test_window['y'].values\n",
    "    y_pred = forecast['yhat1'].values\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    results.append({'Fold': i+1, 'MAE_nprophet': mae, 'RMSE_nprophet': rmse})\n",
    "    print(f\"Split {i+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Risultati CV\n",
    "results_df_np = pd.DataFrame(results)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "print(results_df_np)\n",
    "\n",
    "# Plot Errori\n",
    "results_df_np.plot(\n",
    "    x='Fold',\n",
    "    y=['MAE_nprophet', 'RMSE_nprophet'],\n",
    "    marker='o',\n",
    "    title='TimeSeriesCV Errors',\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d626cc8b-7144-4076-83aa-b183a0944e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvo results df CV\n",
    "with open('pickles/NP_CV_F1_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(results_df_np, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aeec17a-d8a2-43e0-9c42-3ed46c596246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carico i risultati della CV\n",
    "with open('pickles/NP_CV_F1_multistep.pkl', 'rb') as file:\n",
    "    results_df_np = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ae72b1-4815-48f9-9db9-b8a1100ddc28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mi ricavo le date\n",
    "initial_train_size = len(train_df_np)-5*len(test_df_np)\n",
    "h = len(test_df_np)\n",
    "n_windows = 5\n",
    "\n",
    "# Crea lista per i dati delle fold\n",
    "fold_data = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "\n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'ds' in train_df_np.columns:\n",
    "        start_date = train_df_np['ds'].iloc[test_start]\n",
    "        end_date = train_df_np['ds'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_np.index[test_start]\n",
    "        end_date = train_df_np.index[test_end - 1]\n",
    "\n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "\n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "\n",
    "# Converto le date in datetime se necessario\n",
    "if all(isinstance(date, str) for date in fold_intervals_df['start_date']):\n",
    "    fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "    fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "\n",
    "print(\"Fold Intervals DataFrame:\")\n",
    "print(fold_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "842f71f1-a837-414a-81e4-d821dba61f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_np['ds'], train_df_np['y'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "results_df_np.plot(x='Fold', y=['MAE_nprophet', 'RMSE_nprophet'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06554c77-9f67-4f24-bf64-73afc63306e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3bb4331-31a2-4820-b26d-95843a55ce98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.2, \n",
    "    batch_size=512,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"160ms\",epochs=200)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac19638e-ee0f-4e69-9f1d-51afe5d1d680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd93bc9-fbec-4327-aab5-06ff9ecc25a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_test = neuralprophet.predict(test_df_np[future_features])\n",
    "# #salvo le prediction sul test in locale\n",
    "with open('pickles/NP_f1_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae8bf642-b8b1-4d3a-bec6-603118be3be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI DEL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98166ed8-e9a8-4e6a-9411-3c481ade7bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#carico le previsioni del test\n",
    "with open('pickles/NP_f1_multistep.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7bc39ba-6ef1-498d-b935-6122e888c77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "forecast_test['yhat1'] = forecast_test['yhat1'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 2.5%'] = forecast_test['yhat1 2.5%'] * (max_val - min_val) + min_val\n",
    "forecast_test['yhat1 97.5%'] = forecast_test['yhat1 97.5%'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068c60d7-0a18-4604-9321-1ff1a79367dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Neural Prophet Forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e07e47c-8c17-4ddf-b9cc-59e0c38a6b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "# df con previsioni e valori reali\n",
    "forecast_df = pd.DataFrame({'ds': forecast_test['ds'], 'forecast': forecast_test['yhat1']})\n",
    "merged_df = pd.merge(test_df_original[['ds', 'y']], forecast_df, on='ds', how='left')\n",
    "# Calcola metriche\n",
    "mae_val = mean_absolute_error(merged_df['y'], merged_df['forecast'])\n",
    "metriche_np = calcola_metriche(merged_df['y'], merged_df['forecast'],train_df_original['y'],\n",
    "                               modelname=\"NeuralProphet\").round(10)\n",
    "metriche_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f3bb0c-ebe4-4cd8-88ee-6e94647c99a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6167158-589e-42c8-a985-cb29188a8d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_tft = df_tft.dropna().reset_index(drop=True) \n",
    "df_tft['item_id'] = df_tft['unique_id'] \n",
    "df_tft.drop(columns=\"unique_id\", inplace=True) \n",
    "df_tft.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_tft = TimeSeriesDataFrame(\n",
    "    df_tft,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_tft = df_tft.convert_frequency(\"160ms\").interpolate()\n",
    "df_tft.reset_index(inplace=True)\n",
    "df_tft.rename(columns={'timestamp': 'ds','item_id':'unique_id','target':'y'}, inplace=True)\n",
    "hist_exog_features = df_tft.drop(columns=['unique_id', 'ds', 'y'] + futr_exog_list).columns.tolist() #should work\n",
    "train_df_tft = df_tft.iloc[:-h]\n",
    "test_df_tft = df_tft.iloc[-h:]\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9f3866c8-8380-4a16-bc3f-1dd62c2f5fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "#Salvo per dopo\n",
    "train_df_original = train_df_tft.copy() \n",
    "test_df_original = test_df_tft.copy()\n",
    "\n",
    "# Salvo le colonne da riattaccare\n",
    "train_meta = train_df_tft[['ds', 'unique_id']]\n",
    "test_meta = test_df_tft[['ds', 'unique_id']]\n",
    "\n",
    "feature_cols = [col for col in train_df_tft.columns if col not in ['ds', 'unique_id']]\n",
    "train_idx = train_df_tft.index\n",
    "test_idx = test_df_tft.index\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_scaled = pd.DataFrame(scaler_x.fit_transform(train_df_tft[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_scaled = pd.DataFrame(scaler_x.transform(test_df_tft[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "\n",
    "#riconcateno con ds e unique_id\n",
    "train_df_tft = pd.concat([train_df_scaled, train_meta], axis=1)\n",
    "test_df_tft = pd.concat([test_df_scaled, test_meta], axis=1)\n",
    "\n",
    "#controllo le shape\n",
    "train_df_tft.shape, test_df_tft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a7dd444-f986-435b-b616-225414fc00ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TimeSeriesCV - fine tuning Optuna (LENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d8ad550e-782e-4d1a-aff6-fd947724bf10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optuna function\n",
    "folds = pd.DataFrame()\n",
    "cv_metrics = []\n",
    "cv_metrics_df_tft = pd.DataFrame()\n",
    "h = len(test_df_tft)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize for TFT\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [h, int(h/2)])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    n_rnn_layers = trial.suggest_int(\"n_rnn_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    grn_activation = trial.suggest_categorical(\"grn_activation\", [\"ReLU\", \"ELU\", \"Sigmoid\"])\n",
    "    \n",
    "    # Define TFT model\n",
    "    tft = TFT(\n",
    "        h=h,  # forecast horizon\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=grn_activation,\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=learning_rate,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=50, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=batch_size,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=64\n",
    ")\n",
    "\n",
    "    nf = NeuralForecast(models=[tft], freq='160ms')\n",
    "\n",
    "    # Cross-validation for tuning\n",
    "    df_cv = nf.cross_validation(\n",
    "        df=train_df_tft[20000:],\n",
    "        step_size=int(h/2),\n",
    "        n_windows=5\n",
    "    )\n",
    "\n",
    "    # Calculate MAE for each fold\n",
    "    folds = df_cv['cutoff'].unique()\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "        model_data = fold_data[['y', 'TFT']].dropna()\n",
    "\n",
    "        mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "\n",
    "        cv_metrics.append({\n",
    "            'Fold': i+1,\n",
    "            'Model': 'TFT',\n",
    "            'MAE_tft': mae,\n",
    "            'RMSE_tft': rmse\n",
    "        })\n",
    "\n",
    "    # Average MAE across all folds\n",
    "    cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "    mean_mae = cv_metrics_df_tft['MAE_tft'].mean()\n",
    "\n",
    "    return mean_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9eb9133f-75d5-4b01-9e3d-1869a2283ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10) #NON GIRA TROPPO PESANTE, OUT OF MEMORY ERROR DI CUDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70227a6f-fc18-4024-8258-ace6195eaed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "best_params=study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7606c1bf-bdf9-4fba-a1fc-a441a333ce5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Path nella repo Git\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_BestPars_F1_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8ef7efa-0c89-4b45-8157-a03f244689f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # #carico i best params\n",
    "# with open(\"pickles/TFT_BestPars_F1_multistep.pkl\", 'rb') as file:\n",
    "#     best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "957d0e79-aaa7-4e70-8be8-2d0b76c3a03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final CV\n",
    "\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(h/2), \n",
    "        hidden_size=32,\n",
    "        n_rnn_layers=1,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=\"ELU\",\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=0.05,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list = hist_exog_features,\n",
    "        max_steps=200, #aumentare a 100\n",
    "        val_check_steps=10,\n",
    "        batch_size=8,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=8\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"15min\")\n",
    "# final CV\n",
    "df_cv = nf.cross_validation(\n",
    "    df=train_df_tft[20000:],\n",
    "    step_size=len(test_df_tft),\n",
    "    n_windows=5\n",
    ")\n",
    "# Estrai i dati di cross-validation per calcolare le metriche\n",
    "cv_metrics = []\n",
    "\n",
    "# Identifica le fold basate sui timestamp unici\n",
    "folds = df_cv['cutoff'].unique()\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_data = df_cv[df_cv['cutoff'] == fold]\n",
    "    \n",
    "    # Filtra i dati per il modello corrente\n",
    "    model_data = fold_data[['y', 'TFT']]\n",
    "    model_data = model_data.dropna()\n",
    "        \n",
    "    # Calcola MAE e RMSE\n",
    "    mae = calcola_mae(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "    rmse = calcola_rmse(y_true=model_data['y'], y_pred=model_data['TFT'])\n",
    "        \n",
    "    # Aggiungi i risultati\n",
    "    cv_metrics.append({\n",
    "        'Fold': i+1,\n",
    "        'Model': 'TFT',\n",
    "        'MAE_TFT': mae,\n",
    "        'RMSE_TFT': rmse\n",
    "        })\n",
    "\n",
    "# Converti i risultati in DataFrame\n",
    "cv_metrics_df_tft = pd.DataFrame(cv_metrics)\n",
    "\n",
    "#Stampa le metriche nelle varie folds\n",
    "print(\"Metriche per modello:\")\n",
    "print(cv_metrics_df_tft)\n",
    "print(\"Metriche medie per modello:\")\n",
    "mean_metrics = cv_metrics_df_tft.groupby('Model').mean().reset_index()\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e40637bd-81eb-4989-a33e-63757c7ffe82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico la CV fatta su Colab\n",
    "with open('pickles/TFT_CV_F1_multistep.pkl', 'rb') as file:\n",
    "    cv_metrics_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3791942b-c789-4686-a578-37c26ea3059f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_metrics_df_tft.plot(x='Fold', y=['MAE_TFT', 'RMSE_TFT'], marker='o', title='Backtesting Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics_df_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc7a8321-3da1-49bc-9860-60bff6231e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4bc101-cebc-4503-9163-dc1a9df00d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final training\n",
    "import time\n",
    "# final model\n",
    "best_tft = TFT(\n",
    "        h=h,  \n",
    "        input_size=int(h*2),\n",
    "        hidden_size=16,\n",
    "        n_rnn_layers=2,\n",
    "        rnn_type='lstm',\n",
    "        grn_activation=\"ELU\",\n",
    "        loss=DistributionLoss(distribution=\"StudentT\", level=[90]),\n",
    "        learning_rate=0.005,\n",
    "        futr_exog_list=futr_exog_list,\n",
    "        hist_exog_list=hist_exog_features,\n",
    "        max_steps=500, #aumentare a 100\n",
    "        val_check_steps=25,\n",
    "        batch_size=8,\n",
    "        early_stop_patience_steps=-1,\n",
    "        scaler_type=\"standard\",\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"auto\",\n",
    "        one_rnn_initial_state=False,\n",
    "        windows_batch_size=8,\n",
    "\n",
    ")\n",
    "nf = NeuralForecast(models=[best_tft],freq=\"160ms\")\n",
    "# Pulire la memoria GPU prima di iniziare IL FIT\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "start_time = time.time()\n",
    "\n",
    "#fit \n",
    "nf.fit(df=train_df_tft[15000:], # nel training scartato 15k unità per memoria GPU\n",
    "       val_size=len(test_df_tft)) \n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4767cee3-6ac3-48e4-a352-2a3a19716ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft[['unique_id','ds'] + futr_exog_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752c2f34-541a-4cf1-9a48-5265062f0bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #salvo le pred in pickle\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_pred_F1_multistep.pkl\"\n",
    "# with open(save_path, 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c518db4-5081-4b41-964f-14fa1fefecca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('y')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "Y_hat_df_tft['TFT'] = Y_hat_df_tft['TFT'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-median'] = Y_hat_df_tft['TFT-median'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-lo-90'] = Y_hat_df_tft['TFT-lo-90'] * (max_val - min_val) + min_val\n",
    "Y_hat_df_tft['TFT-hi-90'] = Y_hat_df_tft['TFT-hi-90'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b885140d-0fab-4bfc-aecc-7838ac7e136a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le pred\n",
    "with open(\"pickles/TFT_pred_F1_multistep.pkl\", 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd45678-fb06-433f-ab8e-a839abd3f45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "fig.add_trace(go.Scatter(x=train_df_tft['ds'], y=train_df_original['y'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "fig.add_trace(go.Scatter(x=test_df_tft['ds'], y=test_df_original['y'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI MEDIANA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT-median'], mode='lines', \n",
    "    name='MEDIAN Forecast', line=dict(color='green')))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='lightgreen')))\n",
    "\n",
    "# QUANTILI TFT\n",
    "# Fascia tra P10 e P90\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(Y_hat_df_tft['ds']) + \n",
    "    list(Y_hat_df_tft['ds'][::-1]),\n",
    "    y=list(Y_hat_df_tft['TFT-hi-90']) + list(Y_hat_df_tft['TFT-lo-90'][::-1]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(144, 238, 144, 0.3)',  # lightgreen sfumato\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=True,\n",
    "    name='P10-P90'\n",
    "))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"TFT forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_tft['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff985326-3ceb-45f6-9035-d8e32a27d7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche\n",
    "merged_df_tft = pd.merge(test_df_original[['ds', 'y']], Y_hat_df_tft, on='ds', how='left')\n",
    "merged_df_tft.rename(columns={'TFT-lo-90': '0.1','TFT-median':'0.5', 'TFT-hi-90':'0.9'},inplace=True)\n",
    "#merged_df_tft.dropna(inplace=True)\n",
    "metriche_tft = calcola_metriche(merged_df_tft['y'],merged_df_tft['TFT'],train_df_original['y'],\n",
    "                                y_pred_quantiles=merged_df_tft[['0.1','0.5','0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"TFT\").round(10)\n",
    "metriche_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d2169bc-13ef-45da-9972-16097e05b9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375ef24b-d246-4eb0-8358-ae85890ff7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft = nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fb27b4-acdc-445d-b668-cece3d453ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # NON MI FA SALVARE IL MODELLO PERCHE' TROPPO PESANTE\n",
    "# save_path = \"/Workspace/Users/p.pierdomenico@iconsulting.biz/timeseries_ai/TFT_model_F1_multistep.pkl\"\n",
    "# with open(save_path, \"wb\") as f:\n",
    "#     pickle.dump(best_tft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6502327-6656-43d7-9249-9299db2ffb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # carico il modello tft\n",
    "# with open('pickles/TFT_model_F1_multistep.pkl', 'rb') as file:\n",
    "#     best_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2adbeedf-b223-42fd-8407-057b4c4ba9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = best_tft.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde6446f-0adc-4a34-9704-1585d83b36eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(best_tft, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91047a48-1cd7-4a23-bcb5-7d4d5e78a010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(best_tft, plot=44)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7a5e19b-6005-4182-8755-5d5ba9811ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(best_tft, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59600fea-494e-40e7-a72b-1c971b0682fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = best_tft.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1955d00-4492-4c68-a5a0-00cd45b22dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f7e3c06-2964-4e8b-823e-57619d91f583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38cf6d4d-426f-45b6-acb5-35e4359056c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0007c72-5581-4d05-bd93-1caa2c6fd5f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7dc943b-f922-40d2-8db1-bc8507321473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79cac409-c026-4ace-92f2-85cc86a944a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d0dd4b-6786-4289-9fb5-24ed22429b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461ae514-c90a-4fff-b676-acadfad8b502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2789101c-8c11-44b1-967d-9202d0bfb6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7343e7c-afc6-478e-8216-2f4f1c52f306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "mean_attention = (\n",
    "    best_tft\n",
    "    .attention_weights()[best_tft.input_size :, :]\n",
    "    .mean(axis=0)[: best_tft.input_size]\n",
    ")\n",
    "df_importances4 = df_importances4.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances4), 0), df_importances4[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances4[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances4), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4ff4d0-b87d-40ef-b68e-731b4db1ac94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_tft.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23995732-af00-40e5-bb05-4e62a8a4a57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d021c7-b6e0-4dd9-9160-4fef3f0cde36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y') \n",
    "\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos = df_chronos.dropna().reset_index(drop=True) \n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "df_chronos = df_chronos.convert_frequency(\"160ms\").interpolate() #qui perdo qualche osservazione\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id','target']]\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "# Split train e test\n",
    "train_df_chronos = df_chronos.iloc[:-h]\n",
    "test_df_chronos = df_chronos.iloc[-h:]\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87401110-c20f-402d-a1d4-b9a305636875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scaling dei dati\n",
    "train_df_original = train_df_chronos.copy() \n",
    "test_df_original = test_df_chronos.copy()\n",
    "\n",
    "train_idx = train_df_chronos.index\n",
    "test_idx = test_df_chronos.index\n",
    "\n",
    "feature_cols=[col for col in df_chronos.columns if col not in ['timestamp', 'item_id']] #mi serve dopo nello scaling\n",
    "\n",
    "#scaler\n",
    "scaler_x = MinMaxScaler()\n",
    "train_df_chronos = pd.DataFrame(scaler_x.fit_transform(train_df_chronos[feature_cols]),\n",
    "                             columns=feature_cols, index=train_idx)\n",
    "test_df_chronos = pd.DataFrame(scaler_x.transform(test_df_chronos[feature_cols]),\n",
    "                            columns=feature_cols, index=test_idx)\n",
    "#controllo le shape\n",
    "train_df_chronos.shape, test_df_chronos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8dd2eb-2959-48a9-b844-1a8806d311f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TimeSeriesCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8e661ce-aed4-4291-8274-da221ca58a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TimeSeriesCV\n",
    "h = len(test_df_chronos)  # prediction length\n",
    "n_splits = 5\n",
    "initial_train_size = len(train_df_chronos) - 5*h\n",
    "results = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    train_end = initial_train_size + fold * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    train_df_fold = train_df_chronos.iloc[:train_end]\n",
    "    test_df_fold = train_df_chronos.iloc[test_start:test_end]\n",
    "    \n",
    "    train_df_fold.reset_index(inplace=True)\n",
    "    test_df_fold.reset_index(inplace=True)\n",
    "    \n",
    "    train_df_fold[\"timestamp\"] = pd.to_datetime(train_df_fold[\"timestamp\"])\n",
    "    test_df_fold[\"timestamp\"] = pd.to_datetime(test_df_fold[\"timestamp\"])\n",
    "    \n",
    "    # Future timestamps \n",
    "    future_index = pd.date_range(\n",
    "        start=train_df_fold[\"timestamp\"].max() + pd.Timedelta(\"160ms\"),\n",
    "        periods=h,\n",
    "        freq=\"160ms\")\n",
    "    \n",
    "    # Preparo test set con known_covariates\n",
    "    test_df_for_prediction = test_df_fold[test_df_fold[\"timestamp\"].isin(future_index)].copy()\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index(\"timestamp\")\n",
    "    test_df_for_prediction = test_df_for_prediction.reindex(future_index)  # forza continuità\n",
    "    test_df_for_prediction[\"item_id\"] = train_df_fold[\"item_id\"].iloc[0]  # supponiamo 1 sola serie\n",
    "    test_df_for_prediction = test_df_for_prediction.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    test_df_for_prediction = test_df_for_prediction.set_index([\"item_id\", \"timestamp\"])\n",
    "    \n",
    "    # Preparo il train con multindex\n",
    "    train_df_fold = train_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "    print(f\"Train: {train_df_fold.shape}, Test: {test_df_fold.shape}\")\n",
    "    \n",
    "    # inizializzo il predictor\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=h,\n",
    "        target=\"target\",\n",
    "        known_covariates_names=known_covariates_names,\n",
    "        eval_metric=\"MASE\",\n",
    "        freq=\"160ms\")\n",
    "    \n",
    "    # fit del modello\n",
    "    predictor.fit(train_df_fold,\n",
    "        hyperparameters={\n",
    "            \"Chronos\": [\n",
    "                {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "                {\n",
    "                    \"model_path\": \"bolt_small\",\n",
    "                    \"covariate_regressor\": \"CAT\",\n",
    "                    \"target_scaler\": \"standard\",\n",
    "                    \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        time_limit=600,\n",
    "        enable_ensemble=True,)\n",
    "    \n",
    "    predictions = predictor.predict(train_df_fold, known_covariates=test_df_for_prediction)\n",
    "\n",
    "    test_df_with_index = test_df_fold.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    # Debug\n",
    "    print(f\"Forma predictions: {predictions.shape}\")\n",
    "    print(f\"Colonne predictions: {predictions.columns}\")\n",
    "    print(f\"Numero di indici in predictions: {len(predictions.index)}\")\n",
    "    print(f\"Numero di indici in test_df_with_index: {len(test_df_with_index.index)}\")\n",
    "\n",
    "    # trovo solo gli indici comuni\n",
    "    common_indices = predictions.index.intersection(test_df_with_index.index)\n",
    "    print(f\"Indici comuni: {len(common_indices)}\")\n",
    "\n",
    "    # prendo solo gli indici comuni\n",
    "    y_true = test_df_with_index.loc[common_indices, \"target\"]\n",
    "    y_pred = predictions.loc[common_indices, 'mean'].to_numpy()\n",
    "\n",
    "    mae = calcola_mae(y_true, y_pred)\n",
    "    rmse = calcola_rmse(y_true, y_pred)\n",
    "    \n",
    "    results.append({'split': fold+1, 'MAE_chronos': mae, 'RMSE_chronos': rmse})\n",
    "    print(f\"Split {fold+1} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "# risultati finali\n",
    "results_df_chronos = pd.DataFrame(results)\n",
    "results_df_chronos.set_index('split',inplace=True)\n",
    "print(\"\\n=== Backtest Results ===\")\n",
    "results_df_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e77692-382b-4c79-aa81-ceeadf574db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estraggo le date\n",
    "fold_data = []\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "for i in range(n_splits):\n",
    "    train_end = initial_train_size + i * h\n",
    "    test_start = train_end\n",
    "    test_end = test_start + h\n",
    "    \n",
    "    # Ricava date da colonna 'ds' o dall'indice\n",
    "    if 'timestamp' in train_df_chronos.columns:\n",
    "        start_date = train_df_chronos['timestamp'].iloc[test_start]\n",
    "        end_date = train_df_chronos['timestamp'].iloc[test_end - 1]\n",
    "    else:\n",
    "        start_date = train_df_chronos.index[test_start]\n",
    "        end_date = train_df_chronos.index[test_end - 1]\n",
    "    \n",
    "    # Format\n",
    "    if hasattr(start_date, 'strftime'):\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        start_date_str = str(start_date).split('.')[0]\n",
    "        end_date_str = str(end_date).split('.')[0]\n",
    "    \n",
    "    fold_data.append({\n",
    "        'Fold': i + 1,\n",
    "        'start_date': start_date_str,\n",
    "        'end_date': end_date_str\n",
    "    })\n",
    "\n",
    "# Crea DataFrame finale\n",
    "fold_intervals_df = pd.DataFrame(fold_data)\n",
    "fold_intervals_df['start_date'] = pd.to_datetime(fold_intervals_df['start_date'])\n",
    "fold_intervals_df['end_date'] = pd.to_datetime(fold_intervals_df['end_date'])\n",
    "# Stampa il dataframe prima della conversione per verificare i valori\n",
    "print(\"Fold Intervals DataFrame\")\n",
    "fold_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d59461-6d38-4b99-b61b-677eb95a2d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PLOT CV\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot principale: serie temporale originale\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_df_chronos['timestamp'], train_df_chronos['target'])\n",
    "\n",
    "# Evidenzia le fold con colori di sfondo diversi\n",
    "colors = ['lightblue', 'lightgreen', 'lightgray', 'lightpink', 'red']\n",
    "for i, row in fold_intervals_df.iterrows():\n",
    "    plt.axvspan(row['start_date'], row['end_date'], alpha=0.3, color=colors[i % len(colors)], \n",
    "                label=f'Fold {row[\"Fold\"]}' if i == 0 else \"\")\n",
    "    # Aggiungi etichetta della fold al centro dell'intervallo\n",
    "    mid_point = row['start_date'] + (row['end_date'] - row['start_date']) / 2\n",
    "    y_pos = plt.ylim()[1] * 0.9\n",
    "    plt.text(mid_point, y_pos, f'Fold {row[\"Fold\"]}', ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title('Serie Temporale con Folds della Cross Validation')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot degli errori\n",
    "results_df_chronos.reset_index(inplace=True)\n",
    "ax = results_df_chronos.plot(x='split', y=['MAE_chronos','RMSE_chronos'], marker='o', title='TimeSeriesCV Errors')\n",
    "ax.set_xticks(range(1, 6)) # Imposta le tacche sull'asse x da 1 a 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b7bfa1-78f8-40f2-a0eb-7bed6977c3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's training on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparo dataset per training\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=len(test_df_chronos), \n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"160ms\"\n",
    ")\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38aa494-cf26-4c37-8831-ba03a523950d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"ag_args\": {\"name_suffix\": \"ZeroShot\"}\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\",\n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True,\n",
    ")\n",
    "# Valutazione del modello in fase di training\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097a1b89-138c-4a5c-874d-33a0620ebd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PREVISIONI MULTI STEP \n",
    "predictions = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,  # Solo le covariate realmente note in anticipo\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "\n",
    "predictions_0shot = predictor.predict(\n",
    "    train_df_chronos,\n",
    "    known_covariates=test_known_covariates,\n",
    "    model=\"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1ae395-f1b4-45aa-82b5-e41936fd2cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rescaling dei dati\n",
    "target_col_position = feature_cols.index('target')\n",
    "min_val = scaler_x.data_min_[target_col_position]  # min value della feature target\n",
    "max_val = scaler_x.data_max_[target_col_position]  # max value della feature target\n",
    "# Trasformazione inversa manuale\n",
    "predictions['mean'] = predictions['mean'] * (max_val - min_val) + min_val\n",
    "predictions['0.1'] = predictions['0.1'] * (max_val - min_val) + min_val\n",
    "predictions['0.5'] = predictions['0.5'] * (max_val - min_val) + min_val\n",
    "predictions['0.9'] = predictions['0.9'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['mean'] = predictions_0shot['mean'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.5'] = predictions_0shot['0.5'] * (max_val - min_val) + min_val\n",
    "predictions_0shot['0.9'] = predictions_0shot['0.9'] * (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bd5b24-315e-458c-a3f2-f9e79fbfae6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot forecast\n",
    "# Creare la figura\n",
    "fig = go.Figure()\n",
    "# Dati di training\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=train_df_chronos['timestamp'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "# Dati di test\n",
    "\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='With Regressors', line=dict(color='green')))\n",
    "# PREVISIONI CHRONOS 0-shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='0-shot Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# Layout del grafico\n",
    "fig.update_layout(title=\"CHRONOS forecast\", \n",
    "    xaxis_title=\"Timestamp [t]\", yaxis_title=\"Value\", \n",
    "    template='plotly_dark', legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_chronos['timestamp'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(x=ultimo_test, y=0.9, yref=\"paper\", text=\"Fine Test Set\", \n",
    "                  showarrow=True, arrowhead=2, arrowcolor=\"white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c010736-658d-490f-8c7f-76c4879be475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione\n",
    "test_df_original.reset_index(inplace=True)\n",
    "merged_df_chronos = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']], \n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos = calcola_metriche(merged_df_chronos['target'],merged_df_chronos['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos\").round(5)\n",
    "metriche_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0140c7a-af92-4ca4-acf6-b1319b0aff27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# metriche di valutazione 0 shot\n",
    "merged_df_chronos_0shot = pd.merge(\n",
    "        test_df_original[['timestamp', 'target', 'item_id']], \n",
    "        predictions_0shot[['timestamp', 'mean', 'item_id','0.1','0.5','0.9']],\n",
    "        on=['timestamp', 'item_id']\n",
    ")\n",
    "metriche_chronos_0shot = calcola_metriche(merged_df_chronos_0shot['target'],merged_df_chronos_0shot['mean'],\n",
    "                                train_df_original['target'], y_pred_quantiles=merged_df_chronos_0shot[['0.1', '0.5', '0.9']],\n",
    "                                quantiles=[0.1, 0.5, 0.9], modelname=\"Chronos 0 shot\").round(5)\n",
    "metriche_chronos_0shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f561b04-d920-4f3c-92ae-c1f705e8431c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### final evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b49816c3-cd7e-466e-8e22-bba41472fcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = metriche_xgb.join(metriche_lstm).join(metriche_np).join(metriche_chronos).join(metriche_chronos_0shot).join(metriche_tft)\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c20074-1b13-4618-864b-a6641911af6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FINAL PREDICTIONS PLOT FOR SOLAR ENERGY\n",
    "fig = go.Figure()\n",
    "\n",
    "# Dati di training e test\n",
    "fig.add_trace(go.Scatter(x=train_df_np['ds'], y=train_df_original['target'], mode='lines', name='Training Data'))\n",
    "fig.add_trace(go.Scatter(x=test_df_np['ds'], y=test_df_original['target'], mode='lines', name='Testing Data'))\n",
    "\n",
    "# PREVISIONI XGBOOST\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_xgb['ds'], \n",
    "    y=forecast_xgb['forecast'], \n",
    "    mode='lines', \n",
    "    name='XGBoost', \n",
    "    line=dict(color='purple')\n",
    "))\n",
    "\n",
    "# PREVISIONI LSTM\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_hat_df_lstm['ds'], \n",
    "    y=Y_hat_df_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='pink')\n",
    "))\n",
    "\n",
    "# PREVISIONI NEURAL PROPHET\n",
    "# In-sample forecast (test period)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_test['ds'], \n",
    "    y=forecast_test['yhat1'], \n",
    "    mode='lines', \n",
    "    name='NeuralProphet', \n",
    "    line=dict(color='aquamarine')\n",
    "))\n",
    "\n",
    "# PREVISIONI MEDIA TFT\n",
    "fig.add_trace(go.Scatter(x=Y_hat_df_tft['ds'], y=Y_hat_df_tft['TFT'], mode='lines', \n",
    "    name='TFT Forecast', line=dict(color='yellow')))\n",
    "\n",
    "# PREVISIONI CHRONOS\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions['mean'], mode='lines', \n",
    "    name='CHRONOS', line=dict(color='green')))\n",
    "    \n",
    " # PREVISIONI CHRONOS 0 shot\n",
    "fig.add_trace(go.Scatter(x=test_df_chronos['timestamp'], y=predictions_0shot['mean'], mode='lines', \n",
    "    name='CHRONOS 0-SHOT', line=dict(color='lightgreen')))\n",
    "   \n",
    "# Layout del grafico\n",
    "fig.update_layout(\n",
    "    title=\"Forecasts for White Noise\", \n",
    "    xaxis_title=\"Timestamp [t]\", \n",
    "    yaxis_title=\"Value\", \n",
    "    template='plotly_dark', \n",
    "    legend=dict(font=dict(size=15))\n",
    ")\n",
    "\n",
    "# Linea verticale per separare i dati di test dai dati futuri\n",
    "ultimo_test = test_df_np['ds'].max()\n",
    "fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "fig.add_annotation(\n",
    "    x=ultimo_test, \n",
    "    y=0.9, \n",
    "    yref=\"paper\", \n",
    "    text=\"Fine Test Set\", \n",
    "    showarrow=True, \n",
    "    arrowhead=2, \n",
    "    arrowcolor=\"white\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sistemo i vari dataset prima del merge\n",
    "cv_metrics_df_lstm.drop(columns=('Model'),inplace=True)\n",
    "results_df_np.rename(columns={'split': 'Fold'}, inplace=True)\n",
    "results_df_chronos.rename(columns={'split': 'Fold','MAE':'MAE_chronos','RMSE':'RMSE_chronos'}, inplace=True)\n",
    "cv_metrics_df_xgboost.rename(columns={'MAE': 'MAE_XGB','RMSE':'RMSE_XGB','fold':'Fold'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final plot metrics TimeSeriesCV\n",
    "\n",
    "# faccio il merge di tutte le tabelle dei vari CV\n",
    "final_cv = cv_metrics_df_lstm.merge(results_df_chronos, \n",
    "                            on=\"Fold\",how=\"inner\").merge(cv_metrics_df_xgboost[['MAE_XGB','RMSE_XGB','Fold']],\n",
    "                            on=\"Fold\",how=\"inner\").merge(results_df_np,\n",
    "                            on=\"Fold\", how=\"inner\").merge(cv_metrics_df_tft,\n",
    "                            on=\"Fold\",how=\"inner\")\n",
    "# Rescaling CV metrics data\n",
    "columns_to_modify = ['MAE_nprophet', 'RMSE_nprophet', 'MAE_TFT', 'RMSE_TFT', #'MAE_lstm', 'RMSE_lstm',\n",
    "                        'MAE_chronos', 'RMSE_chronos', 'MAE_XGB', 'RMSE_XGB']\n",
    "for col in columns_to_modify:\n",
    "    final_cv[f'{col}'] = final_cv[f'{col}'] * (max_val - min_val) + min_val\n",
    "\n",
    "model_colors = {\n",
    "    'XGB': '#9467bd',    # Purple\n",
    "    'lstm': '#e377c2',       # Pink\n",
    "    'nprophet': '#7fffd4',         # Aquamarine\n",
    "    'TFT': '#ffff00',        # Yellow\n",
    "    'chronos': '#2ca02c',    # Green\n",
    "}\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model - MAE (darker) and RMSE (lighter)\n",
    "for model, color in model_colors.items():\n",
    "    # Add MAE line (darker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'MAE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} MAE',\n",
    "        line=dict(color=color, width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Add RMSE line (lighter with same color but different dash pattern)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=final_cv['Fold'],\n",
    "        y=final_cv[f'RMSE_{model}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{model} RMSE',\n",
    "        line=dict(color=color, width=2, dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison (MAE and RMSE)',\n",
    "    template='plotly_dark',\n",
    "    xaxis=dict(title='Fold',\n",
    "        tickmode='linear',\n",
    "        tick0=1, dtick=1\n",
    "    ),    yaxis=dict(\n",
    "        title='Error Value'\n",
    "    ),    legend=dict(\n",
    "        orientation=\"v\"\n",
    "    ),    hovermode=\"closest\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4650c0b-fa50-4000-888c-9525e96bf29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confronto TFT/CHRONOS\n",
    "tft_chronos = metriche_chronos.join(metriche_tft)\n",
    "tft_chronos                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d0c9e2-5afd-48a8-824d-3532c19397e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **FAVORITA SALES**, panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4751c760-ba6e-4ba7-8396-f283b2b29457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = favorita_train.reset_index().rename(columns={'DATE_TIME': 'ds','sales':'y','Store':'unique_id'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.drop(columns=['index'],inplace=True)\n",
    "df = df[df['unique_id'] != 52] #dava problemi nei vari fit (tutti valori costanti, no variabilità)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0287d90-6b29-4d86-bcd3-f87ebc93affa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione create features per store\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ordina per sicurezza\n",
    "    df = df.sort_values(['unique_id', 'ds'])\n",
    "\n",
    "    # Applicazione delle feature per ogni store\n",
    "    def fe_group(group):\n",
    "        # Lag features (1-7)\n",
    "        for lag in range(1, 8):\n",
    "            group[f'lag_{lag}'] = group[target_col].shift(lag)\n",
    "\n",
    "        # Differencing features\n",
    "        group['diff_1'] = (group[target_col] - group[target_col].shift(1)).shift(1)\n",
    "        group['diff_7'] = (group[target_col] - group[target_col].shift(7)).shift(1)\n",
    "\n",
    "        # Rolling means & std\n",
    "        group['rolling_mean_3'] = group[target_col].rolling(window=3).mean().shift(1)\n",
    "        group['rolling_mean_7'] = group[target_col].rolling(window=7).mean().shift(1)\n",
    "        group['rolling_std_3'] = group[target_col].rolling(window=3).std().shift(1)\n",
    "        group['rolling_std_7'] = group[target_col].rolling(window=7).std().shift(1)\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Applica la funzione per ogni store\n",
    "    df = df.groupby('unique_id', group_keys=False).apply(fe_group)\n",
    "\n",
    "    # Feature temporali (non dipendono da target)\n",
    "    df['dayofweek'] = df['ds'].dt.dayofweek\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "\n",
    "    # Feature cicliche\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b06b33-6949-453e-84c7-cccf3b27e180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68140f4a-2e50-4725-9d6c-75723cbcb77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bec40b9-b47f-40cf-a110-11acb0321879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_lstm, test_df_lstm = split_train_test_by_id(df_lstm, test_frac=0.1)\n",
    "h = len(test_df_lstm) // 53\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129a1a90-89af-4d51-8897-62ebcf96e373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d70daca2-a1db-4b4f-892b-b843c7bc2107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating static df\n",
    "# Step 1: One-hot encoding normale\n",
    "ohe_df = pd.get_dummies(train_df_lstm['unique_id'].drop_duplicates(), prefix='store')\n",
    "\n",
    "# Step 2: Inverti i valori (0 -> 1 e 1 -> 0)\n",
    "ohe_df = 1 - ohe_df\n",
    "\n",
    "# Step 3: Aggiungi la colonna unique_id\n",
    "ohe_df['unique_id'] = train_df_lstm['unique_id'].drop_duplicates().values\n",
    "\n",
    "# Step 4: Reordina colonne\n",
    "static_df = ohe_df[['unique_id'] + [col for col in ohe_df.columns if col != 'unique_id']]\n",
    "\n",
    "# Cast a int\n",
    "static_df = static_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636478d0-bad5-4432-acdb-60024e472245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TimeSeriesCV\n",
    "h = len(test_df_lstm) // train_df_lstm['unique_id'].nunique()  # horizon\n",
    "# === MODELLI ===\n",
    "models = [LSTM(h=h, \n",
    "               input_size=2*h,\n",
    "               loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "               scaler_type='robust',\n",
    "               encoder_n_layers=1,\n",
    "               encoder_hidden_size=64,\n",
    "               decoder_hidden_size=64,\n",
    "               decoder_layers=1,\n",
    "               max_steps=50, # su databricks va aumentato\n",
    "               futr_exog_list=train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "               batch_size=32,\n",
    "               learning_rate=0.01,\n",
    "               recurrent=False,\n",
    "               start_padding_enabled=True\n",
    "               )\n",
    "          ]\n",
    "\n",
    "# === INIT ===\n",
    "nf = NeuralForecast(models=models, freq='B')\n",
    "# === CROSS-VALIDATION ===\n",
    "cv_df = nf.cross_validation(df=train_df_lstm, \n",
    "                            static_df=static_df,\n",
    "                            val_size=h, \n",
    "                            n_windows=5)\n",
    "for uid in df['unique_id'].unique():\n",
    "    y_true = cv_df[cv_df['unique_id'] == uid]['y']\n",
    "    y_pred = cv_df[cv_df['unique_id'] == uid]['LSTM']  \n",
    "    print(f'{uid} MAE:', calcola_mae(y_true, y_pred),\n",
    "          f'{uid} RMSE:', calcola_rmse(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a81fe5a-04da-4cd8-9d5d-b861e3f6d72e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FIT FINALE DEL MODELLO ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "models = [LSTM(h=h, \n",
    "               input_size=int(3*h),\n",
    "               loss=DistributionLoss(distribution=\"Normal\", level=[95]),\n",
    "               scaler_type='robust',\n",
    "               encoder_n_layers=2, \n",
    "               encoder_hidden_size=128,\n",
    "               decoder_hidden_size=64,\n",
    "               decoder_layers=2, \n",
    "               max_steps=100,\n",
    "               futr_exog_list=train_df_lstm.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "               batch_size=32, \n",
    "               learning_rate=0.005, \n",
    "               recurrent=False,\n",
    "               start_padding_enabled=True\n",
    "               )\n",
    "          ]\n",
    "nf = NeuralForecast(models=models, freq='B') \n",
    "print(\"=== STARTING THE MODEL FIT ===\")\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(train_df_lstm, val_size=4*h)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cbbb01-6ea8-46c5-a35a-9bc329088e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm)\n",
    "#salvo in pickle\n",
    "with open('pickles/LSTM_favorita.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbaac6e8-5f04-4d2a-b01c-389c67e67316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18efcb4f-f61e-42fe-8a45-03518f3b1d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni:\n",
    "with open('pickles/LSTM_favorita.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2155951e-0ef9-48b5-9717-efbf876af907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_lstm[train_df_lstm['unique_id'] == store]\n",
    "    test_store = test_df_lstm[test_df_lstm['unique_id'] == store]\n",
    "    forecast_store = Y_hat_df_lstm[Y_hat_df_lstm['unique_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['LSTM'], \n",
    "        mode='lines', \n",
    "        name='LSTM Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ef3315-e520-4548-a549-6cc91430e786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = Y_hat_df_lstm[['ds', 'LSTM', 'unique_id']]\n",
    "merged_df = pd.merge(test_df_lstm[['ds', 'y', 'unique_id']], Y_hat_df_lstm, on=['ds', 'unique_id'], how='left')\n",
    "store_metrics_lstm = []\n",
    "for store in df_lstm['unique_id'].unique():  \n",
    "    store_df = merged_df[merged_df['unique_id'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['LSTM'],\n",
    "                               y_train=train_df_lstm['y'], modelname=\"LSTM\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_lstm.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab3a692-a7ab-4e9d-b3c3-98c4e6509804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_metrics_lstm[13] #posso selezionare qualunque store io voglia valutare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6481e069-44df-475a-b8f5-ef979aa643ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_lstm = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_lstm)):\n",
    "    mae_values.append(store_metrics_lstm[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_lstm[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_lstm[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_lstm[i].iloc[1, 0]) \n",
    "    mase_values_lstm.append(store_metrics_lstm[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_lstm)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_lstm)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_lstm = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_LSTM': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_LSTM':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5be34d-e331-499e-a2e5-d5af1d0e5245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_lstm)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_lstm) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_lstm) > 1) / len(mase_values_lstm) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_lstm) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_lstm) < 1) / len(mase_values_lstm) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a01c862-6f13-47e5-a078-d02d69d94fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d8d934-6da8-40c2-848a-2e60ef306677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepara il dataset\n",
    "df_np = create_time_series_features(df, target_col='y').dropna()\n",
    "df_np['ID'] = df_np['unique_id'].astype(str)\n",
    "df_np.drop(columns=['unique_id'], inplace=True)\n",
    "df_np['settimana_del_anno'] = df_np['settimana_del_anno'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8483ffe2-3a75-410e-bbd5-7485fcc4531d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"ID\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_np, test_df_np = split_train_test_by_id(df_np, test_frac=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c9975a-6e51-4a45-8863-d5a601df7588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f95a609e-8025-4a48-8c62-c75663b1086e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Selezione colonne\n",
    "cols_base = ['ds', 'y', 'ID']\n",
    "regressors = [\n",
    "        'onpromotion', 'promo', 'oil price', 'giorno_settimana',\n",
    "       'settimana_del_mese', 'settimana_del_anno', 'giorno_del_mese',\n",
    "       'mese_del_anno', 'rolling_mean_30', 'sales_lag_1', 'sales_lag_2',\n",
    "       'sales_lag_7', 'sales_lag_14', 'sales_lag_28', 'lag_1', 'lag_2',\n",
    "       'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'diff_1', 'diff_7',\n",
    "       'dayofweek', 'month', 'quarter', 'day_sin', 'day_cos', 'month_sin',\n",
    "       'month_cos', 'rolling_mean_3', 'rolling_mean_7', 'rolling_std_3',\n",
    "       'rolling_std_7'\n",
    "]\n",
    "\n",
    "train_df_np = train_df_np[cols_base + regressors]\n",
    "test_df_np = test_df_np[cols_base + regressors]\n",
    "\n",
    "train_df_np = train_df_np.astype({col: 'float' for col in train_df_np.columns if col not in ['ds', 'ID']})\n",
    "test_df_np = test_df_np.astype({col: 'float' for col in test_df_np.columns if col not in ['ds', 'ID']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "575da2a8-f855-4136-8699-4cff264dcfc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Definisco il modello\n",
    "model = NeuralProphet(\n",
    "    # n_forecasts=1,\n",
    "    # n_lags=0,\n",
    "    quantiles=[0.025, 0.975],\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    loss_func='Huber',\n",
    "    #seasonality_mode='multiplicative',\n",
    "    global_normalization=True,\n",
    "    unknown_data_normalization=True\n",
    ")\n",
    "\n",
    "# 5. Aggiungo i regressori\n",
    "for reg in regressors:\n",
    "    model.add_future_regressor(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097f2f18-35d5-405b-9f0f-e3048c0c84b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Fit del modello\n",
    "import time\n",
    "start_time = time.time()\n",
    "metrics_df = model.fit(\n",
    "    train_df_np,\n",
    "    freq=\"B\", \n",
    "    epochs = 100\n",
    ")\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c3632c-fe63-4e0c-855a-2f1f6619ba06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1e4eccd-8bbc-44b8-be13-1907f5079b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Previsione\n",
    "forecast_test = model.predict(test_df_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50f0ba4-25f8-4a1a-b7e4-730e3a68140d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo le prediction sul test in locale\n",
    "with open('pickles/NP_favorita.pkl', 'wb') as file:\n",
    "    pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e3a807-9628-4172-ad7e-ef4ffc0bc719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c747e2b-f22a-45c8-87f9-0fe71166eac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni del test\n",
    "with open('pickles/NP_favorita.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff7c40da-6afa-4d78-ac09-e62b4e784290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = ['1','3','5','20','35','54'] # degli store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_np[train_df_np['ID'] == store]\n",
    "    test_store = test_df_np[test_df_np['ID'] == store]\n",
    "    forecast_store = forecast_test[forecast_test['ID'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['yhat1'], \n",
    "        mode='lines', \n",
    "        name='NeuralProphet Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b779ac3-90fa-48ea-97f9-223ed7e316ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = forecast_test[['ds', 'yhat1', 'ID']]\n",
    "merged_df = pd.merge(test_df_np[['ds', 'y', 'ID']], forecast_df, on=['ds', 'ID'], how='left')\n",
    "store_metrics_np = []\n",
    "for store in df_np['ID'].unique():  # primi 5 store\n",
    "    store_df = merged_df[merged_df['ID'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['yhat1'],\n",
    "                               y_train=train_df_np['y'], modelname=\"NeuralProphet\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_np.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefa4fa9-53c7-49b4-b03d-5b3bf4bc3988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_metrics_np[12] #posso selezionare qualunque store voglio vedere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c79fcb-6cbb-419c-bf23-9b4b8e8290f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_np = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_np)):\n",
    "    mae_values.append(store_metrics_np[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_np[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_np[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_np[i].iloc[1, 0]) \n",
    "    mase_values_np.append(store_metrics_np[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_np)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_np)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_np = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_NP': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_NP':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f752ec-a305-42dd-8509-332c5beac270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_np)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_np) > 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_np) >= 1) / len(mase_values_np) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_np) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_np) < 1) / len(mase_values_np) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d8e2841-996b-4ff2-a51b-893ed0c44fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4def26f-a79b-4e44-90c0-f51b6f45f73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_tft, test_df_tft = split_train_test_by_id(df_tft, test_frac=0.1)\n",
    "train_df_tft.shape, test_df_tft.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddce6c68-3bb5-4256-8f0c-977e564302c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating static df\n",
    "# Step 1: One-hot encoding normale\n",
    "ohe_df = pd.get_dummies(train_df_tft['unique_id'].drop_duplicates(), prefix='store')\n",
    "\n",
    "# Step 2: Inverti i valori (0 -> 1 e 1 -> 0)\n",
    "ohe_df = 1 - ohe_df\n",
    "\n",
    "# Step 3: Aggiungi la colonna unique_id\n",
    "ohe_df['unique_id'] = train_df_tft['unique_id'].drop_duplicates().values\n",
    "\n",
    "# Step 4: Reordina colonne\n",
    "static_df = ohe_df[['unique_id'] + [col for col in ohe_df.columns if col != 'unique_id']]\n",
    "\n",
    "# Cast a int\n",
    "static_df = static_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "281ae27e-1fbb-4b9d-bb0c-2f7190e00262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model fit\n",
    "h = len(test_df_tft) // train_df_tft['unique_id'].nunique()\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        TFT(\n",
    "            h=h,\n",
    "            input_size=31,\n",
    "            hidden_size=32, # deve essere divisibile per 4\n",
    "            grn_activation=\"ELU\",\n",
    "            rnn_type=\"lstm\",\n",
    "            n_rnn_layers=2, # prima 1\n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[95]),\n",
    "            learning_rate=0.005, # prima 0.01\n",
    "            futr_exog_list=train_df_tft.drop(columns=['unique_id', 'ds', 'y']).columns.tolist(),\n",
    "            max_steps=100,\n",
    "            val_check_steps=25,\n",
    "            early_stop_patience_steps=10,\n",
    "            scaler_type=\"standard\",\n",
    "            windows_batch_size=128,\n",
    "            enable_progress_bar=True,\n",
    "        ),\n",
    "    ],\n",
    "    freq=\"B\", \n",
    ")\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, static_df=static_df, val_size=h)\n",
    "end_time = time.time()\n",
    "tft_model = nf.models[0]\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60333e29-53ec-43b9-a6fa-d9b660f24f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)\n",
    "#salvo in pickle\n",
    "# with open('pickles/TFT_favorita.pkl', 'wb') as file:\n",
    "#     pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264a72c6-f237-4793-91c8-a6bc9049235d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico le previsioni TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b71b93a-c5a6-40f2-b9e3-e1991653a8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni:\n",
    "with open('pickles/TFT_favorita.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe90c569-28f5-41da-a09f-44e3d4d3eb60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_tft[train_df_tft['unique_id'] == store]\n",
    "    test_store = test_df_tft[test_df_tft['unique_id'] == store]\n",
    "    forecast_store = Y_hat_df_tft[Y_hat_df_tft['unique_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['TFT'], \n",
    "        mode='lines', \n",
    "        name='TFT Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b718a0d-3fbf-4b85-99df-ecfcef8b95f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = Y_hat_df_tft[['ds', 'TFT', 'unique_id']]\n",
    "merged_df = pd.merge(test_df_tft[['ds', 'y', 'unique_id']], Y_hat_df_tft, on=['ds', 'unique_id'], how='left')\n",
    "store_metrics_tft = []\n",
    "for store in df_tft['unique_id'].unique():  # 5 store\n",
    "    store_df = merged_df[merged_df['unique_id'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['TFT'],\n",
    "                               y_train=train_df_tft['y'], modelname=\"TFT\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_tft.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ddeedea-dc55-488e-8a29-24b1120bf82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_tft = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_tft)):\n",
    "    mae_values.append(store_metrics_tft[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_tft[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_tft[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_tft[i].iloc[1, 0]) \n",
    "    mase_values_tft.append(store_metrics_tft[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "mae_std_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_tft)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "mae_std_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_tft)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_tft = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_TFT': [mae_mean, mape_mean, smape_mean, mae_std_mean, mase_mean],\n",
    "    'Median_TFT':[mae_median, mape_median, smape_median, mae_std_median, mase_median]\n",
    "})\n",
    "mean_metrics_tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e280f916-1698-4746-bfa0-4cc7c6cce16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_tft)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_tft) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_tft) > 1) / len(mase_values_tft) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_tft) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_tft) < 1) / len(mase_values_tft) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5fff35e-7934-438a-bd20-8c9a564ea137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37054c50-1e3e-4499-b7a4-aa08ec9c2b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # salvo il modello tft\n",
    "# with open('pickles/TFT_model_favorita.pkl', 'wb') as file:\n",
    "#     pickle.dump(tft_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0453e0-a59a-4536-8792-c071bb9a9adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico il modello tft\n",
    "with open('pickles/TFT_model_favorita.pkl', 'rb') as file:\n",
    "    tft_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2eb6a07-17b5-45db-9213-44c6c3af26c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = tft_model.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7445cae9-b197-4e82-8474-3e00c3666ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(tft_model, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ad7ae4-4e5c-476c-98ac-bb88bb75155f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(tft_model, plot=4)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11fda53b-d29f-4696-8f0f-ce575d42e184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(tft_model, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dae6352-d181-4b60-8d3f-8ec65074858a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = tft_model.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7065c872-864d-4dd0-aef0-c8651626439b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b3cb14-b40d-4a1d-a2de-fe6ec6cb0bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3dd3a58-1fa2-4b08-992c-7034514a9b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fee1f47-33ab-480c-8697-699a7ac53a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eecdbfe-6f7f-4f25-a9c8-8639ed03937e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4413a675-91e5-4db2-827a-9e3febdb46a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e267fc54-fdbc-4be9-b725-579f8304a5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc18676-3983-4107-8a2e-f76c590cb13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd7a1acd-d3bf-4837-98c4-5441dd38370f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    tft_model\n",
    "    .attention_weights()[tft_model.input_size :, :]\n",
    "    .mean(axis=0)[: tft_model.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628f93f4-0c53-4d6e-a4e3-72ba9a5a12b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "\n",
    "# Estrai l'importanza delle future variables\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "\n",
    "# Estrai tutte le attention weights mediate nel tempo\n",
    "full_attention = tft_model.attention_weights()[tft_model.input_size:, :].mean(axis=0)\n",
    "\n",
    "# Estrai solo la parte relativa ai 12 time steps futuri\n",
    "n_future_steps = df_importances4.shape[0]\n",
    "mean_attention_future = full_attention[-n_future_steps:]\n",
    "\n",
    "# Applica la ponderazione riga per riga (sul tempo)\n",
    "df_importances4 = df_importances4.multiply(mean_attention_future, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    ax.bar(\n",
    "        np.arange(1, len(df_importances4) + 1), \n",
    "        df_importances4[col].values, \n",
    "        width=0.6, \n",
    "        label=col, \n",
    "        bottom=bottom\n",
    "    )\n",
    "    bottom += df_importances4[col].values\n",
    "\n",
    "# Titoli e assi\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Future time steps\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Linea media di attenzione\n",
    "ax.plot(\n",
    "    np.arange(1, len(df_importances4) + 1),\n",
    "    mean_attention_future,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf30bec-52c5-4033-8835-8f9a227081cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tft_model.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e18072-c2ad-486c-b614-5efd91d60deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d543a5b-55f9-4b73-875b-47658e088c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y').dropna()\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Definizione delle covariate note\n",
    "known_covariates_names = ['onpromotion', 'promo', 'oil price', 'giorno_settimana',\n",
    "       'settimana_del_mese', 'settimana_del_anno', 'giorno_del_mese',\n",
    "       'mese_del_anno', 'rolling_mean_30', 'sales_lag_1', 'sales_lag_2',\n",
    "       'sales_lag_7', 'sales_lag_14', 'sales_lag_28', 'lag_1', 'lag_2',\n",
    "       'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'diff_1', 'diff_7',\n",
    "       'dayofweek', 'month', 'quarter', 'day_sin', 'day_cos', 'month_sin',\n",
    "       'month_cos', 'rolling_mean_3', 'rolling_mean_7', 'rolling_std_3',\n",
    "       'rolling_std_7']\n",
    "\n",
    "# Split train e test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"item_id\"):\n",
    "        group = group.sort_values(\"timestamp\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_chronos, test_df_chronos = split_train_test_by_id(df_chronos, test_frac=0.1)\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0fd6c1-3560-4e59-855b-8202ad67d29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=h,\n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    freq=\"B\" \n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"WithRegressor\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True, \n",
    ")\n",
    "\n",
    "# Valutazione del modello\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47389306-1200-405d-8775-eb8330c4e7a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Previsioni\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "train_df_chronos.set_index([\"item_id\",\"timestamp\"],inplace=True)\n",
    "test_df_chronos.set_index([\"item_id\",\"timestamp\"],inplace=True)\n",
    "\n",
    "predictions = predictor.predict(\n",
    "    data=train_df_chronos, \n",
    "    known_covariates=test_df_chronos\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "# Previsioni\n",
    "predictions_0shot = predictor.predict(\n",
    "    data=train_df_chronos, \n",
    "    known_covariates=test_df_chronos,\n",
    "    model = \"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4184f097-7663-4eac-963e-7fd1e6ad61d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "for store in store_ids:\n",
    "    train_store = train_df_chronos[train_df_chronos['item_id'] == store]\n",
    "    test_store = test_df_chronos[test_df_chronos['item_id'] == store]\n",
    "    forecast_store = predictions[predictions['item_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['timestamp'], y=train_store['target'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['timestamp'], y=test_store['target'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['timestamp'], \n",
    "        y=forecast_store['mean'], \n",
    "        mode='lines', \n",
    "        name='CHRONOS Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['timestamp'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1884d491-7240-4092-af66-c7900864bfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "forecast_df = predictions[['timestamp', 'mean', 'item_id']]\n",
    "merged_df = pd.merge(test_df_chronos[['timestamp', 'target', 'item_id']], predictions, \n",
    "                     on=['timestamp', 'item_id'], how='right')\n",
    "store_metrics_chronos = []\n",
    "df_chronos.reset_index(inplace=True)\n",
    "for store in df_chronos['item_id'].unique():  # 5 store\n",
    "    store_df = merged_df[merged_df['item_id'] == store]\n",
    "    metrics = calcola_metriche(y_true=store_df['target'], y_pred=store_df['mean'],\n",
    "                               y_train=train_df_chronos['target'],\n",
    "                               modelname=\"Chronos\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_chronos.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2028212a-c907-4a26-ae2e-c5e707703aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_metrics_chronos[13] #posso selezionare qualunque store io voglia valutare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc38c2ff-6637-448f-8e72-f961d38358ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_chronos = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_chronos)):\n",
    "    mae_values.append(store_metrics_chronos[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_chronos[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_chronos[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_chronos[i].iloc[1, 0]) \n",
    "    mase_values_chronos.append(store_metrics_chronos[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_chronos)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_chronos)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_chronos = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_Chronos': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_Chronos':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d42725e4-befe-427b-9d1e-502bbdf5882c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_chronos)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_chronos) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_chronos) > 1) / len(mase_values_chronos) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_chronos) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_chronos) < 1) / len(mase_values_chronos) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00d0b13-c0f8-4c80-ae11-3acf1f4ce106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### valutazioni finali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0bbe9d5-0b35-4068-9e36-a6a24a245e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PLOT UNICO con tutte le previsioni per quegli store\n",
    "# forecast plot per store\n",
    "store_ids = ['1','3','5','20','35','54'] # degli store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_np[train_df_np['ID'] == store]\n",
    "    test_store = test_df_np[test_df_np['ID'] == store]\n",
    "    forecast_store_np = forecast_test[forecast_test['ID'] == store]\n",
    "    \n",
    "    predictions.rename(columns={'item_id':'ID'}, inplace=True)\n",
    "    predictions['ID'] = predictions['ID'].astype(str)\n",
    "    forecast_store_chronos = predictions[predictions['ID'] == store]\n",
    "    \n",
    "    predictions_0shot.rename(columns={'item_id':'ID'}, inplace=True)\n",
    "    predictions_0shot['ID'] = predictions_0shot['ID'].astype(str)\n",
    "    forecast_0shot_store_chronos = predictions_0shot[predictions_0shot['ID'] == store]\n",
    "    \n",
    "    \n",
    "    Y_hat_df_lstm.rename(columns={'unique_id':'ID'}, inplace=True)\n",
    "    Y_hat_df_lstm['ID'] = Y_hat_df_lstm['ID'].astype(str)\n",
    "    forecast_store_lstm = Y_hat_df_lstm[Y_hat_df_lstm['ID'] == store]\n",
    "    \n",
    "    # Y_hat_df_tft.rename(columns={'unique_id':'ID'}, inplace=True)\n",
    "    # Y_hat_df_tft['ID'] = Y_hat_df_tft['ID'].astype(str)\n",
    "    # forecast_store_tft = Y_hat_df_tft[Y_hat_df_tft['ID'] == store]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store_np['ds'], \n",
    "        y=forecast_store_np['yhat1'], \n",
    "        mode='lines', \n",
    "        name='NeuralProphet', \n",
    "        line=dict(color='pink')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=forecast_store_lstm['ds'], \n",
    "    y=forecast_store_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='yellow')\n",
    "    ))\n",
    "    # fig.add_trace(go.Scatter(\n",
    "    # x=forecast_store_tft['ds'], \n",
    "    # y=forecast_store_tft['TFT'], \n",
    "    # mode='lines', \n",
    "    # name='TFT', \n",
    "    # line=dict(color='lightblue')\n",
    "    # ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=forecast_store_chronos['timestamp'], \n",
    "    y=forecast_store_chronos['mean'], \n",
    "    mode='lines', \n",
    "    name='Chronos', \n",
    "    line=dict(color='lightgreen')\n",
    "    ))\n",
    "    # fig.add_trace(go.Scatter(\n",
    "    # x=forecast_0shot_store_chronos['timestamp'], \n",
    "    # y=forecast_0shot_store_chronos['mean'], \n",
    "    # mode='lines', \n",
    "    # name='Chronos 0 shot', \n",
    "    # line=dict(color='green')\n",
    "    # ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a47a61b8-35f0-427d-a96d-67ff5fea658a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = pd.merge(mean_metrics_lstm, mean_metrics_np, on='Metric', how='outer')\n",
    "final_metrics = pd.merge(final_metrics, mean_metrics_chronos, on='Metric', how='outer')\n",
    "final_metrics = pd.merge(final_metrics, mean_metrics_tft, on='Metric', how='outer')\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9d2d88-44a7-4942-86f0-461eca2c3e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PLOT DEI MASE PER STORE\n",
    "# Imposta stile\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Linea di riferimento a MASE = 1\n",
    "plt.axhline(y=1, color='red', linestyle='--', label='MASE = 1')\n",
    "\n",
    "# Plot dei MASE\n",
    "plt.plot(mase_values_lstm, label='LSTM', marker='o')\n",
    "plt.plot(mase_values_np, label='NeuralProphet', marker='s')\n",
    "plt.plot(mase_values_chronos, label='Chronos', marker='^')\n",
    "plt.plot(mase_values_tft, label='TFT', marker='d',color=\"aquamarine\") \n",
    "\n",
    "# Etichette e titolo\n",
    "plt.title('MASE per Store', fontsize=16)\n",
    "plt.xlabel('Store ID (indice)', fontsize=12)\n",
    "plt.ylabel('MASE', fontsize=12)\n",
    "store_ids = df['unique_id'].unique()\n",
    "plt.xticks(ticks=range(len(store_ids)), labels=store_ids, rotation=90)\n",
    "\n",
    "# Legenda\n",
    "plt.legend(title='Modelli')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9a42d04-a5ff-4bf1-8228-31d9a634af51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9788b7b1-5ad7-4fa8-8aa1-fbec93c7dd69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **FAVORITA SALES**, panel (multistep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bbcf661-82f1-48fa-a815-3532526b6031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "H=90 gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b036ba9-eff6-409e-a064-6ba729214255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#preparazione dataset\n",
    "df = favorita_train.reset_index().rename(columns={'DATE_TIME': 'ds','sales':'y','Store':'unique_id'})\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.drop(columns=['index'],inplace=True)\n",
    "df = df[df['unique_id'] != 52] #dava problemi nei vari fit (tutti valori costanti, no variabilità)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ac6619-7c5c-4e77-94e3-f7f18093da1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# funzione create features per store\n",
    "def create_time_series_features(df, target_col):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ordina per sicurezza\n",
    "    df = df.sort_values(['unique_id', 'ds'])\n",
    "\n",
    "    # Applicazione delle feature per ogni store\n",
    "    def fe_group(group):\n",
    "        # Lag features (1-7)\n",
    "        for lag in range(1, 8):\n",
    "            group[f'lag_{lag}'] = group[target_col].shift(lag)\n",
    "\n",
    "        # Differencing features\n",
    "        group['diff_1'] = (group[target_col] - group[target_col].shift(1)).shift(1)\n",
    "        group['diff_7'] = (group[target_col] - group[target_col].shift(7)).shift(1)\n",
    "\n",
    "        # Rolling means & std\n",
    "        group['rolling_mean_3'] = group[target_col].rolling(window=3).mean().shift(1)\n",
    "        group['rolling_mean_7'] = group[target_col].rolling(window=7).mean().shift(1)\n",
    "        group['rolling_std_3'] = group[target_col].rolling(window=3).std().shift(1)\n",
    "        group['rolling_std_7'] = group[target_col].rolling(window=7).std().shift(1)\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Applica la funzione per ogni store\n",
    "    df = df.groupby('unique_id', group_keys=False).apply(fe_group)\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "\n",
    "    # Feature cicliche\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['giorno_settimana'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['giorno_settimana'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['mese_del_anno'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['mese_del_anno'] / 12)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4527685-ba08-4ef0-9f11-063027970d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "h = 90\n",
    "futr_exog_list = ['giorno_settimana','settimana_del_mese', \n",
    "                  'settimana_del_anno','giorno_del_mese', 'mese_del_anno',\n",
    "                  'day_sin',  'day_cos', 'month_sin', 'month_cos', 'quarter']\n",
    "future_features = ['ds', 'y'] + futr_exog_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6c4f2e-9898-4c96-8f21-65a229bb9b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe271e7-3bca-4d6b-87e0-8aa6c9038df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lstm = create_time_series_features(df, target_col='y') \n",
    "df_lstm = df_lstm.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f818df7c-3891-487e-8892-3624b18a76b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_lstm, test_df_lstm = split_train_test_by_id(df_lstm, test_frac=0.076)\n",
    "train_df_lstm.shape, test_df_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2228dbb-fb39-4e16-874a-3ebce9536391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating static df\n",
    "# Step 1: One-hot encoding normale\n",
    "ohe_df = pd.get_dummies(train_df_lstm['unique_id'].drop_duplicates(), prefix='store')\n",
    "\n",
    "# Step 2: Inverti i valori (0 -> 1 e 1 -> 0)\n",
    "ohe_df = 1 - ohe_df\n",
    "\n",
    "# Step 3: Aggiungi la colonna unique_id\n",
    "ohe_df['unique_id'] = train_df_lstm['unique_id'].drop_duplicates().values\n",
    "\n",
    "# Step 4: Reordina colonne\n",
    "static_df = ohe_df[['unique_id'] + [col for col in ohe_df.columns if col != 'unique_id']]\n",
    "\n",
    "# Cast a int\n",
    "static_df = static_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71584c68-7642-4594-bd0b-6ce495aa75f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === FIT FINALE DEL MODELLO ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "models = [LSTM(h=h, \n",
    "               input_size=int(4*h),\n",
    "               loss=DistributionLoss(distribution=\"StudentT\", level=[95]),\n",
    "               scaler_type='standard',\n",
    "               encoder_n_layers=3, \n",
    "               encoder_hidden_size=256,\n",
    "               decoder_hidden_size=128,\n",
    "               decoder_layers=2, \n",
    "               max_steps=200,\n",
    "               futr_exog_list=futr_exog_list,\n",
    "               batch_size=128, \n",
    "               learning_rate=0.005, \n",
    "               recurrent=True,\n",
    "               start_padding_enabled=True\n",
    "               )\n",
    "          ]\n",
    "nf = NeuralForecast(models=models, freq='B') \n",
    "print(\"=== STARTING THE MODEL FIT ===\")\n",
    "\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(train_df_lstm, val_size=4*h)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0108fc-dd4b-40e5-9779-1e5f6f42538d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "Y_hat_df_lstm = nf.predict(futr_df=test_df_lstm[['unique_id', 'ds'] + futr_exog_list])\n",
    "#salvo in pickle\n",
    "with open('pickles/LSTM_favorita_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_lstm, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c468e82-8b89-43b3-b3f0-fa524b561c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ee2e93-5168-49a0-9eef-370f5e05ff33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # #carico le previsioni:\n",
    "with open('pickles/LSTM_favorita_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_lstm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d09cad-837f-4034-9c46-05a7888eccc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_lstm[train_df_lstm['unique_id'] == store]\n",
    "    test_store = test_df_lstm[test_df_lstm['unique_id'] == store]\n",
    "    forecast_store = Y_hat_df_lstm[Y_hat_df_lstm['unique_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], \n",
    "                             y=test_store['y'],\n",
    "                             mode='lines', \n",
    "                             name='Testing Data',\n",
    "                             #line=dict(color='red')\n",
    "                             ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['LSTM'], \n",
    "        mode='lines', \n",
    "        name='LSTM Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed6c2ff-2c62-49c7-b863-d78d4c040fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = Y_hat_df_lstm[['ds', 'LSTM', 'unique_id']]\n",
    "merged_df = pd.merge(test_df_lstm[['ds', 'y', 'unique_id']], Y_hat_df_lstm, on=['ds', 'unique_id'], how='left')\n",
    "store_metrics_lstm = []\n",
    "for store in df_lstm['unique_id'].unique():  \n",
    "    store_df = merged_df[merged_df['unique_id'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['LSTM'],\n",
    "                               y_train=train_df_lstm['y'], modelname=\"LSTM\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_lstm.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8491bdad-e273-4828-9555-003331f7cc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_lstm = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_lstm)):\n",
    "    mae_values.append(store_metrics_lstm[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_lstm[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_lstm[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_lstm[i].iloc[1, 0]) \n",
    "    mase_values_lstm.append(store_metrics_lstm[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_lstm)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_lstm)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_lstm = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_LSTM': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_LSTM':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187e473e-42d0-47e3-9187-8cb9f21fb92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_lstm)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_lstm) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_lstm) > 1) / len(mase_values_lstm) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_lstm) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_lstm) < 1) / len(mase_values_lstm) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ac2d4d2-1e64-4bb6-a7cd-84f59bcffa4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NEURAL PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e66bf09-bc84-49b0-b90c-5e9ff50be7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepara il dataset\n",
    "df_np = create_time_series_features(df, target_col='y').dropna()\n",
    "df_np['ID'] = df_np['unique_id'].astype(str)\n",
    "df_np.drop(columns=['unique_id'], inplace=True)\n",
    "df_np['settimana_del_anno'] = df_np['settimana_del_anno'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8006d9e-028b-49a1-8194-795e0eeccb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"ID\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_np, test_df_np = split_train_test_by_id(df_np, test_frac=0.076)\n",
    "train_df_np.shape, test_df_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48aa3632-ae05-459e-98bf-24ab015015a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FIT\n",
    "# Fissa i seed per riproducibilità\n",
    "import random\n",
    "import time\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "np_df_train = train_df_np[future_features + ['ID']].copy()\n",
    "\n",
    "# Modello senza regressori esterni\n",
    "neuralprophet = NeuralProphet(\n",
    "    quantiles=[0.025, 0.975],    \n",
    "    learning_rate=0.01, \n",
    "    batch_size=64,\n",
    "    daily_seasonality=True,    # Impara la stagionalità dai dati\n",
    "    weekly_seasonality=True,   # Impara pattern settimanali\n",
    "    yearly_seasonality=True,   # Impara pattern annuali\n",
    "    loss_func='Huber'\n",
    ")\n",
    "# Aggiungi solo i regressori deterministici\n",
    "for reg in futr_exog_list:\n",
    "    neuralprophet.add_future_regressor(reg)\n",
    "start_time = time.time()\n",
    "metrics_df = neuralprophet.fit(np_df_train, freq=\"B\",epochs=100)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159dad5c-2673-451a-84ed-0f2e3ca2f36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot delle metriche in training\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(metrics_df['epoch'], metrics_df['Loss'], label='Train Loss', color='blue')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['MAE'], label='Train MAE', color='green')\n",
    "plt.plot(metrics_df['epoch'], metrics_df['RMSE'], label='Train RMSE', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training & Validation Metrics by Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3434265a-2ba2-4992-9e4c-4103c6406908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Previsione\n",
    "forecast_test = neuralprophet.predict(test_df_np[future_features+['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0cdf1b8-e906-4fa6-9668-6863a7ea670c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo le prediction sul test in locale\n",
    "with open('pickles/NP_favorita_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(forecast_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f68b9d-e516-4b90-a2ea-93df5d2d4bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CARICO LE PREVISIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477cdc3a-1cfb-40ae-b8aa-53a9a799f82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# carico le previsioni del test\n",
    "with open('pickles/NP_favorita_multistep.pkl', 'rb') as file:\n",
    "    forecast_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42f2629-c628-40f6-b854-e6d688ec0771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = ['1','3','5','20','35','54'] # degli store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_np[train_df_np['ID'] == store]\n",
    "    test_store = test_df_np[test_df_np['ID'] == store]\n",
    "    forecast_store = forecast_test[forecast_test['ID'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['yhat1'], \n",
    "        mode='lines', \n",
    "        name='NeuralProphet Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b641cabb-9ed8-44e2-a178-2382eaa9d397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = forecast_test[['ds', 'yhat1', 'ID']]\n",
    "merged_df = pd.merge(test_df_np[['ds', 'y', 'ID']], forecast_df, on=['ds', 'ID'], how='left')\n",
    "store_metrics_np = []\n",
    "for store in df_np['ID'].unique():  # primi 5 store\n",
    "    store_df = merged_df[merged_df['ID'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['yhat1'],\n",
    "                               y_train=train_df_np['y'], modelname=\"NeuralProphet\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_np.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df5e145-bef1-4eb9-a9e9-fae72cdc5438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_np = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_np)):\n",
    "    mae_values.append(store_metrics_np[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_np[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_np[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_np[i].iloc[1, 0]) \n",
    "    mase_values_np.append(store_metrics_np[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_np)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_np)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_np = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_NP': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_NP':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "894559a8-ba7a-4613-a950-9a01715f39a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_np)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_np) > 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_np) >= 1) / len(mase_values_np) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_np) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_np) < 1) / len(mase_values_np) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10e67454-f268-4452-a91b-5b11a784c354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62df96d8-db43-4358-a8ee-c105603beba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_tft = create_time_series_features(df, target_col='y') \n",
    "df_tft = df_tft.dropna().reset_index(drop=True)\n",
    "# 2. Suddivisione in train/test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"unique_id\"):\n",
    "        group = group.sort_values(\"ds\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_tft, test_df_tft = split_train_test_by_id(df_tft, test_frac=0.076)\n",
    "train_df_tft.shape, test_df_tft.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e707fe4c-75e8-4eff-8ce6-5385bfba9a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating static df\n",
    "# Step 1: One-hot encoding normale\n",
    "ohe_df = pd.get_dummies(train_df_tft['unique_id'].drop_duplicates(), prefix='store')\n",
    "\n",
    "# Step 2: Inverti i valori (0 -> 1 e 1 -> 0)\n",
    "ohe_df = 1 - ohe_df\n",
    "\n",
    "# Step 3: Aggiungi la colonna unique_id\n",
    "ohe_df['unique_id'] = train_df_tft['unique_id'].drop_duplicates().values\n",
    "\n",
    "# Step 4: Reordina colonne\n",
    "static_df = ohe_df[['unique_id'] + [col for col in ohe_df.columns if col != 'unique_id']]\n",
    "\n",
    "# Cast a int\n",
    "static_df = static_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab94263-198e-41a0-852e-f4a282b05100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model fit\n",
    "h = len(test_df_tft) // train_df_tft['unique_id'].nunique()\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        TFT(\n",
    "            h=h,\n",
    "            input_size=h*3,\n",
    "            hidden_size=120, # deve essere divisibile per 4\n",
    "            grn_activation=\"ReLU\",\n",
    "            rnn_type=\"lstm\",\n",
    "            n_rnn_layers=4, \n",
    "            one_rnn_initial_state=False,\n",
    "            loss=DistributionLoss(distribution=\"StudentT\", level=[95]),\n",
    "            learning_rate=0.0075, # prima 0.01\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_features,\n",
    "            max_steps=100,\n",
    "            val_check_steps=25,\n",
    "            early_stop_patience_steps=10,\n",
    "            scaler_type=\"standard\",\n",
    "            windows_batch_size=128, #prima 128\n",
    "            enable_progress_bar=True,\n",
    "        ),\n",
    "    ],\n",
    "    freq=\"B\", \n",
    ")\n",
    "start_time = time.time()\n",
    "#fit \n",
    "nf.fit(df=train_df_tft, static_df=static_df, val_size=h)\n",
    "end_time = time.time()\n",
    "tft_model = nf.models[0]\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tempo di training: {training_time:.4f} secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "580375a3-c3bf-4f62-974c-e118bf3c4303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Y_hat_df_tft = nf.predict(futr_df=test_df_tft)\n",
    "#salvo in pickle\n",
    "with open('pickles/TFT_favorita_multistep.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_hat_df_tft, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9a674c-afcd-44c2-a60a-0dd7413a0b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carico le previsioni TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1543f890-a362-44be-9b66-5732173698a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #carico le previsioni:\n",
    "with open('pickles/TFT_favorita_multistep.pkl', 'rb') as file:\n",
    "    Y_hat_df_tft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d54d1a-a2ed-47d6-aa96-ad825fd47b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_tft[train_df_tft['unique_id'] == store]\n",
    "    test_store = test_df_tft[test_df_tft['unique_id'] == store]\n",
    "    forecast_store = Y_hat_df_tft[Y_hat_df_tft['unique_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['ds'], \n",
    "        y=forecast_store['TFT'], \n",
    "        mode='lines', \n",
    "        name='TFT Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['ds'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba338989-1e18-4f44-a90c-18d7027e2f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "forecast_df = Y_hat_df_tft[['ds', 'TFT', 'unique_id']]\n",
    "merged_df = pd.merge(test_df_tft[['ds', 'y', 'unique_id']], Y_hat_df_tft, on=['ds', 'unique_id'], how='left')\n",
    "store_metrics_tft = []\n",
    "for store in df_tft['unique_id'].unique():  # 5 store\n",
    "    store_df = merged_df[merged_df['unique_id'] == store]\n",
    "    metrics = calcola_metriche(store_df['y'], store_df['TFT'],\n",
    "                               y_train=train_df_tft['y'], modelname=\"TFT\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_tft.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd5a246-8f01-4710-a920-8f045428e9ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_tft = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_tft)):\n",
    "    mae_values.append(store_metrics_tft[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_tft[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_tft[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_tft[i].iloc[1, 0]) \n",
    "    mase_values_tft.append(store_metrics_tft[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "mae_std_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_tft)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "mae_std_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_tft)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_tft = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_TFT': [mae_mean, mape_mean, smape_mean, mae_std_mean, mase_mean],\n",
    "    'Median_TFT':[mae_median, mape_median, smape_median, mae_std_median, mase_median]\n",
    "})\n",
    "mean_metrics_tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d684065e-2fe3-4fa3-9a27-7a41aba2aaeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_tft)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_tft) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_tft) > 1) / len(mase_values_tft) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_tft) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_tft) < 1) / len(mase_values_tft) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b04d9db-8ed8-415e-a22e-8f3d7e8b5860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### TFT EXPLAINABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a23d854-f6f8-467d-af91-b6a17f35c95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# salvo il modello tft\n",
    "# with open('pickles/TFT_model_Favorita_multistep.pkl', 'wb') as file:\n",
    "#     pickle.dump(tft_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "189e2ea9-6e28-45d8-880e-6ee384a4e9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # carico il modello tft\n",
    "with open('pickles/TFT_model_Favorita_multistep.pkl', 'rb') as file:\n",
    "    tft_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a271afc-9df0-4742-9d9f-f53cbe6821c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "attention = tft_model.attention_weights()\n",
    "def plot_attention(\n",
    "    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the attention weights.\n",
    "\n",
    "    Args:\n",
    "        plot (str, optional): The type of plot to generate. Can be one of the following:\n",
    "            - 'time': Display the mean attention weights over time.\n",
    "            - 'all': Display the attention weights for each horizon.\n",
    "            - 'heatmap': Display the attention weights as a heatmap.\n",
    "            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n",
    "        output (str, optional): The type of output to generate. Can be one of the following:\n",
    "            - 'plot': Display the plot directly.\n",
    "            - 'figure': Return the plot as a figure object.\n",
    "        width (int, optional): Width of the plot in pixels. Default is 800.\n",
    "        height (int, optional): Height of the plot in pixels. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n",
    "    \"\"\"\n",
    "\n",
    "    attention = (\n",
    "        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n",
    "        .mean(dim=0)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n",
    "\n",
    "    if plot == \"time\":\n",
    "        attention = attention[self.input_size :, :].mean(axis=0)\n",
    "        ax.plot(np.arange(-self.input_size, self.h), attention)\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Mean Attention\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"all\":\n",
    "        for i in range(self.input_size, attention.shape[0]):\n",
    "            ax.plot(\n",
    "                np.arange(-self.input_size, self.h),\n",
    "                attention[i, :],\n",
    "                label=f\"horizon {i-self.input_size+1}\",\n",
    "            )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(\"Attention per horizon\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    elif plot == \"heatmap\":\n",
    "        cax = ax.imshow(\n",
    "            attention,\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            extent=[-self.input_size, self.h, -self.input_size, self.h],\n",
    "        )\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_title(\"Attention Heatmap\")\n",
    "        ax.set_xlabel(\"Attention (current time step)\")\n",
    "        ax.set_ylabel(\"Attention (previous time step)\")\n",
    "\n",
    "    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n",
    "        i = self.input_size + plot - 1\n",
    "        ax.plot(\n",
    "            np.arange(-self.input_size, self.h),\n",
    "            attention[i, :],\n",
    "            label=f\"horizon {plot}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n",
    "        )\n",
    "        ax.set_title(f\"Attention weight for horizon {plot}\")\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.legend()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output == \"plot\":\n",
    "        plt.show()\n",
    "    elif output == \"figure\":\n",
    "        return fig\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51b9c8b7-6d9f-4065-916d-fcb311155cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEAN ATTENTION\n",
    "plot_attention(tft_model, plot=\"time\")\n",
    "# pesi di attenzione medi nel tempo su tutti gli orizzonti di previsione.\n",
    "# La curva blu rappresenta quanto il modello ha \"guardato\" (attention) \n",
    "# quel punto temporale in input durante la previsione.\n",
    "# Valori più alti significano che il modello ritiene quel punto temporale\n",
    "# particolarmente rilevante per fare le previsioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd16dc2d-9297-468c-9fa0-7d65451d5ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ATTENTION OF A SPECIFIC TIME STEPS\n",
    "plot_attention(tft_model, plot=4)\n",
    "# pesi di attenzione per ogni orizzonte di previsione separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba8a28e-0593-441e-b5d4-a2ddeef827ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_attention(tft_model, plot=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc092047-5589-44fc-b586-3741dcca8da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCES\n",
    "feature_importances = tft_model.feature_importances()\n",
    "print(feature_importances.keys())\n",
    "print(\"\\n ==========PAST FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Past variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531d3eae-ead8-4389-92fa-fed434ed73dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra quanto il modello ha fatto affidamento sulle feature passate \\\n",
    "(quelle che il modello ha potuto osservare prima del punto di previsione). Serve a costruire il contesto dell'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f304358-2596-483b-be73-4c4506778945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCES\n",
    "print(\"\\n ==========FUTURE FEATURE IMPORTANCES==========\")\n",
    "feature_importances[\"Future variable importance over time\"].mean().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08043db6-f7ad-4acb-be0a-3808fd69c969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostra quanto il modello ha sfruttato le feature che sono servite a guidare le previsioni passo-passo nel decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d52cf6ba-d932-4dcb-a447-a190da7c1de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIFFERENZA TRA PAST E FUTURE: \\\n",
    "La diversa importanza riflette quanto ciascuna variabile aiuta nell’analisi del contesto passato (encoder -->> PAST) \\\n",
    "o nella generazione delle previsioni (decoder -->> FUTURE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69290f75-8d5d-4f17-8b1a-0e0d21b69843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances = feature_importances[\"Past variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances.index))\n",
    "\n",
    "for col in df_importances.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances), 0), df_importances[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances[col]\n",
    "ax.set_title(\"Past variable importance over time\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d6cdc2b-e6ea-4f51-b32b-2e1b820d1067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Questo grafico mostra QUANDO (e QUANTO) nel passato ogni feature è stata più utile per costruire il contesto predittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15432ce0-03f7-49a5-b204-0f025546d52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME\n",
    "df_importances2 = feature_importances[\"Future variable importance over time\"]\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances2.index))\n",
    "for col in df_importances2.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances2), 0), df_importances2[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances2[col]\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6eb69f-067e-43fb-bc4a-89188fb71b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Per ogni feature viene mostrato quanto è stata importante per il modello a ciascun orizzonte futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f6ea277-fae2-46f6-9753-f6ec97c371eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PAST VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "df_importances3 = feature_importances[\"Past variable importance over time\"]\n",
    "mean_attention = (\n",
    "    tft_model\n",
    "    .attention_weights()[tft_model.input_size :, :]\n",
    "    .mean(axis=0)[: tft_model.input_size]\n",
    ")\n",
    "df_importances3 = df_importances3.multiply(mean_attention, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances3.index))\n",
    "\n",
    "for col in df_importances3.columns:\n",
    "    p = ax.bar(np.arange(-len(df_importances3), 0), df_importances3[col].values, 0.6, label=col, bottom=bottom)\n",
    "    bottom += df_importances3[col]\n",
    "ax.set_title(\"Past variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.plot(\n",
    "    np.arange(-len(df_importances3), 0),\n",
    "    mean_attention,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07df6098-9fff-4d0a-89b6-3ab8e49cf0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FUTURE VARIABLE IMPORTANCE OVER TIME PONDERATED BY ATTENTION\n",
    "\n",
    "# Estrai l'importanza delle future variables\n",
    "df_importances4 = feature_importances[\"Future variable importance over time\"]\n",
    "\n",
    "# Estrai tutte le attention weights mediate nel tempo\n",
    "full_attention = tft_model.attention_weights()[tft_model.input_size:, :].mean(axis=0)\n",
    "\n",
    "# Estrai solo la parte relativa ai 12 time steps futuri\n",
    "n_future_steps = df_importances4.shape[0]\n",
    "mean_attention_future = full_attention[-n_future_steps:]\n",
    "\n",
    "# Applica la ponderazione riga per riga (sul tempo)\n",
    "df_importances4 = df_importances4.multiply(mean_attention_future, axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "bottom = np.zeros(len(df_importances4.index))\n",
    "\n",
    "for col in df_importances4.columns:\n",
    "    ax.bar(\n",
    "        np.arange(1, len(df_importances4) + 1), \n",
    "        df_importances4[col].values, \n",
    "        width=0.6, \n",
    "        label=col, \n",
    "        bottom=bottom\n",
    "    )\n",
    "    bottom += df_importances4[col].values\n",
    "\n",
    "# Titoli e assi\n",
    "ax.set_title(\"Future variable importance over time ponderated by attention\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_xlabel(\"Future time steps\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Linea media di attenzione\n",
    "ax.plot(\n",
    "    np.arange(1, len(df_importances4) + 1),\n",
    "    mean_attention_future,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"mean_attention\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "428d09b9-8885-45a7-a877-eca29ecc21d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tft_model.feature_importance_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36da1502-f633-41a3-bc99-d7dd68ada935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CHRONOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f80c59-461a-4334-8d53-f6ea84105bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "df_chronos = create_time_series_features(df, target_col='y').dropna()\n",
    "# Creazione dell'oggetto TimeSeriesDataFrame\n",
    "df_chronos['item_id'] = df_chronos['unique_id'] \n",
    "df_chronos.drop(columns=\"unique_id\", inplace=True) \n",
    "df_chronos.rename(columns={'ds': 'timestamp', 'y': 'target'}, inplace=True)\n",
    "\n",
    "# Converti in TimeSeriesDataFrame\n",
    "df_chronos = TimeSeriesDataFrame(\n",
    "    df_chronos,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "# Split train e test\n",
    "#divido train/test ogni serie storica\n",
    "def split_train_test_by_id(df, test_frac=0.1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for uid, group in df.groupby(\"item_id\"):\n",
    "        group = group.sort_values(\"timestamp\")  # Ordina temporalmente\n",
    "        split_idx = int(len(group) * (1 - test_frac))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "    return pd.concat(train_list), pd.concat(test_list)\n",
    "train_df_chronos, test_df_chronos = split_train_test_by_id(df_chronos, test_frac=0.076)\n",
    "\n",
    "# Verifica dimensioni\n",
    "print(f\"Train shape: {train_df_chronos.shape}, Test shape: {test_df_chronos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9b104b-58d9-4528-afff-9dc4e63bd9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit del modello\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=h,\n",
    "    eval_metric=\"MASE\",\n",
    "    target=\"target\",\n",
    "    known_covariates_names=futr_exog_list,\n",
    "    freq=\"B\" \n",
    ")\n",
    "\n",
    "# Aggiunta di più modelli nella configurazione per migliorare le performance\n",
    "predictor.fit(\n",
    "    train_df_chronos,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"CAT\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"smallCAT\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"covariate_regressor\": \"CAT\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"miniCAT\"},\n",
    "            },\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"covariate_regressor\": \"NN_TORCH\", \n",
    "                \"target_scaler\": \"standard\",\n",
    "                \"ag_args\": {\"name_suffix\": \"small_NNtorch\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    time_limit=600,\n",
    "    enable_ensemble=True, \n",
    ")\n",
    "\n",
    "# Valutazione del modello\n",
    "leaderboard = predictor.leaderboard(train_df_chronos)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3f2b00-28db-4a3c-9c52-dfe5b886a3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Previsioni\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "train_df_chronos.set_index([\"item_id\",\"timestamp\"],inplace=True)\n",
    "test_df_chronos.set_index([\"item_id\",\"timestamp\"],inplace=True)\n",
    "\n",
    "test_known_covariates = test_df_chronos[futr_exog_list]\n",
    "predictions = predictor.predict(\n",
    "    data=train_df_chronos, \n",
    "    known_covariates=test_known_covariates\n",
    ")\n",
    "predictions.reset_index(inplace=True)\n",
    "# Previsioni\n",
    "predictions_0shot = predictor.predict(\n",
    "    data=train_df_chronos, \n",
    "    known_covariates=test_known_covariates,\n",
    "    model = \"ChronosZeroShot[bolt_small]\"\n",
    ")\n",
    "predictions_0shot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b92e8ae-d541-4703-9ee1-cb728d50d15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# forecast plot per store\n",
    "store_ids = [1,3,5,20,35,54] #5 store a caso\n",
    "train_df_chronos.reset_index(inplace=True)\n",
    "test_df_chronos.reset_index(inplace=True)\n",
    "for store in store_ids:\n",
    "    train_store = train_df_chronos[train_df_chronos['item_id'] == store]\n",
    "    test_store = test_df_chronos[test_df_chronos['item_id'] == store]\n",
    "    forecast_store = predictions[predictions['item_id'] == store]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['timestamp'], y=train_store['target'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['timestamp'], y=test_store['target'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store['timestamp'], \n",
    "        y=forecast_store['mean'], \n",
    "        mode='lines', \n",
    "        name='CHRONOS Forecast', \n",
    "        line=dict(color='green')\n",
    "    ))\n",
    "\n",
    "    ultimo_test = test_store['timestamp'].max()\n",
    "    fig.add_vline(x=ultimo_test, line_dash=\"dash\", line_color=\"white\", opacity=0.5)\n",
    "    fig.add_annotation(\n",
    "        x=ultimo_test, \n",
    "        y=0.9, \n",
    "        yref=\"paper\", \n",
    "        text=\"Fine Test Set\", \n",
    "        showarrow=True, \n",
    "        arrowhead=2, \n",
    "        arrowcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c46e95-4183-47b8-a83d-bc777625b848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store\n",
    "#test_df_chronos.reset_index(inplace=True)\n",
    "forecast_df = predictions[['timestamp', 'mean', 'item_id']]\n",
    "merged_df = pd.merge(test_df_chronos[['timestamp', 'target', 'item_id']], predictions, \n",
    "                     on=['timestamp', 'item_id'], how='right')\n",
    "store_metrics_chronos = []\n",
    "df_chronos.reset_index(inplace=True)\n",
    "for store in df_chronos['item_id'].unique():  # 5 store\n",
    "    store_df = merged_df[merged_df['item_id'] == store]\n",
    "    metrics = calcola_metriche(y_true=store_df['target'], y_pred=store_df['mean'],\n",
    "                               y_train=train_df_chronos['target'],\n",
    "                               modelname=\"Chronos\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_chronos.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c16b8f-2cfc-4633-baab-6cc54c90d990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcola metriche per ogni store 0 shot\n",
    "#test_df_chronos.reset_index(inplace=True)\n",
    "forecast_df_0shot = predictions_0shot[['timestamp', 'mean', 'item_id']]\n",
    "merged_df_0shot = pd.merge(test_df_chronos[['timestamp', 'target', 'item_id']], predictions_0shot, \n",
    "                     on=['timestamp', 'item_id'], how='right')\n",
    "store_metrics_chronos_0shot = []\n",
    "#df_chronos.reset_index(inplace=True)\n",
    "for store in df_chronos['item_id'].unique():  # 5 store\n",
    "    store_df = merged_df_0shot[merged_df_0shot['item_id'] == store]\n",
    "    metrics = calcola_metriche(y_true=store_df['target'], y_pred=store_df['mean'],\n",
    "                               y_train=train_df_chronos['target'],\n",
    "                               modelname=\"Chronos_0shot\")\n",
    "    metrics['Store_ID'] = store\n",
    "    store_metrics_chronos_0shot.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4a2902e-0f7f-4ed3-9474-dc19e2dcbe5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MEDIE E MEDIANE delle metriche\n",
    "mae_values = []\n",
    "mape_values = []\n",
    "smape_values = []\n",
    "rmse_values = []\n",
    "mase_values_chronos = []\n",
    "\n",
    "# Collect metrics from each store\n",
    "for i in range(len(store_metrics_chronos)):\n",
    "    mae_values.append(store_metrics_chronos[i].iloc[0, 0])  \n",
    "    mape_values.append(store_metrics_chronos[i].iloc[2, 0])  \n",
    "    smape_values.append(store_metrics_chronos[i].iloc[3, 0]) \n",
    "    rmse_values.append(store_metrics_chronos[i].iloc[1, 0]) \n",
    "    mase_values_chronos.append(store_metrics_chronos[i].iloc[4, 0]) \n",
    "\n",
    "# Calculate means\n",
    "mae_mean = np.mean(mae_values)\n",
    "mape_mean = np.mean(mape_values)\n",
    "smape_mean = np.mean(smape_values)\n",
    "rmse_mean = np.mean(rmse_values)\n",
    "mase_mean = np.mean(mase_values_chronos)\n",
    "\n",
    "# Calculate medians\n",
    "mae_median = np.median(mae_values)\n",
    "mape_median = np.median(mape_values)\n",
    "smape_median = np.median(smape_values)\n",
    "rmse_median = np.median(rmse_values)\n",
    "mase_median = np.median(mase_values_chronos)\n",
    "\n",
    "# Create a DataFrame to store the aggregated metrics\n",
    "mean_metrics_chronos = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MAPE', 'SMAPE', 'RMSE', 'MASE'],\n",
    "    'Mean_Chronos': [mae_mean, mape_mean, smape_mean, rmse_mean, mase_mean],\n",
    "    'Median_Chronos':[mae_median, mape_median, smape_median, rmse_median, mase_median]\n",
    "})\n",
    "mean_metrics_chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d75c79f-61ba-46d5-8d0e-9ef82c4c7634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(mase_values_chronos)\n",
    "plt.axhline(y=1, color='red', linestyle='--')\n",
    "print(\"MASE >= 1:\", np.sum(np.array(mase_values_chronos) >= 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_chronos) > 1) / len(mase_values_chronos) * 100).round(2), \"% degli store\")\n",
    "print(\"MASE < 1:\", np.sum(np.array(mase_values_chronos) < 1), \"Percentuale:\",\n",
    "      (np.sum(np.array(mase_values_chronos) < 1) / len(mase_values_chronos) * 100).round(2), \"% degli store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d48257e2-9261-4b0d-b19b-5dca425703a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### valutazioni finali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965684cc-8374-475e-b63c-075c8292c0c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PLOT UNICO con tutte le previsioni per quegli store\n",
    "# forecast plot per store\n",
    "store_ids = ['1','3','5','20','35','54'] # degli store a caso\n",
    "\n",
    "for store in store_ids:\n",
    "    train_store = train_df_np[train_df_np['ID'] == store]\n",
    "    test_store = test_df_np[test_df_np['ID'] == store]\n",
    "    forecast_store_np = forecast_test[forecast_test['ID'] == store]\n",
    "    \n",
    "    predictions.rename(columns={'item_id':'ID'}, inplace=True)\n",
    "    predictions['ID'] = predictions['ID'].astype(str)\n",
    "    forecast_store_chronos = predictions[predictions['ID'] == store]\n",
    "    \n",
    "    predictions_0shot.rename(columns={'item_id':'ID'}, inplace=True)\n",
    "    predictions_0shot['ID'] = predictions_0shot['ID'].astype(str)\n",
    "    forecast_0shot_store_chronos = predictions_0shot[predictions_0shot['ID'] == store]\n",
    "    \n",
    "    \n",
    "    Y_hat_df_lstm.rename(columns={'unique_id':'ID'}, inplace=True)\n",
    "    Y_hat_df_lstm['ID'] = Y_hat_df_lstm['ID'].astype(str)\n",
    "    forecast_store_lstm = Y_hat_df_lstm[Y_hat_df_lstm['ID'] == store]\n",
    "    \n",
    "    # Y_hat_df_tft.rename(columns={'unique_id':'ID'}, inplace=True)\n",
    "    # Y_hat_df_tft['ID'] = Y_hat_df_tft['ID'].astype(str)\n",
    "    # forecast_store_tft = Y_hat_df_tft[Y_hat_df_tft['ID'] == store]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_store['ds'], y=train_store['y'], mode='lines', name='Training Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_store['ds'], y=test_store['y'], mode='lines', name='Testing Data'))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_store_np['ds'], \n",
    "        y=forecast_store_np['yhat1'], \n",
    "        mode='lines', \n",
    "        name='NeuralProphet', \n",
    "        line=dict(color='pink')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=forecast_store_lstm['ds'], \n",
    "    y=forecast_store_lstm['LSTM'], \n",
    "    mode='lines', \n",
    "    name='LSTM', \n",
    "    line=dict(color='yellow')\n",
    "    ))\n",
    "    # fig.add_trace(go.Scatter(\n",
    "    # x=forecast_store_tft['ds'], \n",
    "    # y=forecast_store_tft['TFT'], \n",
    "    # mode='lines', \n",
    "    # name='TFT', \n",
    "    # line=dict(color='lightblue')\n",
    "    # ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=forecast_store_chronos['timestamp'], \n",
    "    y=forecast_store_chronos['mean'], \n",
    "    mode='lines', \n",
    "    name='Chronos', \n",
    "    line=dict(color='lightgreen')\n",
    "    ))\n",
    "    # fig.add_trace(go.Scatter(\n",
    "    # x=forecast_0shot_store_chronos['timestamp'], \n",
    "    # y=forecast_0shot_store_chronos['mean'], \n",
    "    # mode='lines', \n",
    "    # name='Chronos 0 shot', \n",
    "    # line=dict(color='green')\n",
    "    # ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast - Store {store}\",\n",
    "        xaxis_title=\"Timestamp [t]\",\n",
    "        yaxis_title=\"Sales\",\n",
    "        template='plotly_dark',\n",
    "        legend=dict(font=dict(size=15))\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac1f3f6-5dca-44cc-ba11-e676ec0fee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_metrics = pd.merge(mean_metrics_lstm, mean_metrics_np, on='Metric', how='outer')\n",
    "final_metrics = pd.merge(final_metrics, mean_metrics_chronos, on='Metric', how='outer')\n",
    "final_metrics = pd.merge(final_metrics, mean_metrics_tft, on='Metric', how='outer')\n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627bf91f-9030-4f1d-9539-fb8457aeb9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PLOT DEI MASE PER STORE\n",
    "# Imposta stile\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Linea di riferimento a MASE = 1\n",
    "plt.axhline(y=1, color='red', linestyle='--', label='MASE = 1')\n",
    "\n",
    "# Plot dei MASE\n",
    "plt.plot(mase_values_lstm, label='LSTM', marker='o')\n",
    "plt.plot(mase_values_np, label='NeuralProphet', marker='s')\n",
    "plt.plot(mase_values_chronos, label='Chronos', marker='^')\n",
    "plt.plot(mase_values_tft, label='TFT', marker='d',color=\"aquamarine\") \n",
    "\n",
    "# Etichette e titolo\n",
    "plt.title('MASE per Store', fontsize=16)\n",
    "plt.xlabel('Store ID (indice)', fontsize=12)\n",
    "plt.ylabel('MASE', fontsize=12)\n",
    "store_ids = df['unique_id'].unique()\n",
    "plt.xticks(ticks=range(len(store_ids)), labels=store_ids, rotation=90)\n",
    "\n",
    "# Legenda\n",
    "plt.legend(title='Modelli')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_MODELS",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
