# **Time Series Forecasting with Transformers**  
_Masterâ€™s thesis project â€“ Robust forecasting across heterogeneous datasets_

## ğŸš€ Overview
This project explores the application of **Transformer models** to time series forecasting, comparing them against classical approaches and gradient boosting models on datasets with varying characteristics (frequency, exogenous variables, missing data, outliers).  
Goal: understand **when** and **why** Transformers outperform traditional methods.

## ğŸ“Š Project Contents
- âœ”ï¸ Advanced preprocessing (lag features, scaling, missing-value handling)  
- âœ”ï¸ Transformer implementation for forecasting  
- âœ”ï¸ Comparison with ARIMA, XGBoost, and baseline models  
- âœ”ï¸ **h=1** (recursive) and **multistep** forecasts  
- âœ”ï¸ Evaluation with **MASE**  
- âœ”ï¸ Cross-dataset performance analysis  
- âœ”ï¸ Hyperparameter tuning with Optuna  

## ğŸ§  Why this project?
Transformers are reshaping forecasting. This project highlights their strengths, limitations, and the real-world conditions where they generalize better than traditional models.

## ğŸ› ï¸ Tech Stack
- Python, PyTorch  
- XGBoost  
- Optuna  
- Pandas, NumPy, Scikit-learn  
- Matplotlib  

## ğŸ”¥ Highlights
- Rigorous, comparative study  
- Reusable forecasting pipeline  
- Practical insights for real-world applications
